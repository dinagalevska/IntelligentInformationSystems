{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TaCedV5cIzL",
        "outputId": "11621c68-5803-48d2-f3ac-87f30c3af96f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch_geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZ-aqSaocWfT",
        "outputId": "06cf5692-a09d-4cb2-d57e-1ed880505844"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.5.0)\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqxP9ehZcbbK",
        "outputId": "8095d9ee-4879-4354-a41c-d45684342fcd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.2.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.2.1+cpu.html"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzaFaDBwcxMp",
        "outputId": "2ed81fdc-3611-4c55-ba3a-a3bc60a56158"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.2.1+cpu.html\n",
            "Collecting pyg_lib\n",
            "  Downloading https://data.pyg.org/whl/torch-2.2.0%2Bcpu/pyg_lib-0.4.0%2Bpt22cpu-cp310-cp310-linux_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.2.0%2Bcpu/torch_scatter-2.1.2%2Bpt22cpu-cp310-cp310-linux_x86_64.whl (508 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.1/508.1 kB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.2.0%2Bcpu/torch_sparse-0.6.18%2Bpt22cpu-cp310-cp310-linux_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.2.0%2Bcpu/torch_cluster-1.6.3%2Bpt22cpu-cp310-cp310-linux_x86_64.whl (770 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m770.0/770.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_spline_conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.2.0%2Bcpu/torch_spline_conv-1.2.2%2Bpt22cpu-cp310-cp310-linux_x86_64.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.9/213.9 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_sparse) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->torch_sparse) (1.25.2)\n",
            "Installing collected packages: torch_spline_conv, torch_scatter, pyg_lib, torch_sparse, torch_cluster\n",
            "Successfully installed pyg_lib-0.4.0+pt22cpu torch_cluster-1.6.3+pt22cpu torch_scatter-2.1.2+pt22cpu torch_sparse-0.6.18+pt22cpu torch_spline_conv-1.2.2+pt22cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import Linear\n",
        "from torch.nn.functional import dropout\n",
        "from torch_geometric.nn import SAGEConv, global_mean_pool, GCNConv\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data import random_split"
      ],
      "metadata": {
        "id": "6WD0Wm5ccdK7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = TUDataset(root='../data/TUDataset/DD', name='DD').to('cuda')"
      ],
      "metadata": {
        "id": "SdjUPNaOc4Dk"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGQ1Qijqc8Oy",
        "outputId": "1329984d-443d-4d40-e534-9aea9bb69176"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1178"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkH4wC1BdJI4",
        "outputId": "d91cf3eb-60d9-4ec8-ddab-6285e67aebd7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(edge_index=[2, 1798], x=[327, 89], y=[1])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, val_set, test_set = random_split(data, [0.7, 0.2, 0.1])"
      ],
      "metadata": {
        "id": "-aZNO_QJgmCK"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_set, batch_size=2, shuffle=True)"
      ],
      "metadata": {
        "id": "HfF6JS2PdS2n"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader = DataLoader(val_set, batch_size=2, shuffle = True)"
      ],
      "metadata": {
        "id": "nbq3oF2xgy8O"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, val_loader, optimizer, criterion, epochs=5):\n",
        "    for epoch in range(epochs):\n",
        "        for i, batch in enumerate(train_loader):\n",
        "            model.train()\n",
        "\n",
        "            out = model(batch.x, batch.edge_index, batch.batch)\n",
        "\n",
        "            loss = criterion(out, batch.y)\n",
        "            loss.backward()\n",
        "            train_loss = loss.item()\n",
        "\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            print(f'Epoch: {epoch:03d}, Step: {i:03d}, Loss: {train_loss:.4f}')\n",
        "\n",
        "        for i, batch in enumerate(val_loader):\n",
        "            model.eval()\n",
        "\n",
        "            out = model(batch.x, batch.edge_index, batch.batch)\n",
        "\n",
        "            loss = criterion(out, batch.y)\n",
        "            val_loss = loss.item()\n",
        "\n",
        "            print(f'Epoch: {epoch:03d}, Step: {i:03d}, Val Loss: {val_loss:.4f}')"
      ],
      "metadata": {
        "id": "JJ9AYHlaeQXv"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "\n",
        "        self.conv1 = SAGEConv((-1, -1), 64)\n",
        "        self.conv2 = SAGEConv((-1, -1), 128)\n",
        "        self.conv3 = SAGEConv((-1, -1), 64)\n",
        "\n",
        "        self.linear1 = Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # 1. Obtain node embeddings\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        # 2. Readout layer\n",
        "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
        "\n",
        "        # 3. Apply a final classifier\n",
        "        x = dropout(x, p=0.5, training=self.training)\n",
        "        x = self.linear1(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "SJxhGnV7eRFf"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise-1 - Graph Classification with GraphSAGE"
      ],
      "metadata": {
        "id": "w_JEOy9Od8lQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GraphSAGE(num_classes=2).to('cuda')"
      ],
      "metadata": {
        "id": "UFQqLAoTdxwY"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = Adam(model.parameters(), lr=0.01)\n",
        "criterion = CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "BCOI4I_JeqQA"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, train_loader, val_loader, optimizer, criterion, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHq8yYPWfDdn",
        "outputId": "b4de1415-204c-4b88-9ff0-97b46271fb75"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 000, Step: 000, Loss: 0.6793\n",
            "Epoch: 000, Step: 001, Loss: 0.6424\n",
            "Epoch: 000, Step: 002, Loss: 0.8263\n",
            "Epoch: 000, Step: 003, Loss: 0.6683\n",
            "Epoch: 000, Step: 004, Loss: 0.6780\n",
            "Epoch: 000, Step: 005, Loss: 0.8388\n",
            "Epoch: 000, Step: 006, Loss: 0.8534\n",
            "Epoch: 000, Step: 007, Loss: 0.6929\n",
            "Epoch: 000, Step: 008, Loss: 0.7579\n",
            "Epoch: 000, Step: 009, Loss: 0.7165\n",
            "Epoch: 000, Step: 010, Loss: 0.6709\n",
            "Epoch: 000, Step: 011, Loss: 0.6723\n",
            "Epoch: 000, Step: 012, Loss: 0.6494\n",
            "Epoch: 000, Step: 013, Loss: 0.6230\n",
            "Epoch: 000, Step: 014, Loss: 0.7667\n",
            "Epoch: 000, Step: 015, Loss: 0.6585\n",
            "Epoch: 000, Step: 016, Loss: 0.6459\n",
            "Epoch: 000, Step: 017, Loss: 0.6147\n",
            "Epoch: 000, Step: 018, Loss: 0.6827\n",
            "Epoch: 000, Step: 019, Loss: 0.6173\n",
            "Epoch: 000, Step: 020, Loss: 0.7201\n",
            "Epoch: 000, Step: 021, Loss: 0.7323\n",
            "Epoch: 000, Step: 022, Loss: 0.9054\n",
            "Epoch: 000, Step: 023, Loss: 0.6792\n",
            "Epoch: 000, Step: 024, Loss: 0.7193\n",
            "Epoch: 000, Step: 025, Loss: 0.5558\n",
            "Epoch: 000, Step: 026, Loss: 0.8269\n",
            "Epoch: 000, Step: 027, Loss: 0.7813\n",
            "Epoch: 000, Step: 028, Loss: 0.6104\n",
            "Epoch: 000, Step: 029, Loss: 0.6765\n",
            "Epoch: 000, Step: 030, Loss: 0.7162\n",
            "Epoch: 000, Step: 031, Loss: 0.6552\n",
            "Epoch: 000, Step: 032, Loss: 0.5298\n",
            "Epoch: 000, Step: 033, Loss: 0.6620\n",
            "Epoch: 000, Step: 034, Loss: 0.6208\n",
            "Epoch: 000, Step: 035, Loss: 0.7347\n",
            "Epoch: 000, Step: 036, Loss: 0.4893\n",
            "Epoch: 000, Step: 037, Loss: 0.4963\n",
            "Epoch: 000, Step: 038, Loss: 0.4601\n",
            "Epoch: 000, Step: 039, Loss: 0.6180\n",
            "Epoch: 000, Step: 040, Loss: 0.5770\n",
            "Epoch: 000, Step: 041, Loss: 0.6012\n",
            "Epoch: 000, Step: 042, Loss: 0.5264\n",
            "Epoch: 000, Step: 043, Loss: 0.1974\n",
            "Epoch: 000, Step: 044, Loss: 1.1279\n",
            "Epoch: 000, Step: 045, Loss: 0.8156\n",
            "Epoch: 000, Step: 046, Loss: 1.6989\n",
            "Epoch: 000, Step: 047, Loss: 0.6987\n",
            "Epoch: 000, Step: 048, Loss: 1.1255\n",
            "Epoch: 000, Step: 049, Loss: 0.5736\n",
            "Epoch: 000, Step: 050, Loss: 0.5787\n",
            "Epoch: 000, Step: 051, Loss: 0.6408\n",
            "Epoch: 000, Step: 052, Loss: 0.6016\n",
            "Epoch: 000, Step: 053, Loss: 0.5806\n",
            "Epoch: 000, Step: 054, Loss: 0.6732\n",
            "Epoch: 000, Step: 055, Loss: 0.6783\n",
            "Epoch: 000, Step: 056, Loss: 0.5604\n",
            "Epoch: 000, Step: 057, Loss: 0.6549\n",
            "Epoch: 000, Step: 058, Loss: 0.6132\n",
            "Epoch: 000, Step: 059, Loss: 0.7921\n",
            "Epoch: 000, Step: 060, Loss: 0.6239\n",
            "Epoch: 000, Step: 061, Loss: 0.6277\n",
            "Epoch: 000, Step: 062, Loss: 0.8598\n",
            "Epoch: 000, Step: 063, Loss: 0.6092\n",
            "Epoch: 000, Step: 064, Loss: 0.6191\n",
            "Epoch: 000, Step: 065, Loss: 0.8313\n",
            "Epoch: 000, Step: 066, Loss: 0.6221\n",
            "Epoch: 000, Step: 067, Loss: 0.6024\n",
            "Epoch: 000, Step: 068, Loss: 0.7543\n",
            "Epoch: 000, Step: 069, Loss: 0.8193\n",
            "Epoch: 000, Step: 070, Loss: 0.7824\n",
            "Epoch: 000, Step: 071, Loss: 0.8457\n",
            "Epoch: 000, Step: 072, Loss: 0.6680\n",
            "Epoch: 000, Step: 073, Loss: 0.6872\n",
            "Epoch: 000, Step: 074, Loss: 0.6828\n",
            "Epoch: 000, Step: 075, Loss: 0.6750\n",
            "Epoch: 000, Step: 076, Loss: 0.6630\n",
            "Epoch: 000, Step: 077, Loss: 0.6834\n",
            "Epoch: 000, Step: 078, Loss: 0.6752\n",
            "Epoch: 000, Step: 079, Loss: 0.6903\n",
            "Epoch: 000, Step: 080, Loss: 0.6949\n",
            "Epoch: 000, Step: 081, Loss: 0.6824\n",
            "Epoch: 000, Step: 082, Loss: 0.6542\n",
            "Epoch: 000, Step: 083, Loss: 0.6729\n",
            "Epoch: 000, Step: 084, Loss: 0.6607\n",
            "Epoch: 000, Step: 085, Loss: 0.6828\n",
            "Epoch: 000, Step: 086, Loss: 0.6676\n",
            "Epoch: 000, Step: 087, Loss: 0.6819\n",
            "Epoch: 000, Step: 088, Loss: 0.6183\n",
            "Epoch: 000, Step: 089, Loss: 0.6067\n",
            "Epoch: 000, Step: 090, Loss: 0.7125\n",
            "Epoch: 000, Step: 091, Loss: 0.6132\n",
            "Epoch: 000, Step: 092, Loss: 0.7840\n",
            "Epoch: 000, Step: 093, Loss: 0.5872\n",
            "Epoch: 000, Step: 094, Loss: 0.6927\n",
            "Epoch: 000, Step: 095, Loss: 0.7722\n",
            "Epoch: 000, Step: 096, Loss: 0.5263\n",
            "Epoch: 000, Step: 097, Loss: 0.6755\n",
            "Epoch: 000, Step: 098, Loss: 0.7499\n",
            "Epoch: 000, Step: 099, Loss: 0.5766\n",
            "Epoch: 000, Step: 100, Loss: 0.7506\n",
            "Epoch: 000, Step: 101, Loss: 0.7170\n",
            "Epoch: 000, Step: 102, Loss: 0.6002\n",
            "Epoch: 000, Step: 103, Loss: 0.5751\n",
            "Epoch: 000, Step: 104, Loss: 0.8036\n",
            "Epoch: 000, Step: 105, Loss: 0.5917\n",
            "Epoch: 000, Step: 106, Loss: 0.6065\n",
            "Epoch: 000, Step: 107, Loss: 0.7772\n",
            "Epoch: 000, Step: 108, Loss: 0.7860\n",
            "Epoch: 000, Step: 109, Loss: 0.6520\n",
            "Epoch: 000, Step: 110, Loss: 0.9062\n",
            "Epoch: 000, Step: 111, Loss: 0.5533\n",
            "Epoch: 000, Step: 112, Loss: 0.5387\n",
            "Epoch: 000, Step: 113, Loss: 0.8113\n",
            "Epoch: 000, Step: 114, Loss: 0.6495\n",
            "Epoch: 000, Step: 115, Loss: 0.8245\n",
            "Epoch: 000, Step: 116, Loss: 0.7875\n",
            "Epoch: 000, Step: 117, Loss: 0.5650\n",
            "Epoch: 000, Step: 118, Loss: 0.5937\n",
            "Epoch: 000, Step: 119, Loss: 0.7143\n",
            "Epoch: 000, Step: 120, Loss: 0.6485\n",
            "Epoch: 000, Step: 121, Loss: 0.7508\n",
            "Epoch: 000, Step: 122, Loss: 0.6360\n",
            "Epoch: 000, Step: 123, Loss: 0.6633\n",
            "Epoch: 000, Step: 124, Loss: 0.7526\n",
            "Epoch: 000, Step: 125, Loss: 0.6581\n",
            "Epoch: 000, Step: 126, Loss: 0.7443\n",
            "Epoch: 000, Step: 127, Loss: 0.6326\n",
            "Epoch: 000, Step: 128, Loss: 0.6643\n",
            "Epoch: 000, Step: 129, Loss: 0.6924\n",
            "Epoch: 000, Step: 130, Loss: 0.6532\n",
            "Epoch: 000, Step: 131, Loss: 0.7201\n",
            "Epoch: 000, Step: 132, Loss: 0.5526\n",
            "Epoch: 000, Step: 133, Loss: 0.5416\n",
            "Epoch: 000, Step: 134, Loss: 0.5725\n",
            "Epoch: 000, Step: 135, Loss: 0.6247\n",
            "Epoch: 000, Step: 136, Loss: 0.6627\n",
            "Epoch: 000, Step: 137, Loss: 0.6088\n",
            "Epoch: 000, Step: 138, Loss: 0.6088\n",
            "Epoch: 000, Step: 139, Loss: 0.7564\n",
            "Epoch: 000, Step: 140, Loss: 0.3911\n",
            "Epoch: 000, Step: 141, Loss: 0.7852\n",
            "Epoch: 000, Step: 142, Loss: 0.8844\n",
            "Epoch: 000, Step: 143, Loss: 0.7643\n",
            "Epoch: 000, Step: 144, Loss: 0.4903\n",
            "Epoch: 000, Step: 145, Loss: 0.3431\n",
            "Epoch: 000, Step: 146, Loss: 0.5073\n",
            "Epoch: 000, Step: 147, Loss: 0.8417\n",
            "Epoch: 000, Step: 148, Loss: 0.8016\n",
            "Epoch: 000, Step: 149, Loss: 0.8782\n",
            "Epoch: 000, Step: 150, Loss: 0.8782\n",
            "Epoch: 000, Step: 151, Loss: 0.8838\n",
            "Epoch: 000, Step: 152, Loss: 0.7474\n",
            "Epoch: 000, Step: 153, Loss: 0.9603\n",
            "Epoch: 000, Step: 154, Loss: 0.7103\n",
            "Epoch: 000, Step: 155, Loss: 0.7839\n",
            "Epoch: 000, Step: 156, Loss: 0.5644\n",
            "Epoch: 000, Step: 157, Loss: 0.4711\n",
            "Epoch: 000, Step: 158, Loss: 0.9154\n",
            "Epoch: 000, Step: 159, Loss: 0.6732\n",
            "Epoch: 000, Step: 160, Loss: 0.5590\n",
            "Epoch: 000, Step: 161, Loss: 0.5331\n",
            "Epoch: 000, Step: 162, Loss: 0.6060\n",
            "Epoch: 000, Step: 163, Loss: 0.7109\n",
            "Epoch: 000, Step: 164, Loss: 0.6370\n",
            "Epoch: 000, Step: 165, Loss: 0.5257\n",
            "Epoch: 000, Step: 166, Loss: 0.6044\n",
            "Epoch: 000, Step: 167, Loss: 0.6649\n",
            "Epoch: 000, Step: 168, Loss: 0.6573\n",
            "Epoch: 000, Step: 169, Loss: 0.6148\n",
            "Epoch: 000, Step: 170, Loss: 0.8036\n",
            "Epoch: 000, Step: 171, Loss: 0.7351\n",
            "Epoch: 000, Step: 172, Loss: 0.7494\n",
            "Epoch: 000, Step: 173, Loss: 0.6653\n",
            "Epoch: 000, Step: 174, Loss: 0.6067\n",
            "Epoch: 000, Step: 175, Loss: 0.6431\n",
            "Epoch: 000, Step: 176, Loss: 0.8117\n",
            "Epoch: 000, Step: 177, Loss: 0.5772\n",
            "Epoch: 000, Step: 178, Loss: 0.7328\n",
            "Epoch: 000, Step: 179, Loss: 0.5745\n",
            "Epoch: 000, Step: 180, Loss: 0.7443\n",
            "Epoch: 000, Step: 181, Loss: 0.5956\n",
            "Epoch: 000, Step: 182, Loss: 0.8153\n",
            "Epoch: 000, Step: 183, Loss: 0.9070\n",
            "Epoch: 000, Step: 184, Loss: 0.8525\n",
            "Epoch: 000, Step: 185, Loss: 0.6349\n",
            "Epoch: 000, Step: 186, Loss: 0.5799\n",
            "Epoch: 000, Step: 187, Loss: 0.6142\n",
            "Epoch: 000, Step: 188, Loss: 0.6355\n",
            "Epoch: 000, Step: 189, Loss: 0.8114\n",
            "Epoch: 000, Step: 190, Loss: 0.8071\n",
            "Epoch: 000, Step: 191, Loss: 0.6030\n",
            "Epoch: 000, Step: 192, Loss: 0.7322\n",
            "Epoch: 000, Step: 193, Loss: 0.7422\n",
            "Epoch: 000, Step: 194, Loss: 0.8710\n",
            "Epoch: 000, Step: 195, Loss: 0.7328\n",
            "Epoch: 000, Step: 196, Loss: 0.6743\n",
            "Epoch: 000, Step: 197, Loss: 0.8405\n",
            "Epoch: 000, Step: 198, Loss: 0.7889\n",
            "Epoch: 000, Step: 199, Loss: 0.8498\n",
            "Epoch: 000, Step: 200, Loss: 0.6116\n",
            "Epoch: 000, Step: 201, Loss: 0.6454\n",
            "Epoch: 000, Step: 202, Loss: 0.6383\n",
            "Epoch: 000, Step: 203, Loss: 0.6597\n",
            "Epoch: 000, Step: 204, Loss: 0.6211\n",
            "Epoch: 000, Step: 205, Loss: 0.7821\n",
            "Epoch: 000, Step: 206, Loss: 0.4960\n",
            "Epoch: 000, Step: 000, Val Loss: 0.6942\n",
            "Epoch: 000, Step: 001, Val Loss: 0.7170\n",
            "Epoch: 000, Step: 002, Val Loss: 0.6942\n",
            "Epoch: 000, Step: 003, Val Loss: 0.7170\n",
            "Epoch: 000, Step: 004, Val Loss: 0.6486\n",
            "Epoch: 000, Step: 005, Val Loss: 0.7170\n",
            "Epoch: 000, Step: 006, Val Loss: 0.6942\n",
            "Epoch: 000, Step: 007, Val Loss: 0.6714\n",
            "Epoch: 000, Step: 008, Val Loss: 0.6714\n",
            "Epoch: 000, Step: 009, Val Loss: 0.6486\n",
            "Epoch: 000, Step: 010, Val Loss: 0.6714\n",
            "Epoch: 000, Step: 011, Val Loss: 0.6942\n",
            "Epoch: 000, Step: 012, Val Loss: 0.7170\n",
            "Epoch: 000, Step: 013, Val Loss: 0.6942\n",
            "Epoch: 000, Step: 014, Val Loss: 0.6714\n",
            "Epoch: 000, Step: 015, Val Loss: 0.6942\n",
            "Epoch: 000, Step: 016, Val Loss: 0.6714\n",
            "Epoch: 000, Step: 017, Val Loss: 0.6942\n",
            "Epoch: 000, Step: 018, Val Loss: 0.7170\n",
            "Epoch: 000, Step: 019, Val Loss: 0.6714\n",
            "Epoch: 000, Step: 020, Val Loss: 0.6714\n",
            "Epoch: 000, Step: 021, Val Loss: 0.6942\n",
            "Epoch: 000, Step: 022, Val Loss: 0.6942\n",
            "Epoch: 000, Step: 023, Val Loss: 0.6714\n",
            "Epoch: 000, Step: 024, Val Loss: 0.6942\n",
            "Epoch: 000, Step: 025, Val Loss: 0.6942\n",
            "Epoch: 000, Step: 026, Val Loss: 0.6942\n",
            "Epoch: 000, Step: 027, Val Loss: 0.6942\n",
            "Epoch: 000, Step: 028, Val Loss: 0.6942\n",
            "Epoch: 000, Step: 029, Val Loss: 0.6486\n",
            "Epoch: 000, Step: 030, Val Loss: 0.7170\n",
            "Epoch: 000, Step: 031, Val Loss: 0.6942\n",
            "Epoch: 000, Step: 032, Val Loss: 0.6714\n",
            "Epoch: 000, Step: 033, Val Loss: 0.7170\n",
            "Epoch: 000, Step: 034, Val Loss: 0.6486\n",
            "Epoch: 000, Step: 035, Val Loss: 0.6714\n",
            "Epoch: 000, Step: 036, Val Loss: 0.7170\n",
            "Epoch: 000, Step: 037, Val Loss: 0.6942\n",
            "Epoch: 000, Step: 038, Val Loss: 0.6942\n",
            "Epoch: 000, Step: 039, Val Loss: 0.6942\n",
            "Epoch: 000, Step: 040, Val Loss: 0.6942\n",
            "Epoch: 000, Step: 041, Val Loss: 0.6714\n",
            "Epoch: 000, Step: 042, Val Loss: 0.6486\n",
            "Epoch: 000, Step: 043, Val Loss: 0.6486\n",
            "Epoch: 000, Step: 044, Val Loss: 0.7170\n",
            "Epoch: 000, Step: 045, Val Loss: 0.6942\n",
            "Epoch: 000, Step: 046, Val Loss: 0.6942\n",
            "Epoch: 000, Step: 047, Val Loss: 0.7170\n",
            "Epoch: 000, Step: 048, Val Loss: 0.6942\n",
            "Epoch: 000, Step: 049, Val Loss: 0.6942\n",
            "Epoch: 000, Step: 050, Val Loss: 0.6942\n",
            "Epoch: 000, Step: 051, Val Loss: 0.6714\n",
            "Epoch: 000, Step: 052, Val Loss: 0.6714\n",
            "Epoch: 000, Step: 053, Val Loss: 0.7170\n",
            "Epoch: 000, Step: 054, Val Loss: 0.6486\n",
            "Epoch: 000, Step: 055, Val Loss: 0.6486\n",
            "Epoch: 000, Step: 056, Val Loss: 0.6942\n",
            "Epoch: 000, Step: 057, Val Loss: 0.6486\n",
            "Epoch: 000, Step: 058, Val Loss: 0.6942\n",
            "Epoch: 001, Step: 000, Loss: 0.7142\n",
            "Epoch: 001, Step: 001, Loss: 0.7763\n",
            "Epoch: 001, Step: 002, Loss: 0.6919\n",
            "Epoch: 001, Step: 003, Loss: 0.6189\n",
            "Epoch: 001, Step: 004, Loss: 0.8300\n",
            "Epoch: 001, Step: 005, Loss: 0.6585\n",
            "Epoch: 001, Step: 006, Loss: 0.5486\n",
            "Epoch: 001, Step: 007, Loss: 0.6851\n",
            "Epoch: 001, Step: 008, Loss: 0.6525\n",
            "Epoch: 001, Step: 009, Loss: 0.6823\n",
            "Epoch: 001, Step: 010, Loss: 0.5343\n",
            "Epoch: 001, Step: 011, Loss: 0.7033\n",
            "Epoch: 001, Step: 012, Loss: 0.9728\n",
            "Epoch: 001, Step: 013, Loss: 0.7176\n",
            "Epoch: 001, Step: 014, Loss: 0.6484\n",
            "Epoch: 001, Step: 015, Loss: 0.6274\n",
            "Epoch: 001, Step: 016, Loss: 0.6142\n",
            "Epoch: 001, Step: 017, Loss: 0.7011\n",
            "Epoch: 001, Step: 018, Loss: 0.8347\n",
            "Epoch: 001, Step: 019, Loss: 0.7095\n",
            "Epoch: 001, Step: 020, Loss: 0.8598\n",
            "Epoch: 001, Step: 021, Loss: 0.5184\n",
            "Epoch: 001, Step: 022, Loss: 0.6259\n",
            "Epoch: 001, Step: 023, Loss: 0.7707\n",
            "Epoch: 001, Step: 024, Loss: 0.6850\n",
            "Epoch: 001, Step: 025, Loss: 0.8305\n",
            "Epoch: 001, Step: 026, Loss: 0.5656\n",
            "Epoch: 001, Step: 027, Loss: 0.8047\n",
            "Epoch: 001, Step: 028, Loss: 0.8332\n",
            "Epoch: 001, Step: 029, Loss: 0.7240\n",
            "Epoch: 001, Step: 030, Loss: 0.7076\n",
            "Epoch: 001, Step: 031, Loss: 0.6913\n",
            "Epoch: 001, Step: 032, Loss: 0.5997\n",
            "Epoch: 001, Step: 033, Loss: 0.6686\n",
            "Epoch: 001, Step: 034, Loss: 0.6607\n",
            "Epoch: 001, Step: 035, Loss: 0.7519\n",
            "Epoch: 001, Step: 036, Loss: 0.6630\n",
            "Epoch: 001, Step: 037, Loss: 0.6977\n",
            "Epoch: 001, Step: 038, Loss: 0.6410\n",
            "Epoch: 001, Step: 039, Loss: 0.7346\n",
            "Epoch: 001, Step: 040, Loss: 0.5677\n",
            "Epoch: 001, Step: 041, Loss: 0.7343\n",
            "Epoch: 001, Step: 042, Loss: 0.5683\n",
            "Epoch: 001, Step: 043, Loss: 0.7097\n",
            "Epoch: 001, Step: 044, Loss: 0.6757\n",
            "Epoch: 001, Step: 045, Loss: 0.5767\n",
            "Epoch: 001, Step: 046, Loss: 0.7399\n",
            "Epoch: 001, Step: 047, Loss: 0.6634\n",
            "Epoch: 001, Step: 048, Loss: 0.6885\n",
            "Epoch: 001, Step: 049, Loss: 0.6798\n",
            "Epoch: 001, Step: 050, Loss: 0.5812\n",
            "Epoch: 001, Step: 051, Loss: 0.6402\n",
            "Epoch: 001, Step: 052, Loss: 0.5933\n",
            "Epoch: 001, Step: 053, Loss: 0.5675\n",
            "Epoch: 001, Step: 054, Loss: 0.7138\n",
            "Epoch: 001, Step: 055, Loss: 0.6975\n",
            "Epoch: 001, Step: 056, Loss: 0.6651\n",
            "Epoch: 001, Step: 057, Loss: 0.5831\n",
            "Epoch: 001, Step: 058, Loss: 0.6913\n",
            "Epoch: 001, Step: 059, Loss: 0.7678\n",
            "Epoch: 001, Step: 060, Loss: 1.0465\n",
            "Epoch: 001, Step: 061, Loss: 0.4318\n",
            "Epoch: 001, Step: 062, Loss: 0.6616\n",
            "Epoch: 001, Step: 063, Loss: 0.9509\n",
            "Epoch: 001, Step: 064, Loss: 0.6914\n",
            "Epoch: 001, Step: 065, Loss: 1.1075\n",
            "Epoch: 001, Step: 066, Loss: 0.5995\n",
            "Epoch: 001, Step: 067, Loss: 0.5807\n",
            "Epoch: 001, Step: 068, Loss: 0.6963\n",
            "Epoch: 001, Step: 069, Loss: 0.7572\n",
            "Epoch: 001, Step: 070, Loss: 0.9975\n",
            "Epoch: 001, Step: 071, Loss: 0.6999\n",
            "Epoch: 001, Step: 072, Loss: 0.6444\n",
            "Epoch: 001, Step: 073, Loss: 0.6242\n",
            "Epoch: 001, Step: 074, Loss: 0.6685\n",
            "Epoch: 001, Step: 075, Loss: 0.7803\n",
            "Epoch: 001, Step: 076, Loss: 0.5842\n",
            "Epoch: 001, Step: 077, Loss: 0.6538\n",
            "Epoch: 001, Step: 078, Loss: 0.7730\n",
            "Epoch: 001, Step: 079, Loss: 0.5404\n",
            "Epoch: 001, Step: 080, Loss: 0.7035\n",
            "Epoch: 001, Step: 081, Loss: 0.5693\n",
            "Epoch: 001, Step: 082, Loss: 0.6974\n",
            "Epoch: 001, Step: 083, Loss: 0.7544\n",
            "Epoch: 001, Step: 084, Loss: 0.6645\n",
            "Epoch: 001, Step: 085, Loss: 0.7213\n",
            "Epoch: 001, Step: 086, Loss: 0.7676\n",
            "Epoch: 001, Step: 087, Loss: 0.7294\n",
            "Epoch: 001, Step: 088, Loss: 0.6591\n",
            "Epoch: 001, Step: 089, Loss: 0.6761\n",
            "Epoch: 001, Step: 090, Loss: 0.6274\n",
            "Epoch: 001, Step: 091, Loss: 0.6836\n",
            "Epoch: 001, Step: 092, Loss: 0.6103\n",
            "Epoch: 001, Step: 093, Loss: 0.5374\n",
            "Epoch: 001, Step: 094, Loss: 0.7237\n",
            "Epoch: 001, Step: 095, Loss: 0.7102\n",
            "Epoch: 001, Step: 096, Loss: 0.7565\n",
            "Epoch: 001, Step: 097, Loss: 0.6928\n",
            "Epoch: 001, Step: 098, Loss: 0.5561\n",
            "Epoch: 001, Step: 099, Loss: 0.5187\n",
            "Epoch: 001, Step: 100, Loss: 0.4566\n",
            "Epoch: 001, Step: 101, Loss: 0.7787\n",
            "Epoch: 001, Step: 102, Loss: 0.5733\n",
            "Epoch: 001, Step: 103, Loss: 0.5412\n",
            "Epoch: 001, Step: 104, Loss: 0.7456\n",
            "Epoch: 001, Step: 105, Loss: 0.4458\n",
            "Epoch: 001, Step: 106, Loss: 1.1007\n",
            "Epoch: 001, Step: 107, Loss: 0.8076\n",
            "Epoch: 001, Step: 108, Loss: 1.0335\n",
            "Epoch: 001, Step: 109, Loss: 0.6500\n",
            "Epoch: 001, Step: 110, Loss: 0.8773\n",
            "Epoch: 001, Step: 111, Loss: 0.6458\n",
            "Epoch: 001, Step: 112, Loss: 0.5279\n",
            "Epoch: 001, Step: 113, Loss: 0.5653\n",
            "Epoch: 001, Step: 114, Loss: 0.4803\n",
            "Epoch: 001, Step: 115, Loss: 0.7181\n",
            "Epoch: 001, Step: 116, Loss: 0.8671\n",
            "Epoch: 001, Step: 117, Loss: 0.8687\n",
            "Epoch: 001, Step: 118, Loss: 0.5904\n",
            "Epoch: 001, Step: 119, Loss: 0.7207\n",
            "Epoch: 001, Step: 120, Loss: 0.7239\n",
            "Epoch: 001, Step: 121, Loss: 0.6658\n",
            "Epoch: 001, Step: 122, Loss: 0.6924\n",
            "Epoch: 001, Step: 123, Loss: 0.5092\n",
            "Epoch: 001, Step: 124, Loss: 0.6367\n",
            "Epoch: 001, Step: 125, Loss: 0.9564\n",
            "Epoch: 001, Step: 126, Loss: 0.6518\n",
            "Epoch: 001, Step: 127, Loss: 0.7197\n",
            "Epoch: 001, Step: 128, Loss: 0.6414\n",
            "Epoch: 001, Step: 129, Loss: 0.6777\n",
            "Epoch: 001, Step: 130, Loss: 0.7695\n",
            "Epoch: 001, Step: 131, Loss: 0.7386\n",
            "Epoch: 001, Step: 132, Loss: 0.5357\n",
            "Epoch: 001, Step: 133, Loss: 0.5773\n",
            "Epoch: 001, Step: 134, Loss: 0.5386\n",
            "Epoch: 001, Step: 135, Loss: 0.6793\n",
            "Epoch: 001, Step: 136, Loss: 0.7377\n",
            "Epoch: 001, Step: 137, Loss: 0.7614\n",
            "Epoch: 001, Step: 138, Loss: 0.5994\n",
            "Epoch: 001, Step: 139, Loss: 0.8347\n",
            "Epoch: 001, Step: 140, Loss: 0.8795\n",
            "Epoch: 001, Step: 141, Loss: 0.6452\n",
            "Epoch: 001, Step: 142, Loss: 0.7562\n",
            "Epoch: 001, Step: 143, Loss: 0.8497\n",
            "Epoch: 001, Step: 144, Loss: 0.9551\n",
            "Epoch: 001, Step: 145, Loss: 0.6453\n",
            "Epoch: 001, Step: 146, Loss: 0.5826\n",
            "Epoch: 001, Step: 147, Loss: 0.5412\n",
            "Epoch: 001, Step: 148, Loss: 0.6019\n",
            "Epoch: 001, Step: 149, Loss: 0.6880\n",
            "Epoch: 001, Step: 150, Loss: 0.7282\n",
            "Epoch: 001, Step: 151, Loss: 0.8076\n",
            "Epoch: 001, Step: 152, Loss: 0.7125\n",
            "Epoch: 001, Step: 153, Loss: 0.6120\n",
            "Epoch: 001, Step: 154, Loss: 0.7149\n",
            "Epoch: 001, Step: 155, Loss: 0.6495\n",
            "Epoch: 001, Step: 156, Loss: 0.7126\n",
            "Epoch: 001, Step: 157, Loss: 0.6406\n",
            "Epoch: 001, Step: 158, Loss: 0.8072\n",
            "Epoch: 001, Step: 159, Loss: 0.6256\n",
            "Epoch: 001, Step: 160, Loss: 0.5934\n",
            "Epoch: 001, Step: 161, Loss: 0.5420\n",
            "Epoch: 001, Step: 162, Loss: 0.7320\n",
            "Epoch: 001, Step: 163, Loss: 0.6523\n",
            "Epoch: 001, Step: 164, Loss: 0.7766\n",
            "Epoch: 001, Step: 165, Loss: 0.6471\n",
            "Epoch: 001, Step: 166, Loss: 0.7165\n",
            "Epoch: 001, Step: 167, Loss: 0.7474\n",
            "Epoch: 001, Step: 168, Loss: 0.5983\n",
            "Epoch: 001, Step: 169, Loss: 0.6104\n",
            "Epoch: 001, Step: 170, Loss: 0.8230\n",
            "Epoch: 001, Step: 171, Loss: 0.6829\n",
            "Epoch: 001, Step: 172, Loss: 0.7662\n",
            "Epoch: 001, Step: 173, Loss: 0.7224\n",
            "Epoch: 001, Step: 174, Loss: 0.6832\n",
            "Epoch: 001, Step: 175, Loss: 0.6393\n",
            "Epoch: 001, Step: 176, Loss: 0.5545\n",
            "Epoch: 001, Step: 177, Loss: 0.7822\n",
            "Epoch: 001, Step: 178, Loss: 0.6200\n",
            "Epoch: 001, Step: 179, Loss: 0.8041\n",
            "Epoch: 001, Step: 180, Loss: 0.8214\n",
            "Epoch: 001, Step: 181, Loss: 0.7199\n",
            "Epoch: 001, Step: 182, Loss: 0.6135\n",
            "Epoch: 001, Step: 183, Loss: 0.6518\n",
            "Epoch: 001, Step: 184, Loss: 0.7116\n",
            "Epoch: 001, Step: 185, Loss: 0.7216\n",
            "Epoch: 001, Step: 186, Loss: 0.7532\n",
            "Epoch: 001, Step: 187, Loss: 0.7176\n",
            "Epoch: 001, Step: 188, Loss: 0.7245\n",
            "Epoch: 001, Step: 189, Loss: 0.7219\n",
            "Epoch: 001, Step: 190, Loss: 0.7528\n",
            "Epoch: 001, Step: 191, Loss: 0.7151\n",
            "Epoch: 001, Step: 192, Loss: 0.6717\n",
            "Epoch: 001, Step: 193, Loss: 0.6662\n",
            "Epoch: 001, Step: 194, Loss: 0.7033\n",
            "Epoch: 001, Step: 195, Loss: 0.6651\n",
            "Epoch: 001, Step: 196, Loss: 0.7131\n",
            "Epoch: 001, Step: 197, Loss: 0.6777\n",
            "Epoch: 001, Step: 198, Loss: 0.6695\n",
            "Epoch: 001, Step: 199, Loss: 0.7089\n",
            "Epoch: 001, Step: 200, Loss: 0.7502\n",
            "Epoch: 001, Step: 201, Loss: 0.6955\n",
            "Epoch: 001, Step: 202, Loss: 0.7198\n",
            "Epoch: 001, Step: 203, Loss: 0.7038\n",
            "Epoch: 001, Step: 204, Loss: 0.6600\n",
            "Epoch: 001, Step: 205, Loss: 0.6542\n",
            "Epoch: 001, Step: 206, Loss: 0.7313\n",
            "Epoch: 001, Step: 000, Val Loss: 0.6822\n",
            "Epoch: 001, Step: 001, Val Loss: 0.6822\n",
            "Epoch: 001, Step: 002, Val Loss: 0.6933\n",
            "Epoch: 001, Step: 003, Val Loss: 0.6934\n",
            "Epoch: 001, Step: 004, Val Loss: 0.6709\n",
            "Epoch: 001, Step: 005, Val Loss: 0.6934\n",
            "Epoch: 001, Step: 006, Val Loss: 0.6709\n",
            "Epoch: 001, Step: 007, Val Loss: 0.7047\n",
            "Epoch: 001, Step: 008, Val Loss: 0.7046\n",
            "Epoch: 001, Step: 009, Val Loss: 0.6822\n",
            "Epoch: 001, Step: 010, Val Loss: 0.6710\n",
            "Epoch: 001, Step: 011, Val Loss: 0.6822\n",
            "Epoch: 001, Step: 012, Val Loss: 0.6822\n",
            "Epoch: 001, Step: 013, Val Loss: 0.6822\n",
            "Epoch: 001, Step: 014, Val Loss: 0.6709\n",
            "Epoch: 001, Step: 015, Val Loss: 0.6822\n",
            "Epoch: 001, Step: 016, Val Loss: 0.6822\n",
            "Epoch: 001, Step: 017, Val Loss: 0.6934\n",
            "Epoch: 001, Step: 018, Val Loss: 0.6935\n",
            "Epoch: 001, Step: 019, Val Loss: 0.6934\n",
            "Epoch: 001, Step: 020, Val Loss: 0.6934\n",
            "Epoch: 001, Step: 021, Val Loss: 0.6709\n",
            "Epoch: 001, Step: 022, Val Loss: 0.6934\n",
            "Epoch: 001, Step: 023, Val Loss: 0.6934\n",
            "Epoch: 001, Step: 024, Val Loss: 0.6822\n",
            "Epoch: 001, Step: 025, Val Loss: 0.7159\n",
            "Epoch: 001, Step: 026, Val Loss: 0.6934\n",
            "Epoch: 001, Step: 027, Val Loss: 0.6822\n",
            "Epoch: 001, Step: 028, Val Loss: 0.6822\n",
            "Epoch: 001, Step: 029, Val Loss: 0.7046\n",
            "Epoch: 001, Step: 030, Val Loss: 0.6934\n",
            "Epoch: 001, Step: 031, Val Loss: 0.6934\n",
            "Epoch: 001, Step: 032, Val Loss: 0.6822\n",
            "Epoch: 001, Step: 033, Val Loss: 0.6709\n",
            "Epoch: 001, Step: 034, Val Loss: 0.6709\n",
            "Epoch: 001, Step: 035, Val Loss: 0.6934\n",
            "Epoch: 001, Step: 036, Val Loss: 0.6822\n",
            "Epoch: 001, Step: 037, Val Loss: 0.6934\n",
            "Epoch: 001, Step: 038, Val Loss: 0.7046\n",
            "Epoch: 001, Step: 039, Val Loss: 0.6934\n",
            "Epoch: 001, Step: 040, Val Loss: 0.6822\n",
            "Epoch: 001, Step: 041, Val Loss: 0.6934\n",
            "Epoch: 001, Step: 042, Val Loss: 0.6934\n",
            "Epoch: 001, Step: 043, Val Loss: 0.6934\n",
            "Epoch: 001, Step: 044, Val Loss: 0.7046\n",
            "Epoch: 001, Step: 045, Val Loss: 0.6821\n",
            "Epoch: 001, Step: 046, Val Loss: 0.6822\n",
            "Epoch: 001, Step: 047, Val Loss: 0.6934\n",
            "Epoch: 001, Step: 048, Val Loss: 0.7046\n",
            "Epoch: 001, Step: 049, Val Loss: 0.6822\n",
            "Epoch: 001, Step: 050, Val Loss: 0.6934\n",
            "Epoch: 001, Step: 051, Val Loss: 0.7046\n",
            "Epoch: 001, Step: 052, Val Loss: 0.6822\n",
            "Epoch: 001, Step: 053, Val Loss: 0.7159\n",
            "Epoch: 001, Step: 054, Val Loss: 0.6934\n",
            "Epoch: 001, Step: 055, Val Loss: 0.7046\n",
            "Epoch: 001, Step: 056, Val Loss: 0.6934\n",
            "Epoch: 001, Step: 057, Val Loss: 0.7046\n",
            "Epoch: 001, Step: 058, Val Loss: 0.6822\n",
            "Epoch: 002, Step: 000, Loss: 0.6744\n",
            "Epoch: 002, Step: 001, Loss: 0.6691\n",
            "Epoch: 002, Step: 002, Loss: 0.6896\n",
            "Epoch: 002, Step: 003, Loss: 0.6756\n",
            "Epoch: 002, Step: 004, Loss: 0.6840\n",
            "Epoch: 002, Step: 005, Loss: 0.7136\n",
            "Epoch: 002, Step: 006, Loss: 0.7700\n",
            "Epoch: 002, Step: 007, Loss: 0.7202\n",
            "Epoch: 002, Step: 008, Loss: 0.6836\n",
            "Epoch: 002, Step: 009, Loss: 0.7279\n",
            "Epoch: 002, Step: 010, Loss: 0.6901\n",
            "Epoch: 002, Step: 011, Loss: 0.6691\n",
            "Epoch: 002, Step: 012, Loss: 0.6719\n",
            "Epoch: 002, Step: 013, Loss: 0.6619\n",
            "Epoch: 002, Step: 014, Loss: 0.6920\n",
            "Epoch: 002, Step: 015, Loss: 0.7086\n",
            "Epoch: 002, Step: 016, Loss: 0.6902\n",
            "Epoch: 002, Step: 017, Loss: 0.6867\n",
            "Epoch: 002, Step: 018, Loss: 0.7157\n",
            "Epoch: 002, Step: 019, Loss: 0.6945\n",
            "Epoch: 002, Step: 020, Loss: 0.7037\n",
            "Epoch: 002, Step: 021, Loss: 0.6841\n",
            "Epoch: 002, Step: 022, Loss: 0.7398\n",
            "Epoch: 002, Step: 023, Loss: 0.6642\n",
            "Epoch: 002, Step: 024, Loss: 0.6799\n",
            "Epoch: 002, Step: 025, Loss: 0.6635\n",
            "Epoch: 002, Step: 026, Loss: 0.6768\n",
            "Epoch: 002, Step: 027, Loss: 0.6853\n",
            "Epoch: 002, Step: 028, Loss: 0.7043\n",
            "Epoch: 002, Step: 029, Loss: 0.6212\n",
            "Epoch: 002, Step: 030, Loss: 0.6956\n",
            "Epoch: 002, Step: 031, Loss: 0.7253\n",
            "Epoch: 002, Step: 032, Loss: 0.8076\n",
            "Epoch: 002, Step: 033, Loss: 0.6905\n",
            "Epoch: 002, Step: 034, Loss: 0.6954\n",
            "Epoch: 002, Step: 035, Loss: 0.7130\n",
            "Epoch: 002, Step: 036, Loss: 0.6058\n",
            "Epoch: 002, Step: 037, Loss: 0.7087\n",
            "Epoch: 002, Step: 038, Loss: 0.7047\n",
            "Epoch: 002, Step: 039, Loss: 0.8901\n",
            "Epoch: 002, Step: 040, Loss: 0.7636\n",
            "Epoch: 002, Step: 041, Loss: 0.6619\n",
            "Epoch: 002, Step: 042, Loss: 0.6438\n",
            "Epoch: 002, Step: 043, Loss: 0.5603\n",
            "Epoch: 002, Step: 044, Loss: 0.7095\n",
            "Epoch: 002, Step: 045, Loss: 0.7050\n",
            "Epoch: 002, Step: 046, Loss: 0.6995\n",
            "Epoch: 002, Step: 047, Loss: 0.6312\n",
            "Epoch: 002, Step: 048, Loss: 0.6535\n",
            "Epoch: 002, Step: 049, Loss: 0.6435\n",
            "Epoch: 002, Step: 050, Loss: 0.6827\n",
            "Epoch: 002, Step: 051, Loss: 0.5680\n",
            "Epoch: 002, Step: 052, Loss: 0.7806\n",
            "Epoch: 002, Step: 053, Loss: 0.7098\n",
            "Epoch: 002, Step: 054, Loss: 0.5766\n",
            "Epoch: 002, Step: 055, Loss: 0.6917\n",
            "Epoch: 002, Step: 056, Loss: 0.7085\n",
            "Epoch: 002, Step: 057, Loss: 0.6188\n",
            "Epoch: 002, Step: 058, Loss: 0.6913\n",
            "Epoch: 002, Step: 059, Loss: 0.7245\n",
            "Epoch: 002, Step: 060, Loss: 0.8129\n",
            "Epoch: 002, Step: 061, Loss: 0.6077\n",
            "Epoch: 002, Step: 062, Loss: 0.7009\n",
            "Epoch: 002, Step: 063, Loss: 0.8193\n",
            "Epoch: 002, Step: 064, Loss: 0.8665\n",
            "Epoch: 002, Step: 065, Loss: 0.6887\n",
            "Epoch: 002, Step: 066, Loss: 0.6079\n",
            "Epoch: 002, Step: 067, Loss: 0.6981\n",
            "Epoch: 002, Step: 068, Loss: 0.6166\n",
            "Epoch: 002, Step: 069, Loss: 0.6499\n",
            "Epoch: 002, Step: 070, Loss: 0.7085\n",
            "Epoch: 002, Step: 071, Loss: 0.5718\n",
            "Epoch: 002, Step: 072, Loss: 0.7499\n",
            "Epoch: 002, Step: 073, Loss: 0.7161\n",
            "Epoch: 002, Step: 074, Loss: 0.7589\n",
            "Epoch: 002, Step: 075, Loss: 0.6844\n",
            "Epoch: 002, Step: 076, Loss: 0.6231\n",
            "Epoch: 002, Step: 077, Loss: 0.6623\n",
            "Epoch: 002, Step: 078, Loss: 0.6867\n",
            "Epoch: 002, Step: 079, Loss: 0.6157\n",
            "Epoch: 002, Step: 080, Loss: 0.6908\n",
            "Epoch: 002, Step: 081, Loss: 0.7672\n",
            "Epoch: 002, Step: 082, Loss: 0.7719\n",
            "Epoch: 002, Step: 083, Loss: 0.7434\n",
            "Epoch: 002, Step: 084, Loss: 0.6596\n",
            "Epoch: 002, Step: 085, Loss: 0.6011\n",
            "Epoch: 002, Step: 086, Loss: 0.7488\n",
            "Epoch: 002, Step: 087, Loss: 0.6422\n",
            "Epoch: 002, Step: 088, Loss: 0.5895\n",
            "Epoch: 002, Step: 089, Loss: 0.6888\n",
            "Epoch: 002, Step: 090, Loss: 0.7149\n",
            "Epoch: 002, Step: 091, Loss: 0.7070\n",
            "Epoch: 002, Step: 092, Loss: 0.7052\n",
            "Epoch: 002, Step: 093, Loss: 0.6322\n",
            "Epoch: 002, Step: 094, Loss: 0.5683\n",
            "Epoch: 002, Step: 095, Loss: 0.6252\n",
            "Epoch: 002, Step: 096, Loss: 0.7051\n",
            "Epoch: 002, Step: 097, Loss: 0.7246\n",
            "Epoch: 002, Step: 098, Loss: 0.6267\n",
            "Epoch: 002, Step: 099, Loss: 0.6642\n",
            "Epoch: 002, Step: 100, Loss: 0.6961\n",
            "Epoch: 002, Step: 101, Loss: 0.6152\n",
            "Epoch: 002, Step: 102, Loss: 0.8565\n",
            "Epoch: 002, Step: 103, Loss: 0.7042\n",
            "Epoch: 002, Step: 104, Loss: 0.5615\n",
            "Epoch: 002, Step: 105, Loss: 0.6731\n",
            "Epoch: 002, Step: 106, Loss: 0.6481\n",
            "Epoch: 002, Step: 107, Loss: 0.7508\n",
            "Epoch: 002, Step: 108, Loss: 0.5730\n",
            "Epoch: 002, Step: 109, Loss: 0.7388\n",
            "Epoch: 002, Step: 110, Loss: 0.5488\n",
            "Epoch: 002, Step: 111, Loss: 0.6090\n",
            "Epoch: 002, Step: 112, Loss: 0.7148\n",
            "Epoch: 002, Step: 113, Loss: 0.6198\n",
            "Epoch: 002, Step: 114, Loss: 0.8087\n",
            "Epoch: 002, Step: 115, Loss: 0.7362\n",
            "Epoch: 002, Step: 116, Loss: 0.8495\n",
            "Epoch: 002, Step: 117, Loss: 0.5876\n",
            "Epoch: 002, Step: 118, Loss: 0.6207\n",
            "Epoch: 002, Step: 119, Loss: 0.4822\n",
            "Epoch: 002, Step: 120, Loss: 0.5886\n",
            "Epoch: 002, Step: 121, Loss: 0.6083\n",
            "Epoch: 002, Step: 122, Loss: 0.5122\n",
            "Epoch: 002, Step: 123, Loss: 0.4696\n",
            "Epoch: 002, Step: 124, Loss: 0.4782\n",
            "Epoch: 002, Step: 125, Loss: 0.8998\n",
            "Epoch: 002, Step: 126, Loss: 0.7281\n",
            "Epoch: 002, Step: 127, Loss: 0.7551\n",
            "Epoch: 002, Step: 128, Loss: 0.6944\n",
            "Epoch: 002, Step: 129, Loss: 0.7302\n",
            "Epoch: 002, Step: 130, Loss: 0.6214\n",
            "Epoch: 002, Step: 131, Loss: 1.0955\n",
            "Epoch: 002, Step: 132, Loss: 0.7183\n",
            "Epoch: 002, Step: 133, Loss: 0.6215\n",
            "Epoch: 002, Step: 134, Loss: 0.7504\n",
            "Epoch: 002, Step: 135, Loss: 0.7379\n",
            "Epoch: 002, Step: 136, Loss: 0.5672\n",
            "Epoch: 002, Step: 137, Loss: 0.7179\n",
            "Epoch: 002, Step: 138, Loss: 0.7212\n",
            "Epoch: 002, Step: 139, Loss: 0.8809\n",
            "Epoch: 002, Step: 140, Loss: 0.4812\n",
            "Epoch: 002, Step: 141, Loss: 0.6123\n",
            "Epoch: 002, Step: 142, Loss: 0.5211\n",
            "Epoch: 002, Step: 143, Loss: 0.8612\n",
            "Epoch: 002, Step: 144, Loss: 0.7398\n",
            "Epoch: 002, Step: 145, Loss: 0.8064\n",
            "Epoch: 002, Step: 146, Loss: 0.6586\n",
            "Epoch: 002, Step: 147, Loss: 0.7017\n",
            "Epoch: 002, Step: 148, Loss: 0.6251\n",
            "Epoch: 002, Step: 149, Loss: 0.6384\n",
            "Epoch: 002, Step: 150, Loss: 0.7464\n",
            "Epoch: 002, Step: 151, Loss: 0.6242\n",
            "Epoch: 002, Step: 152, Loss: 0.6303\n",
            "Epoch: 002, Step: 153, Loss: 0.6295\n",
            "Epoch: 002, Step: 154, Loss: 0.8184\n",
            "Epoch: 002, Step: 155, Loss: 0.8698\n",
            "Epoch: 002, Step: 156, Loss: 0.7012\n",
            "Epoch: 002, Step: 157, Loss: 0.7983\n",
            "Epoch: 002, Step: 158, Loss: 0.7092\n",
            "Epoch: 002, Step: 159, Loss: 0.6396\n",
            "Epoch: 002, Step: 160, Loss: 0.7592\n",
            "Epoch: 002, Step: 161, Loss: 0.5828\n",
            "Epoch: 002, Step: 162, Loss: 0.7682\n",
            "Epoch: 002, Step: 163, Loss: 0.7481\n",
            "Epoch: 002, Step: 164, Loss: 0.7653\n",
            "Epoch: 002, Step: 165, Loss: 0.7215\n",
            "Epoch: 002, Step: 166, Loss: 0.7059\n",
            "Epoch: 002, Step: 167, Loss: 0.6909\n",
            "Epoch: 002, Step: 168, Loss: 0.6975\n",
            "Epoch: 002, Step: 169, Loss: 0.6869\n",
            "Epoch: 002, Step: 170, Loss: 0.6674\n",
            "Epoch: 002, Step: 171, Loss: 0.6738\n",
            "Epoch: 002, Step: 172, Loss: 0.6769\n",
            "Epoch: 002, Step: 173, Loss: 0.7066\n",
            "Epoch: 002, Step: 174, Loss: 0.6793\n",
            "Epoch: 002, Step: 175, Loss: 0.7141\n",
            "Epoch: 002, Step: 176, Loss: 0.6655\n",
            "Epoch: 002, Step: 177, Loss: 0.6939\n",
            "Epoch: 002, Step: 178, Loss: 0.6987\n",
            "Epoch: 002, Step: 179, Loss: 0.6943\n",
            "Epoch: 002, Step: 180, Loss: 0.7191\n",
            "Epoch: 002, Step: 181, Loss: 0.6501\n",
            "Epoch: 002, Step: 182, Loss: 0.6979\n",
            "Epoch: 002, Step: 183, Loss: 0.7011\n",
            "Epoch: 002, Step: 184, Loss: 0.6695\n",
            "Epoch: 002, Step: 185, Loss: 0.6709\n",
            "Epoch: 002, Step: 186, Loss: 0.7379\n",
            "Epoch: 002, Step: 187, Loss: 0.7291\n",
            "Epoch: 002, Step: 188, Loss: 0.6880\n",
            "Epoch: 002, Step: 189, Loss: 0.7147\n",
            "Epoch: 002, Step: 190, Loss: 0.6125\n",
            "Epoch: 002, Step: 191, Loss: 0.6695\n",
            "Epoch: 002, Step: 192, Loss: 0.7688\n",
            "Epoch: 002, Step: 193, Loss: 0.6900\n",
            "Epoch: 002, Step: 194, Loss: 0.6375\n",
            "Epoch: 002, Step: 195, Loss: 0.6981\n",
            "Epoch: 002, Step: 196, Loss: 0.6327\n",
            "Epoch: 002, Step: 197, Loss: 0.6327\n",
            "Epoch: 002, Step: 198, Loss: 0.7088\n",
            "Epoch: 002, Step: 199, Loss: 0.7042\n",
            "Epoch: 002, Step: 200, Loss: 0.5356\n",
            "Epoch: 002, Step: 201, Loss: 0.5355\n",
            "Epoch: 002, Step: 202, Loss: 0.6234\n",
            "Epoch: 002, Step: 203, Loss: 0.7876\n",
            "Epoch: 002, Step: 204, Loss: 0.7209\n",
            "Epoch: 002, Step: 205, Loss: 0.6355\n",
            "Epoch: 002, Step: 206, Loss: 0.5250\n",
            "Epoch: 002, Step: 000, Val Loss: 0.8358\n",
            "Epoch: 002, Step: 001, Val Loss: 0.4875\n",
            "Epoch: 002, Step: 002, Val Loss: 0.6044\n",
            "Epoch: 002, Step: 003, Val Loss: 0.6037\n",
            "Epoch: 002, Step: 004, Val Loss: 0.7199\n",
            "Epoch: 002, Step: 005, Val Loss: 0.6037\n",
            "Epoch: 002, Step: 006, Val Loss: 0.6037\n",
            "Epoch: 002, Step: 007, Val Loss: 0.7199\n",
            "Epoch: 002, Step: 008, Val Loss: 0.6037\n",
            "Epoch: 002, Step: 009, Val Loss: 0.8361\n",
            "Epoch: 002, Step: 010, Val Loss: 0.7199\n",
            "Epoch: 002, Step: 011, Val Loss: 0.6037\n",
            "Epoch: 002, Step: 012, Val Loss: 0.7199\n",
            "Epoch: 002, Step: 013, Val Loss: 0.7199\n",
            "Epoch: 002, Step: 014, Val Loss: 0.6021\n",
            "Epoch: 002, Step: 015, Val Loss: 0.7199\n",
            "Epoch: 002, Step: 016, Val Loss: 0.4875\n",
            "Epoch: 002, Step: 017, Val Loss: 0.6043\n",
            "Epoch: 002, Step: 018, Val Loss: 0.7199\n",
            "Epoch: 002, Step: 019, Val Loss: 0.6037\n",
            "Epoch: 002, Step: 020, Val Loss: 0.7199\n",
            "Epoch: 002, Step: 021, Val Loss: 0.6043\n",
            "Epoch: 002, Step: 022, Val Loss: 0.7199\n",
            "Epoch: 002, Step: 023, Val Loss: 0.4875\n",
            "Epoch: 002, Step: 024, Val Loss: 0.7199\n",
            "Epoch: 002, Step: 025, Val Loss: 0.6037\n",
            "Epoch: 002, Step: 026, Val Loss: 0.6037\n",
            "Epoch: 002, Step: 027, Val Loss: 0.6037\n",
            "Epoch: 002, Step: 028, Val Loss: 0.7199\n",
            "Epoch: 002, Step: 029, Val Loss: 0.7199\n",
            "Epoch: 002, Step: 030, Val Loss: 0.8355\n",
            "Epoch: 002, Step: 031, Val Loss: 0.7199\n",
            "Epoch: 002, Step: 032, Val Loss: 0.7199\n",
            "Epoch: 002, Step: 033, Val Loss: 0.6037\n",
            "Epoch: 002, Step: 034, Val Loss: 0.7199\n",
            "Epoch: 002, Step: 035, Val Loss: 0.6037\n",
            "Epoch: 002, Step: 036, Val Loss: 0.6037\n",
            "Epoch: 002, Step: 037, Val Loss: 0.7199\n",
            "Epoch: 002, Step: 038, Val Loss: 0.7199\n",
            "Epoch: 002, Step: 039, Val Loss: 0.9523\n",
            "Epoch: 002, Step: 040, Val Loss: 0.8361\n",
            "Epoch: 002, Step: 041, Val Loss: 0.7192\n",
            "Epoch: 002, Step: 042, Val Loss: 0.7199\n",
            "Epoch: 002, Step: 043, Val Loss: 0.7199\n",
            "Epoch: 002, Step: 044, Val Loss: 0.4875\n",
            "Epoch: 002, Step: 045, Val Loss: 0.7199\n",
            "Epoch: 002, Step: 046, Val Loss: 0.7199\n",
            "Epoch: 002, Step: 047, Val Loss: 0.7202\n",
            "Epoch: 002, Step: 048, Val Loss: 0.8361\n",
            "Epoch: 002, Step: 049, Val Loss: 0.8361\n",
            "Epoch: 002, Step: 050, Val Loss: 0.7199\n",
            "Epoch: 002, Step: 051, Val Loss: 0.7199\n",
            "Epoch: 002, Step: 052, Val Loss: 0.4875\n",
            "Epoch: 002, Step: 053, Val Loss: 0.6037\n",
            "Epoch: 002, Step: 054, Val Loss: 0.4875\n",
            "Epoch: 002, Step: 055, Val Loss: 0.7199\n",
            "Epoch: 002, Step: 056, Val Loss: 0.7199\n",
            "Epoch: 002, Step: 057, Val Loss: 0.8361\n",
            "Epoch: 002, Step: 058, Val Loss: 0.7199\n",
            "Epoch: 003, Step: 000, Loss: 0.8390\n",
            "Epoch: 003, Step: 001, Loss: 0.8021\n",
            "Epoch: 003, Step: 002, Loss: 0.5947\n",
            "Epoch: 003, Step: 003, Loss: 0.7217\n",
            "Epoch: 003, Step: 004, Loss: 0.6187\n",
            "Epoch: 003, Step: 005, Loss: 0.5682\n",
            "Epoch: 003, Step: 006, Loss: 0.7177\n",
            "Epoch: 003, Step: 007, Loss: 0.4488\n",
            "Epoch: 003, Step: 008, Loss: 0.7368\n",
            "Epoch: 003, Step: 009, Loss: 0.5010\n",
            "Epoch: 003, Step: 010, Loss: 0.5995\n",
            "Epoch: 003, Step: 011, Loss: 0.7554\n",
            "Epoch: 003, Step: 012, Loss: 0.8636\n",
            "Epoch: 003, Step: 013, Loss: 0.7031\n",
            "Epoch: 003, Step: 014, Loss: 0.7509\n",
            "Epoch: 003, Step: 015, Loss: 0.7119\n",
            "Epoch: 003, Step: 016, Loss: 0.7172\n",
            "Epoch: 003, Step: 017, Loss: 0.8191\n",
            "Epoch: 003, Step: 018, Loss: 0.7042\n",
            "Epoch: 003, Step: 019, Loss: 0.7308\n",
            "Epoch: 003, Step: 020, Loss: 0.7351\n",
            "Epoch: 003, Step: 021, Loss: 0.5016\n",
            "Epoch: 003, Step: 022, Loss: 0.6396\n",
            "Epoch: 003, Step: 023, Loss: 0.7183\n",
            "Epoch: 003, Step: 024, Loss: 0.7614\n",
            "Epoch: 003, Step: 025, Loss: 0.6309\n",
            "Epoch: 003, Step: 026, Loss: 0.7152\n",
            "Epoch: 003, Step: 027, Loss: 0.6452\n",
            "Epoch: 003, Step: 028, Loss: 0.6981\n",
            "Epoch: 003, Step: 029, Loss: 0.6535\n",
            "Epoch: 003, Step: 030, Loss: 0.6004\n",
            "Epoch: 003, Step: 031, Loss: 0.5101\n",
            "Epoch: 003, Step: 032, Loss: 0.5048\n",
            "Epoch: 003, Step: 033, Loss: 0.4992\n",
            "Epoch: 003, Step: 034, Loss: 0.7254\n",
            "Epoch: 003, Step: 035, Loss: 0.7316\n",
            "Epoch: 003, Step: 036, Loss: 0.7436\n",
            "Epoch: 003, Step: 037, Loss: 0.8579\n",
            "Epoch: 003, Step: 038, Loss: 0.4876\n",
            "Epoch: 003, Step: 039, Loss: 0.6101\n",
            "Epoch: 003, Step: 040, Loss: 0.7574\n",
            "Epoch: 003, Step: 041, Loss: 0.7227\n",
            "Epoch: 003, Step: 042, Loss: 0.5721\n",
            "Epoch: 003, Step: 043, Loss: 0.7183\n",
            "Epoch: 003, Step: 044, Loss: 0.6206\n",
            "Epoch: 003, Step: 045, Loss: 0.7160\n",
            "Epoch: 003, Step: 046, Loss: 0.7125\n",
            "Epoch: 003, Step: 047, Loss: 0.6122\n",
            "Epoch: 003, Step: 048, Loss: 0.6093\n",
            "Epoch: 003, Step: 049, Loss: 0.7041\n",
            "Epoch: 003, Step: 050, Loss: 0.5751\n",
            "Epoch: 003, Step: 051, Loss: 0.6996\n",
            "Epoch: 003, Step: 052, Loss: 0.8827\n",
            "Epoch: 003, Step: 053, Loss: 0.9816\n",
            "Epoch: 003, Step: 054, Loss: 0.8847\n",
            "Epoch: 003, Step: 055, Loss: 0.8337\n",
            "Epoch: 003, Step: 056, Loss: 0.4865\n",
            "Epoch: 003, Step: 057, Loss: 0.7328\n",
            "Epoch: 003, Step: 058, Loss: 0.6062\n",
            "Epoch: 003, Step: 059, Loss: 0.7856\n",
            "Epoch: 003, Step: 060, Loss: 0.6169\n",
            "Epoch: 003, Step: 061, Loss: 0.6872\n",
            "Epoch: 003, Step: 062, Loss: 0.6309\n",
            "Epoch: 003, Step: 063, Loss: 0.6952\n",
            "Epoch: 003, Step: 064, Loss: 0.6980\n",
            "Epoch: 003, Step: 065, Loss: 0.8674\n",
            "Epoch: 003, Step: 066, Loss: 0.5456\n",
            "Epoch: 003, Step: 067, Loss: 0.6320\n",
            "Epoch: 003, Step: 068, Loss: 0.7164\n",
            "Epoch: 003, Step: 069, Loss: 0.6407\n",
            "Epoch: 003, Step: 070, Loss: 0.6290\n",
            "Epoch: 003, Step: 071, Loss: 0.6545\n",
            "Epoch: 003, Step: 072, Loss: 0.6951\n",
            "Epoch: 003, Step: 073, Loss: 0.7223\n",
            "Epoch: 003, Step: 074, Loss: 0.7177\n",
            "Epoch: 003, Step: 075, Loss: 0.6979\n",
            "Epoch: 003, Step: 076, Loss: 0.7115\n",
            "Epoch: 003, Step: 077, Loss: 0.5690\n",
            "Epoch: 003, Step: 078, Loss: 0.7731\n",
            "Epoch: 003, Step: 079, Loss: 0.6161\n",
            "Epoch: 003, Step: 080, Loss: 0.7888\n",
            "Epoch: 003, Step: 081, Loss: 0.7007\n",
            "Epoch: 003, Step: 082, Loss: 0.7028\n",
            "Epoch: 003, Step: 083, Loss: 0.6463\n",
            "Epoch: 003, Step: 084, Loss: 0.5634\n",
            "Epoch: 003, Step: 085, Loss: 0.8434\n",
            "Epoch: 003, Step: 086, Loss: 0.6401\n",
            "Epoch: 003, Step: 087, Loss: 0.6257\n",
            "Epoch: 003, Step: 088, Loss: 0.7075\n",
            "Epoch: 003, Step: 089, Loss: 0.7728\n",
            "Epoch: 003, Step: 090, Loss: 0.6348\n",
            "Epoch: 003, Step: 091, Loss: 0.7198\n",
            "Epoch: 003, Step: 092, Loss: 0.7022\n",
            "Epoch: 003, Step: 093, Loss: 0.6461\n",
            "Epoch: 003, Step: 094, Loss: 0.6429\n",
            "Epoch: 003, Step: 095, Loss: 0.6400\n",
            "Epoch: 003, Step: 096, Loss: 0.6965\n",
            "Epoch: 003, Step: 097, Loss: 0.7087\n",
            "Epoch: 003, Step: 098, Loss: 0.7129\n",
            "Epoch: 003, Step: 099, Loss: 0.6288\n",
            "Epoch: 003, Step: 100, Loss: 0.6380\n",
            "Epoch: 003, Step: 101, Loss: 0.7292\n",
            "Epoch: 003, Step: 102, Loss: 0.6436\n",
            "Epoch: 003, Step: 103, Loss: 0.6902\n",
            "Epoch: 003, Step: 104, Loss: 0.6380\n",
            "Epoch: 003, Step: 105, Loss: 0.7017\n",
            "Epoch: 003, Step: 106, Loss: 0.5598\n",
            "Epoch: 003, Step: 107, Loss: 0.5700\n",
            "Epoch: 003, Step: 108, Loss: 0.7076\n",
            "Epoch: 003, Step: 109, Loss: 0.6123\n",
            "Epoch: 003, Step: 110, Loss: 0.7097\n",
            "Epoch: 003, Step: 111, Loss: 0.6272\n",
            "Epoch: 003, Step: 112, Loss: 0.7138\n",
            "Epoch: 003, Step: 113, Loss: 0.8054\n",
            "Epoch: 003, Step: 114, Loss: 0.6016\n",
            "Epoch: 003, Step: 115, Loss: 0.6112\n",
            "Epoch: 003, Step: 116, Loss: 0.7141\n",
            "Epoch: 003, Step: 117, Loss: 0.6141\n",
            "Epoch: 003, Step: 118, Loss: 0.7095\n",
            "Epoch: 003, Step: 119, Loss: 0.7111\n",
            "Epoch: 003, Step: 120, Loss: 0.8284\n",
            "Epoch: 003, Step: 121, Loss: 0.7137\n",
            "Epoch: 003, Step: 122, Loss: 0.6253\n",
            "Epoch: 003, Step: 123, Loss: 0.5077\n",
            "Epoch: 003, Step: 124, Loss: 0.6228\n",
            "Epoch: 003, Step: 125, Loss: 0.7241\n",
            "Epoch: 003, Step: 126, Loss: 0.7255\n",
            "Epoch: 003, Step: 127, Loss: 0.5863\n",
            "Epoch: 003, Step: 128, Loss: 0.8609\n",
            "Epoch: 003, Step: 129, Loss: 0.7203\n",
            "Epoch: 003, Step: 130, Loss: 0.6183\n",
            "Epoch: 003, Step: 131, Loss: 0.5146\n",
            "Epoch: 003, Step: 132, Loss: 0.6930\n",
            "Epoch: 003, Step: 133, Loss: 0.9245\n",
            "Epoch: 003, Step: 134, Loss: 0.7375\n",
            "Epoch: 003, Step: 135, Loss: 0.7139\n",
            "Epoch: 003, Step: 136, Loss: 0.6183\n",
            "Epoch: 003, Step: 137, Loss: 0.6108\n",
            "Epoch: 003, Step: 138, Loss: 0.8952\n",
            "Epoch: 003, Step: 139, Loss: 0.7095\n",
            "Epoch: 003, Step: 140, Loss: 0.6275\n",
            "Epoch: 003, Step: 141, Loss: 0.6172\n",
            "Epoch: 003, Step: 142, Loss: 0.7947\n",
            "Epoch: 003, Step: 143, Loss: 0.6367\n",
            "Epoch: 003, Step: 144, Loss: 0.7861\n",
            "Epoch: 003, Step: 145, Loss: 0.7090\n",
            "Epoch: 003, Step: 146, Loss: 0.6236\n",
            "Epoch: 003, Step: 147, Loss: 0.7726\n",
            "Epoch: 003, Step: 148, Loss: 0.7024\n",
            "Epoch: 003, Step: 149, Loss: 0.7029\n",
            "Epoch: 003, Step: 150, Loss: 0.7660\n",
            "Epoch: 003, Step: 151, Loss: 0.6991\n",
            "Epoch: 003, Step: 152, Loss: 0.8109\n",
            "Epoch: 003, Step: 153, Loss: 0.6980\n",
            "Epoch: 003, Step: 154, Loss: 0.6631\n",
            "Epoch: 003, Step: 155, Loss: 0.6839\n",
            "Epoch: 003, Step: 156, Loss: 0.7080\n",
            "Epoch: 003, Step: 157, Loss: 0.6797\n",
            "Epoch: 003, Step: 158, Loss: 0.6682\n",
            "Epoch: 003, Step: 159, Loss: 0.6330\n",
            "Epoch: 003, Step: 160, Loss: 0.7306\n",
            "Epoch: 003, Step: 161, Loss: 0.7146\n",
            "Epoch: 003, Step: 162, Loss: 0.7312\n",
            "Epoch: 003, Step: 163, Loss: 0.7203\n",
            "Epoch: 003, Step: 164, Loss: 0.6783\n",
            "Epoch: 003, Step: 165, Loss: 0.6976\n",
            "Epoch: 003, Step: 166, Loss: 0.6858\n",
            "Epoch: 003, Step: 167, Loss: 0.6660\n",
            "Epoch: 003, Step: 168, Loss: 0.7330\n",
            "Epoch: 003, Step: 169, Loss: 0.7204\n",
            "Epoch: 003, Step: 170, Loss: 0.6893\n",
            "Epoch: 003, Step: 171, Loss: 0.6575\n",
            "Epoch: 003, Step: 172, Loss: 0.7197\n",
            "Epoch: 003, Step: 173, Loss: 0.6616\n",
            "Epoch: 003, Step: 174, Loss: 0.7011\n",
            "Epoch: 003, Step: 175, Loss: 0.7055\n",
            "Epoch: 003, Step: 176, Loss: 0.6756\n",
            "Epoch: 003, Step: 177, Loss: 0.6795\n",
            "Epoch: 003, Step: 178, Loss: 0.7059\n",
            "Epoch: 003, Step: 179, Loss: 0.6807\n",
            "Epoch: 003, Step: 180, Loss: 0.6947\n",
            "Epoch: 003, Step: 181, Loss: 0.7198\n",
            "Epoch: 003, Step: 182, Loss: 0.6318\n",
            "Epoch: 003, Step: 183, Loss: 0.6658\n",
            "Epoch: 003, Step: 184, Loss: 0.6503\n",
            "Epoch: 003, Step: 185, Loss: 0.7524\n",
            "Epoch: 003, Step: 186, Loss: 0.6988\n",
            "Epoch: 003, Step: 187, Loss: 0.6533\n",
            "Epoch: 003, Step: 188, Loss: 0.6494\n",
            "Epoch: 003, Step: 189, Loss: 0.6903\n",
            "Epoch: 003, Step: 190, Loss: 0.6809\n",
            "Epoch: 003, Step: 191, Loss: 0.6955\n",
            "Epoch: 003, Step: 192, Loss: 0.6453\n",
            "Epoch: 003, Step: 193, Loss: 0.6412\n",
            "Epoch: 003, Step: 194, Loss: 0.6320\n",
            "Epoch: 003, Step: 195, Loss: 0.5689\n",
            "Epoch: 003, Step: 196, Loss: 0.5459\n",
            "Epoch: 003, Step: 197, Loss: 0.7156\n",
            "Epoch: 003, Step: 198, Loss: 0.8002\n",
            "Epoch: 003, Step: 199, Loss: 0.8260\n",
            "Epoch: 003, Step: 200, Loss: 0.6732\n",
            "Epoch: 003, Step: 201, Loss: 0.6059\n",
            "Epoch: 003, Step: 202, Loss: 0.6817\n",
            "Epoch: 003, Step: 203, Loss: 0.6003\n",
            "Epoch: 003, Step: 204, Loss: 0.7337\n",
            "Epoch: 003, Step: 205, Loss: 0.6355\n",
            "Epoch: 003, Step: 206, Loss: 0.4338\n",
            "Epoch: 003, Step: 000, Val Loss: 0.7206\n",
            "Epoch: 003, Step: 001, Val Loss: 0.4854\n",
            "Epoch: 003, Step: 002, Val Loss: 0.7206\n",
            "Epoch: 003, Step: 003, Val Loss: 0.7209\n",
            "Epoch: 003, Step: 004, Val Loss: 0.4851\n",
            "Epoch: 003, Step: 005, Val Loss: 0.4853\n",
            "Epoch: 003, Step: 006, Val Loss: 0.4850\n",
            "Epoch: 003, Step: 007, Val Loss: 0.8382\n",
            "Epoch: 003, Step: 008, Val Loss: 0.4872\n",
            "Epoch: 003, Step: 009, Val Loss: 0.4850\n",
            "Epoch: 003, Step: 010, Val Loss: 0.6034\n",
            "Epoch: 003, Step: 011, Val Loss: 0.6025\n",
            "Epoch: 003, Step: 012, Val Loss: 0.6029\n",
            "Epoch: 003, Step: 013, Val Loss: 0.4854\n",
            "Epoch: 003, Step: 014, Val Loss: 0.6030\n",
            "Epoch: 003, Step: 015, Val Loss: 0.7208\n",
            "Epoch: 003, Step: 016, Val Loss: 0.8379\n",
            "Epoch: 003, Step: 017, Val Loss: 0.8379\n",
            "Epoch: 003, Step: 018, Val Loss: 0.7206\n",
            "Epoch: 003, Step: 019, Val Loss: 0.7206\n",
            "Epoch: 003, Step: 020, Val Loss: 0.6030\n",
            "Epoch: 003, Step: 021, Val Loss: 0.7206\n",
            "Epoch: 003, Step: 022, Val Loss: 0.8381\n",
            "Epoch: 003, Step: 023, Val Loss: 0.7201\n",
            "Epoch: 003, Step: 024, Val Loss: 0.7203\n",
            "Epoch: 003, Step: 025, Val Loss: 0.7155\n",
            "Epoch: 003, Step: 026, Val Loss: 0.4852\n",
            "Epoch: 003, Step: 027, Val Loss: 0.9563\n",
            "Epoch: 003, Step: 028, Val Loss: 0.8386\n",
            "Epoch: 003, Step: 029, Val Loss: 0.7196\n",
            "Epoch: 003, Step: 030, Val Loss: 0.9562\n",
            "Epoch: 003, Step: 031, Val Loss: 0.6049\n",
            "Epoch: 003, Step: 032, Val Loss: 0.7207\n",
            "Epoch: 003, Step: 033, Val Loss: 0.4852\n",
            "Epoch: 003, Step: 034, Val Loss: 0.6032\n",
            "Epoch: 003, Step: 035, Val Loss: 0.6036\n",
            "Epoch: 003, Step: 036, Val Loss: 0.7207\n",
            "Epoch: 003, Step: 037, Val Loss: 0.6028\n",
            "Epoch: 003, Step: 038, Val Loss: 0.7209\n",
            "Epoch: 003, Step: 039, Val Loss: 0.6025\n",
            "Epoch: 003, Step: 040, Val Loss: 0.8360\n",
            "Epoch: 003, Step: 041, Val Loss: 0.6052\n",
            "Epoch: 003, Step: 042, Val Loss: 0.7206\n",
            "Epoch: 003, Step: 043, Val Loss: 0.4852\n",
            "Epoch: 003, Step: 044, Val Loss: 0.4849\n",
            "Epoch: 003, Step: 045, Val Loss: 0.8380\n",
            "Epoch: 003, Step: 046, Val Loss: 0.6031\n",
            "Epoch: 003, Step: 047, Val Loss: 0.7205\n",
            "Epoch: 003, Step: 048, Val Loss: 0.7208\n",
            "Epoch: 003, Step: 049, Val Loss: 0.6029\n",
            "Epoch: 003, Step: 050, Val Loss: 0.6031\n",
            "Epoch: 003, Step: 051, Val Loss: 0.8384\n",
            "Epoch: 003, Step: 052, Val Loss: 0.7204\n",
            "Epoch: 003, Step: 053, Val Loss: 0.8366\n",
            "Epoch: 003, Step: 054, Val Loss: 0.9557\n",
            "Epoch: 003, Step: 055, Val Loss: 0.4865\n",
            "Epoch: 003, Step: 056, Val Loss: 0.8382\n",
            "Epoch: 003, Step: 057, Val Loss: 0.8378\n",
            "Epoch: 003, Step: 058, Val Loss: 0.8373\n",
            "Epoch: 004, Step: 000, Loss: 1.0075\n",
            "Epoch: 004, Step: 001, Loss: 0.6856\n",
            "Epoch: 004, Step: 002, Loss: 0.6219\n",
            "Epoch: 004, Step: 003, Loss: 0.6079\n",
            "Epoch: 004, Step: 004, Loss: 0.5955\n",
            "Epoch: 004, Step: 005, Loss: 0.8380\n",
            "Epoch: 004, Step: 006, Loss: 0.5811\n",
            "Epoch: 004, Step: 007, Loss: 0.6289\n",
            "Epoch: 004, Step: 008, Loss: 0.5979\n",
            "Epoch: 004, Step: 009, Loss: 0.6930\n",
            "Epoch: 004, Step: 010, Loss: 0.7411\n",
            "Epoch: 004, Step: 011, Loss: 0.5883\n",
            "Epoch: 004, Step: 012, Loss: 0.6005\n",
            "Epoch: 004, Step: 013, Loss: 0.6746\n",
            "Epoch: 004, Step: 014, Loss: 0.4797\n",
            "Epoch: 004, Step: 015, Loss: 0.5798\n",
            "Epoch: 004, Step: 016, Loss: 0.6063\n",
            "Epoch: 004, Step: 017, Loss: 0.8692\n",
            "Epoch: 004, Step: 018, Loss: 0.7289\n",
            "Epoch: 004, Step: 019, Loss: 0.4392\n",
            "Epoch: 004, Step: 020, Loss: 0.7186\n",
            "Epoch: 004, Step: 021, Loss: 0.7772\n",
            "Epoch: 004, Step: 022, Loss: 0.6810\n",
            "Epoch: 004, Step: 023, Loss: 0.7660\n",
            "Epoch: 004, Step: 024, Loss: 0.6842\n",
            "Epoch: 004, Step: 025, Loss: 0.7527\n",
            "Epoch: 004, Step: 026, Loss: 0.6796\n",
            "Epoch: 004, Step: 027, Loss: 0.9569\n",
            "Epoch: 004, Step: 028, Loss: 0.7698\n",
            "Epoch: 004, Step: 029, Loss: 0.7846\n",
            "Epoch: 004, Step: 030, Loss: 0.5910\n",
            "Epoch: 004, Step: 031, Loss: 0.5848\n",
            "Epoch: 004, Step: 032, Loss: 0.7547\n",
            "Epoch: 004, Step: 033, Loss: 0.8243\n",
            "Epoch: 004, Step: 034, Loss: 0.5920\n",
            "Epoch: 004, Step: 035, Loss: 0.5928\n",
            "Epoch: 004, Step: 036, Loss: 0.7023\n",
            "Epoch: 004, Step: 037, Loss: 0.7516\n",
            "Epoch: 004, Step: 038, Loss: 0.7000\n",
            "Epoch: 004, Step: 039, Loss: 0.7217\n",
            "Epoch: 004, Step: 040, Loss: 0.7004\n",
            "Epoch: 004, Step: 041, Loss: 0.5339\n",
            "Epoch: 004, Step: 042, Loss: 0.5515\n",
            "Epoch: 004, Step: 043, Loss: 0.6301\n",
            "Epoch: 004, Step: 044, Loss: 0.7030\n",
            "Epoch: 004, Step: 045, Loss: 0.7102\n",
            "Epoch: 004, Step: 046, Loss: 0.6168\n",
            "Epoch: 004, Step: 047, Loss: 0.8813\n",
            "Epoch: 004, Step: 048, Loss: 0.6179\n",
            "Epoch: 004, Step: 049, Loss: 0.6927\n",
            "Epoch: 004, Step: 050, Loss: 0.7228\n",
            "Epoch: 004, Step: 051, Loss: 0.7169\n",
            "Epoch: 004, Step: 052, Loss: 0.6787\n",
            "Epoch: 004, Step: 053, Loss: 0.6916\n",
            "Epoch: 004, Step: 054, Loss: 0.7175\n",
            "Epoch: 004, Step: 055, Loss: 0.7558\n",
            "Epoch: 004, Step: 056, Loss: 0.8621\n",
            "Epoch: 004, Step: 057, Loss: 0.7655\n",
            "Epoch: 004, Step: 058, Loss: 0.7118\n",
            "Epoch: 004, Step: 059, Loss: 0.6525\n",
            "Epoch: 004, Step: 060, Loss: 0.7245\n",
            "Epoch: 004, Step: 061, Loss: 0.6689\n",
            "Epoch: 004, Step: 062, Loss: 0.6544\n",
            "Epoch: 004, Step: 063, Loss: 0.6748\n",
            "Epoch: 004, Step: 064, Loss: 0.6715\n",
            "Epoch: 004, Step: 065, Loss: 0.6641\n",
            "Epoch: 004, Step: 066, Loss: 0.6839\n",
            "Epoch: 004, Step: 067, Loss: 0.6709\n",
            "Epoch: 004, Step: 068, Loss: 0.7323\n",
            "Epoch: 004, Step: 069, Loss: 0.6872\n",
            "Epoch: 004, Step: 070, Loss: 0.6510\n",
            "Epoch: 004, Step: 071, Loss: 0.6920\n",
            "Epoch: 004, Step: 072, Loss: 0.6584\n",
            "Epoch: 004, Step: 073, Loss: 0.7470\n",
            "Epoch: 004, Step: 074, Loss: 0.7106\n",
            "Epoch: 004, Step: 075, Loss: 0.7790\n",
            "Epoch: 004, Step: 076, Loss: 0.6474\n",
            "Epoch: 004, Step: 077, Loss: 0.7594\n",
            "Epoch: 004, Step: 078, Loss: 0.6962\n",
            "Epoch: 004, Step: 079, Loss: 0.7059\n",
            "Epoch: 004, Step: 080, Loss: 0.6504\n",
            "Epoch: 004, Step: 081, Loss: 0.7003\n",
            "Epoch: 004, Step: 082, Loss: 0.6643\n",
            "Epoch: 004, Step: 083, Loss: 0.7206\n",
            "Epoch: 004, Step: 084, Loss: 0.6676\n",
            "Epoch: 004, Step: 085, Loss: 0.6644\n",
            "Epoch: 004, Step: 086, Loss: 0.6785\n",
            "Epoch: 004, Step: 087, Loss: 0.7184\n",
            "Epoch: 004, Step: 088, Loss: 0.6725\n",
            "Epoch: 004, Step: 089, Loss: 0.6640\n",
            "Epoch: 004, Step: 090, Loss: 0.7452\n",
            "Epoch: 004, Step: 091, Loss: 0.6746\n",
            "Epoch: 004, Step: 092, Loss: 0.6650\n",
            "Epoch: 004, Step: 093, Loss: 0.6932\n",
            "Epoch: 004, Step: 094, Loss: 0.6626\n",
            "Epoch: 004, Step: 095, Loss: 0.6842\n",
            "Epoch: 004, Step: 096, Loss: 0.6892\n",
            "Epoch: 004, Step: 097, Loss: 0.6626\n",
            "Epoch: 004, Step: 098, Loss: 0.6983\n",
            "Epoch: 004, Step: 099, Loss: 0.6997\n",
            "Epoch: 004, Step: 100, Loss: 0.6640\n",
            "Epoch: 004, Step: 101, Loss: 0.6923\n",
            "Epoch: 004, Step: 102, Loss: 0.7029\n",
            "Epoch: 004, Step: 103, Loss: 0.6057\n",
            "Epoch: 004, Step: 104, Loss: 0.6466\n",
            "Epoch: 004, Step: 105, Loss: 0.7629\n",
            "Epoch: 004, Step: 106, Loss: 0.6932\n",
            "Epoch: 004, Step: 107, Loss: 0.7162\n",
            "Epoch: 004, Step: 108, Loss: 0.6950\n",
            "Epoch: 004, Step: 109, Loss: 0.6414\n",
            "Epoch: 004, Step: 110, Loss: 0.6266\n",
            "Epoch: 004, Step: 111, Loss: 0.6989\n",
            "Epoch: 004, Step: 112, Loss: 0.6896\n",
            "Epoch: 004, Step: 113, Loss: 0.7027\n",
            "Epoch: 004, Step: 114, Loss: 0.6916\n",
            "Epoch: 004, Step: 115, Loss: 0.7812\n",
            "Epoch: 004, Step: 116, Loss: 0.6562\n",
            "Epoch: 004, Step: 117, Loss: 0.6157\n",
            "Epoch: 004, Step: 118, Loss: 0.7013\n",
            "Epoch: 004, Step: 119, Loss: 0.5476\n",
            "Epoch: 004, Step: 120, Loss: 0.6518\n",
            "Epoch: 004, Step: 121, Loss: 0.6268\n",
            "Epoch: 004, Step: 122, Loss: 0.6467\n",
            "Epoch: 004, Step: 123, Loss: 0.7021\n",
            "Epoch: 004, Step: 124, Loss: 0.7078\n",
            "Epoch: 004, Step: 125, Loss: 0.6794\n",
            "Epoch: 004, Step: 126, Loss: 0.7710\n",
            "Epoch: 004, Step: 127, Loss: 0.6869\n",
            "Epoch: 004, Step: 128, Loss: 0.8737\n",
            "Epoch: 004, Step: 129, Loss: 0.7940\n",
            "Epoch: 004, Step: 130, Loss: 0.5194\n",
            "Epoch: 004, Step: 131, Loss: 0.7327\n",
            "Epoch: 004, Step: 132, Loss: 0.7077\n",
            "Epoch: 004, Step: 133, Loss: 0.5865\n",
            "Epoch: 004, Step: 134, Loss: 0.7620\n",
            "Epoch: 004, Step: 135, Loss: 0.5870\n",
            "Epoch: 004, Step: 136, Loss: 0.5358\n",
            "Epoch: 004, Step: 137, Loss: 0.7083\n",
            "Epoch: 004, Step: 138, Loss: 0.6848\n",
            "Epoch: 004, Step: 139, Loss: 0.5094\n",
            "Epoch: 004, Step: 140, Loss: 0.4742\n",
            "Epoch: 004, Step: 141, Loss: 0.4279\n",
            "Epoch: 004, Step: 142, Loss: 0.7764\n",
            "Epoch: 004, Step: 143, Loss: 0.7281\n",
            "Epoch: 004, Step: 144, Loss: 0.5841\n",
            "Epoch: 004, Step: 145, Loss: 0.3778\n",
            "Epoch: 004, Step: 146, Loss: 0.9756\n",
            "Epoch: 004, Step: 147, Loss: 0.7945\n",
            "Epoch: 004, Step: 148, Loss: 0.4578\n",
            "Epoch: 004, Step: 149, Loss: 0.5966\n",
            "Epoch: 004, Step: 150, Loss: 0.9288\n",
            "Epoch: 004, Step: 151, Loss: 0.5958\n",
            "Epoch: 004, Step: 152, Loss: 0.7473\n",
            "Epoch: 004, Step: 153, Loss: 1.3100\n",
            "Epoch: 004, Step: 154, Loss: 0.5121\n",
            "Epoch: 004, Step: 155, Loss: 0.7649\n",
            "Epoch: 004, Step: 156, Loss: 0.5685\n",
            "Epoch: 004, Step: 157, Loss: 0.5045\n",
            "Epoch: 004, Step: 158, Loss: 0.7146\n",
            "Epoch: 004, Step: 159, Loss: 0.4904\n",
            "Epoch: 004, Step: 160, Loss: 0.5385\n",
            "Epoch: 004, Step: 161, Loss: 0.6087\n",
            "Epoch: 004, Step: 162, Loss: 0.7690\n",
            "Epoch: 004, Step: 163, Loss: 0.6786\n",
            "Epoch: 004, Step: 164, Loss: 0.8265\n",
            "Epoch: 004, Step: 165, Loss: 0.6637\n",
            "Epoch: 004, Step: 166, Loss: 0.7714\n",
            "Epoch: 004, Step: 167, Loss: 0.8835\n",
            "Epoch: 004, Step: 168, Loss: 0.5734\n",
            "Epoch: 004, Step: 169, Loss: 0.6113\n",
            "Epoch: 004, Step: 170, Loss: 0.5876\n",
            "Epoch: 004, Step: 171, Loss: 0.7709\n",
            "Epoch: 004, Step: 172, Loss: 0.6167\n",
            "Epoch: 004, Step: 173, Loss: 0.7789\n",
            "Epoch: 004, Step: 174, Loss: 0.6887\n",
            "Epoch: 004, Step: 175, Loss: 0.6384\n",
            "Epoch: 004, Step: 176, Loss: 0.9587\n",
            "Epoch: 004, Step: 177, Loss: 0.8391\n",
            "Epoch: 004, Step: 178, Loss: 0.6700\n",
            "Epoch: 004, Step: 179, Loss: 0.7226\n",
            "Epoch: 004, Step: 180, Loss: 0.6888\n",
            "Epoch: 004, Step: 181, Loss: 0.5985\n",
            "Epoch: 004, Step: 182, Loss: 0.6248\n",
            "Epoch: 004, Step: 183, Loss: 0.7069\n",
            "Epoch: 004, Step: 184, Loss: 0.6392\n",
            "Epoch: 004, Step: 185, Loss: 0.7612\n",
            "Epoch: 004, Step: 186, Loss: 0.7714\n",
            "Epoch: 004, Step: 187, Loss: 0.6538\n",
            "Epoch: 004, Step: 188, Loss: 0.5555\n",
            "Epoch: 004, Step: 189, Loss: 0.6214\n",
            "Epoch: 004, Step: 190, Loss: 0.6204\n",
            "Epoch: 004, Step: 191, Loss: 0.6525\n",
            "Epoch: 004, Step: 192, Loss: 0.6306\n",
            "Epoch: 004, Step: 193, Loss: 0.8248\n",
            "Epoch: 004, Step: 194, Loss: 0.7275\n",
            "Epoch: 004, Step: 195, Loss: 0.7002\n",
            "Epoch: 004, Step: 196, Loss: 0.5676\n",
            "Epoch: 004, Step: 197, Loss: 0.7390\n",
            "Epoch: 004, Step: 198, Loss: 0.8412\n",
            "Epoch: 004, Step: 199, Loss: 0.6063\n",
            "Epoch: 004, Step: 200, Loss: 0.7275\n",
            "Epoch: 004, Step: 201, Loss: 0.7253\n",
            "Epoch: 004, Step: 202, Loss: 0.8101\n",
            "Epoch: 004, Step: 203, Loss: 0.6102\n",
            "Epoch: 004, Step: 204, Loss: 0.9128\n",
            "Epoch: 004, Step: 205, Loss: 0.6426\n",
            "Epoch: 004, Step: 206, Loss: 0.8962\n",
            "Epoch: 004, Step: 000, Val Loss: 0.6414\n",
            "Epoch: 004, Step: 001, Val Loss: 0.7586\n",
            "Epoch: 004, Step: 002, Val Loss: 0.6414\n",
            "Epoch: 004, Step: 003, Val Loss: 0.7586\n",
            "Epoch: 004, Step: 004, Val Loss: 0.7000\n",
            "Epoch: 004, Step: 005, Val Loss: 0.6414\n",
            "Epoch: 004, Step: 006, Val Loss: 0.6677\n",
            "Epoch: 004, Step: 007, Val Loss: 0.5828\n",
            "Epoch: 004, Step: 008, Val Loss: 0.6414\n",
            "Epoch: 004, Step: 009, Val Loss: 0.5936\n",
            "Epoch: 004, Step: 010, Val Loss: 0.6414\n",
            "Epoch: 004, Step: 011, Val Loss: 0.5828\n",
            "Epoch: 004, Step: 012, Val Loss: 0.8172\n",
            "Epoch: 004, Step: 013, Val Loss: 0.6414\n",
            "Epoch: 004, Step: 014, Val Loss: 0.6414\n",
            "Epoch: 004, Step: 015, Val Loss: 0.7000\n",
            "Epoch: 004, Step: 016, Val Loss: 0.7000\n",
            "Epoch: 004, Step: 017, Val Loss: 0.7000\n",
            "Epoch: 004, Step: 018, Val Loss: 0.7402\n",
            "Epoch: 004, Step: 019, Val Loss: 0.6414\n",
            "Epoch: 004, Step: 020, Val Loss: 0.6414\n",
            "Epoch: 004, Step: 021, Val Loss: 0.6414\n",
            "Epoch: 004, Step: 022, Val Loss: 0.7000\n",
            "Epoch: 004, Step: 023, Val Loss: 0.7000\n",
            "Epoch: 004, Step: 024, Val Loss: 0.7586\n",
            "Epoch: 004, Step: 025, Val Loss: 0.5828\n",
            "Epoch: 004, Step: 026, Val Loss: 0.7000\n",
            "Epoch: 004, Step: 027, Val Loss: 0.7000\n",
            "Epoch: 004, Step: 028, Val Loss: 0.7000\n",
            "Epoch: 004, Step: 029, Val Loss: 0.7000\n",
            "Epoch: 004, Step: 030, Val Loss: 0.5828\n",
            "Epoch: 004, Step: 031, Val Loss: 0.7586\n",
            "Epoch: 004, Step: 032, Val Loss: 0.7586\n",
            "Epoch: 004, Step: 033, Val Loss: 0.5828\n",
            "Epoch: 004, Step: 034, Val Loss: 0.7000\n",
            "Epoch: 004, Step: 035, Val Loss: 0.6895\n",
            "Epoch: 004, Step: 036, Val Loss: 0.6414\n",
            "Epoch: 004, Step: 037, Val Loss: 0.7000\n",
            "Epoch: 004, Step: 038, Val Loss: 0.6414\n",
            "Epoch: 004, Step: 039, Val Loss: 0.6414\n",
            "Epoch: 004, Step: 040, Val Loss: 0.7000\n",
            "Epoch: 004, Step: 041, Val Loss: 0.7239\n",
            "Epoch: 004, Step: 042, Val Loss: 0.7000\n",
            "Epoch: 004, Step: 043, Val Loss: 0.8172\n",
            "Epoch: 004, Step: 044, Val Loss: 0.7586\n",
            "Epoch: 004, Step: 045, Val Loss: 0.5828\n",
            "Epoch: 004, Step: 046, Val Loss: 0.7586\n",
            "Epoch: 004, Step: 047, Val Loss: 0.6414\n",
            "Epoch: 004, Step: 048, Val Loss: 0.5828\n",
            "Epoch: 004, Step: 049, Val Loss: 0.6209\n",
            "Epoch: 004, Step: 050, Val Loss: 0.7000\n",
            "Epoch: 004, Step: 051, Val Loss: 0.5828\n",
            "Epoch: 004, Step: 052, Val Loss: 0.7722\n",
            "Epoch: 004, Step: 053, Val Loss: 0.7000\n",
            "Epoch: 004, Step: 054, Val Loss: 0.7240\n",
            "Epoch: 004, Step: 055, Val Loss: 0.7586\n",
            "Epoch: 004, Step: 056, Val Loss: 0.7000\n",
            "Epoch: 004, Step: 057, Val Loss: 0.5828\n",
            "Epoch: 004, Step: 058, Val Loss: 0.7586\n",
            "Epoch: 005, Step: 000, Loss: 0.7132\n",
            "Epoch: 005, Step: 001, Loss: 0.6590\n",
            "Epoch: 005, Step: 002, Loss: 0.6320\n",
            "Epoch: 005, Step: 003, Loss: 0.6703\n",
            "Epoch: 005, Step: 004, Loss: 0.6722\n",
            "Epoch: 005, Step: 005, Loss: 0.6911\n",
            "Epoch: 005, Step: 006, Loss: 0.6781\n",
            "Epoch: 005, Step: 007, Loss: 0.6901\n",
            "Epoch: 005, Step: 008, Loss: 0.6623\n",
            "Epoch: 005, Step: 009, Loss: 0.7537\n",
            "Epoch: 005, Step: 010, Loss: 0.6894\n",
            "Epoch: 005, Step: 011, Loss: 0.6509\n",
            "Epoch: 005, Step: 012, Loss: 0.6673\n",
            "Epoch: 005, Step: 013, Loss: 0.6901\n",
            "Epoch: 005, Step: 014, Loss: 0.7138\n",
            "Epoch: 005, Step: 015, Loss: 0.6860\n",
            "Epoch: 005, Step: 016, Loss: 0.6748\n",
            "Epoch: 005, Step: 017, Loss: 0.7455\n",
            "Epoch: 005, Step: 018, Loss: 0.7417\n",
            "Epoch: 005, Step: 019, Loss: 0.7468\n",
            "Epoch: 005, Step: 020, Loss: 0.6821\n",
            "Epoch: 005, Step: 021, Loss: 0.6934\n",
            "Epoch: 005, Step: 022, Loss: 0.7030\n",
            "Epoch: 005, Step: 023, Loss: 0.6927\n",
            "Epoch: 005, Step: 024, Loss: 0.6837\n",
            "Epoch: 005, Step: 025, Loss: 0.6859\n",
            "Epoch: 005, Step: 026, Loss: 0.7026\n",
            "Epoch: 005, Step: 027, Loss: 0.6366\n",
            "Epoch: 005, Step: 028, Loss: 0.6547\n",
            "Epoch: 005, Step: 029, Loss: 0.6151\n",
            "Epoch: 005, Step: 030, Loss: 0.6432\n",
            "Epoch: 005, Step: 031, Loss: 0.7419\n",
            "Epoch: 005, Step: 032, Loss: 0.7706\n",
            "Epoch: 005, Step: 033, Loss: 0.6489\n",
            "Epoch: 005, Step: 034, Loss: 0.7066\n",
            "Epoch: 005, Step: 035, Loss: 0.6507\n",
            "Epoch: 005, Step: 036, Loss: 0.5748\n",
            "Epoch: 005, Step: 037, Loss: 0.7123\n",
            "Epoch: 005, Step: 038, Loss: 0.6860\n",
            "Epoch: 005, Step: 039, Loss: 0.6449\n",
            "Epoch: 005, Step: 040, Loss: 0.7219\n",
            "Epoch: 005, Step: 041, Loss: 0.6624\n",
            "Epoch: 005, Step: 042, Loss: 0.7214\n",
            "Epoch: 005, Step: 043, Loss: 0.6750\n",
            "Epoch: 005, Step: 044, Loss: 0.7195\n",
            "Epoch: 005, Step: 045, Loss: 0.7210\n",
            "Epoch: 005, Step: 046, Loss: 0.7819\n",
            "Epoch: 005, Step: 047, Loss: 0.5280\n",
            "Epoch: 005, Step: 048, Loss: 0.8200\n",
            "Epoch: 005, Step: 049, Loss: 0.6262\n",
            "Epoch: 005, Step: 050, Loss: 0.8044\n",
            "Epoch: 005, Step: 051, Loss: 0.7104\n",
            "Epoch: 005, Step: 052, Loss: 0.7192\n",
            "Epoch: 005, Step: 053, Loss: 0.6386\n",
            "Epoch: 005, Step: 054, Loss: 0.6329\n",
            "Epoch: 005, Step: 055, Loss: 0.6284\n",
            "Epoch: 005, Step: 056, Loss: 0.6090\n",
            "Epoch: 005, Step: 057, Loss: 0.6826\n",
            "Epoch: 005, Step: 058, Loss: 0.7162\n",
            "Epoch: 005, Step: 059, Loss: 0.7625\n",
            "Epoch: 005, Step: 060, Loss: 0.6245\n",
            "Epoch: 005, Step: 061, Loss: 0.7059\n",
            "Epoch: 005, Step: 062, Loss: 0.6320\n",
            "Epoch: 005, Step: 063, Loss: 0.6989\n",
            "Epoch: 005, Step: 064, Loss: 0.7719\n",
            "Epoch: 005, Step: 065, Loss: 0.7139\n",
            "Epoch: 005, Step: 066, Loss: 0.7094\n",
            "Epoch: 005, Step: 067, Loss: 0.7769\n",
            "Epoch: 005, Step: 068, Loss: 0.6306\n",
            "Epoch: 005, Step: 069, Loss: 0.8247\n",
            "Epoch: 005, Step: 070, Loss: 0.7425\n",
            "Epoch: 005, Step: 071, Loss: 0.6960\n",
            "Epoch: 005, Step: 072, Loss: 0.6523\n",
            "Epoch: 005, Step: 073, Loss: 0.6845\n",
            "Epoch: 005, Step: 074, Loss: 0.6856\n",
            "Epoch: 005, Step: 075, Loss: 0.6895\n",
            "Epoch: 005, Step: 076, Loss: 0.6201\n",
            "Epoch: 005, Step: 077, Loss: 0.6447\n",
            "Epoch: 005, Step: 078, Loss: 0.6460\n",
            "Epoch: 005, Step: 079, Loss: 0.7391\n",
            "Epoch: 005, Step: 080, Loss: 0.7277\n",
            "Epoch: 005, Step: 081, Loss: 0.7178\n",
            "Epoch: 005, Step: 082, Loss: 0.6778\n",
            "Epoch: 005, Step: 083, Loss: 0.6429\n",
            "Epoch: 005, Step: 084, Loss: 0.7025\n",
            "Epoch: 005, Step: 085, Loss: 0.6737\n",
            "Epoch: 005, Step: 086, Loss: 0.5893\n",
            "Epoch: 005, Step: 087, Loss: 0.7045\n",
            "Epoch: 005, Step: 088, Loss: 0.6983\n",
            "Epoch: 005, Step: 089, Loss: 0.7492\n",
            "Epoch: 005, Step: 090, Loss: 0.6356\n",
            "Epoch: 005, Step: 091, Loss: 0.7388\n",
            "Epoch: 005, Step: 092, Loss: 0.6978\n",
            "Epoch: 005, Step: 093, Loss: 0.6798\n",
            "Epoch: 005, Step: 094, Loss: 0.6391\n",
            "Epoch: 005, Step: 095, Loss: 0.6184\n",
            "Epoch: 005, Step: 096, Loss: 0.5557\n",
            "Epoch: 005, Step: 097, Loss: 0.7753\n",
            "Epoch: 005, Step: 098, Loss: 0.7352\n",
            "Epoch: 005, Step: 099, Loss: 0.6265\n",
            "Epoch: 005, Step: 100, Loss: 0.6200\n",
            "Epoch: 005, Step: 101, Loss: 0.7182\n",
            "Epoch: 005, Step: 102, Loss: 0.8006\n",
            "Epoch: 005, Step: 103, Loss: 0.6259\n",
            "Epoch: 005, Step: 104, Loss: 0.7210\n",
            "Epoch: 005, Step: 105, Loss: 0.6125\n",
            "Epoch: 005, Step: 106, Loss: 0.6189\n",
            "Epoch: 005, Step: 107, Loss: 0.6281\n",
            "Epoch: 005, Step: 108, Loss: 0.6049\n",
            "Epoch: 005, Step: 109, Loss: 0.7080\n",
            "Epoch: 005, Step: 110, Loss: 0.7019\n",
            "Epoch: 005, Step: 111, Loss: 0.6992\n",
            "Epoch: 005, Step: 112, Loss: 0.7209\n",
            "Epoch: 005, Step: 113, Loss: 0.6101\n",
            "Epoch: 005, Step: 114, Loss: 0.7234\n",
            "Epoch: 005, Step: 115, Loss: 0.6832\n",
            "Epoch: 005, Step: 116, Loss: 0.8258\n",
            "Epoch: 005, Step: 117, Loss: 0.7291\n",
            "Epoch: 005, Step: 118, Loss: 0.5101\n",
            "Epoch: 005, Step: 119, Loss: 0.6116\n",
            "Epoch: 005, Step: 120, Loss: 0.6049\n",
            "Epoch: 005, Step: 121, Loss: 0.5938\n",
            "Epoch: 005, Step: 122, Loss: 0.8686\n",
            "Epoch: 005, Step: 123, Loss: 0.6166\n",
            "Epoch: 005, Step: 124, Loss: 0.5967\n",
            "Epoch: 005, Step: 125, Loss: 0.7186\n",
            "Epoch: 005, Step: 126, Loss: 0.6051\n",
            "Epoch: 005, Step: 127, Loss: 0.7148\n",
            "Epoch: 005, Step: 128, Loss: 0.8246\n",
            "Epoch: 005, Step: 129, Loss: 0.6283\n",
            "Epoch: 005, Step: 130, Loss: 0.6148\n",
            "Epoch: 005, Step: 131, Loss: 0.6255\n",
            "Epoch: 005, Step: 132, Loss: 0.4894\n",
            "Epoch: 005, Step: 133, Loss: 0.7234\n",
            "Epoch: 005, Step: 134, Loss: 0.8765\n",
            "Epoch: 005, Step: 135, Loss: 0.6427\n",
            "Epoch: 005, Step: 136, Loss: 0.8597\n",
            "Epoch: 005, Step: 137, Loss: 0.6074\n",
            "Epoch: 005, Step: 138, Loss: 0.6154\n",
            "Epoch: 005, Step: 139, Loss: 0.6714\n",
            "Epoch: 005, Step: 140, Loss: 0.8015\n",
            "Epoch: 005, Step: 141, Loss: 0.6167\n",
            "Epoch: 005, Step: 142, Loss: 0.7587\n",
            "Epoch: 005, Step: 143, Loss: 0.8309\n",
            "Epoch: 005, Step: 144, Loss: 0.7501\n",
            "Epoch: 005, Step: 145, Loss: 0.7956\n",
            "Epoch: 005, Step: 146, Loss: 0.6608\n",
            "Epoch: 005, Step: 147, Loss: 0.5831\n",
            "Epoch: 005, Step: 148, Loss: 0.7778\n",
            "Epoch: 005, Step: 149, Loss: 0.6364\n",
            "Epoch: 005, Step: 150, Loss: 0.6854\n",
            "Epoch: 005, Step: 151, Loss: 0.6599\n",
            "Epoch: 005, Step: 152, Loss: 0.7164\n",
            "Epoch: 005, Step: 153, Loss: 0.7051\n",
            "Epoch: 005, Step: 154, Loss: 0.6424\n",
            "Epoch: 005, Step: 155, Loss: 0.6600\n",
            "Epoch: 005, Step: 156, Loss: 0.7070\n",
            "Epoch: 005, Step: 157, Loss: 0.6480\n",
            "Epoch: 005, Step: 158, Loss: 0.7226\n",
            "Epoch: 005, Step: 159, Loss: 0.6734\n",
            "Epoch: 005, Step: 160, Loss: 0.7204\n",
            "Epoch: 005, Step: 161, Loss: 0.6260\n",
            "Epoch: 005, Step: 162, Loss: 0.6450\n",
            "Epoch: 005, Step: 163, Loss: 0.5972\n",
            "Epoch: 005, Step: 164, Loss: 0.6804\n",
            "Epoch: 005, Step: 165, Loss: 0.7257\n",
            "Epoch: 005, Step: 166, Loss: 0.6137\n",
            "Epoch: 005, Step: 167, Loss: 0.6996\n",
            "Epoch: 005, Step: 168, Loss: 0.7052\n",
            "Epoch: 005, Step: 169, Loss: 0.7399\n",
            "Epoch: 005, Step: 170, Loss: 0.5812\n",
            "Epoch: 005, Step: 171, Loss: 0.6659\n",
            "Epoch: 005, Step: 172, Loss: 0.6034\n",
            "Epoch: 005, Step: 173, Loss: 0.7214\n",
            "Epoch: 005, Step: 174, Loss: 0.7551\n",
            "Epoch: 005, Step: 175, Loss: 0.7380\n",
            "Epoch: 005, Step: 176, Loss: 0.5220\n",
            "Epoch: 005, Step: 177, Loss: 0.4501\n",
            "Epoch: 005, Step: 178, Loss: 0.5947\n",
            "Epoch: 005, Step: 179, Loss: 0.4657\n",
            "Epoch: 005, Step: 180, Loss: 0.5749\n",
            "Epoch: 005, Step: 181, Loss: 0.7382\n",
            "Epoch: 005, Step: 182, Loss: 0.5442\n",
            "Epoch: 005, Step: 183, Loss: 0.9887\n",
            "Epoch: 005, Step: 184, Loss: 0.5765\n",
            "Epoch: 005, Step: 185, Loss: 0.9330\n",
            "Epoch: 005, Step: 186, Loss: 0.6189\n",
            "Epoch: 005, Step: 187, Loss: 0.5668\n",
            "Epoch: 005, Step: 188, Loss: 0.7214\n",
            "Epoch: 005, Step: 189, Loss: 0.7848\n",
            "Epoch: 005, Step: 190, Loss: 0.7504\n",
            "Epoch: 005, Step: 191, Loss: 0.9393\n",
            "Epoch: 005, Step: 192, Loss: 0.9458\n",
            "Epoch: 005, Step: 193, Loss: 0.5516\n",
            "Epoch: 005, Step: 194, Loss: 0.7483\n",
            "Epoch: 005, Step: 195, Loss: 0.9711\n",
            "Epoch: 005, Step: 196, Loss: 0.6377\n",
            "Epoch: 005, Step: 197, Loss: 0.6297\n",
            "Epoch: 005, Step: 198, Loss: 0.6023\n",
            "Epoch: 005, Step: 199, Loss: 0.6105\n",
            "Epoch: 005, Step: 200, Loss: 0.7097\n",
            "Epoch: 005, Step: 201, Loss: 0.7307\n",
            "Epoch: 005, Step: 202, Loss: 0.6131\n",
            "Epoch: 005, Step: 203, Loss: 0.7257\n",
            "Epoch: 005, Step: 204, Loss: 0.7026\n",
            "Epoch: 005, Step: 205, Loss: 0.7258\n",
            "Epoch: 005, Step: 206, Loss: 0.5092\n",
            "Epoch: 005, Step: 000, Val Loss: 0.7576\n",
            "Epoch: 005, Step: 001, Val Loss: 0.8593\n",
            "Epoch: 005, Step: 002, Val Loss: 0.8185\n",
            "Epoch: 005, Step: 003, Val Loss: 0.6022\n",
            "Epoch: 005, Step: 004, Val Loss: 0.7454\n",
            "Epoch: 005, Step: 005, Val Loss: 0.5790\n",
            "Epoch: 005, Step: 006, Val Loss: 0.7279\n",
            "Epoch: 005, Step: 007, Val Loss: 1.0214\n",
            "Epoch: 005, Step: 008, Val Loss: 0.7444\n",
            "Epoch: 005, Step: 009, Val Loss: 0.7218\n",
            "Epoch: 005, Step: 010, Val Loss: 0.7770\n",
            "Epoch: 005, Step: 011, Val Loss: 0.6430\n",
            "Epoch: 005, Step: 012, Val Loss: 0.9096\n",
            "Epoch: 005, Step: 013, Val Loss: 1.1873\n",
            "Epoch: 005, Step: 014, Val Loss: 0.6020\n",
            "Epoch: 005, Step: 015, Val Loss: 0.4726\n",
            "Epoch: 005, Step: 016, Val Loss: 0.8667\n",
            "Epoch: 005, Step: 017, Val Loss: 0.8631\n",
            "Epoch: 005, Step: 018, Val Loss: 0.9333\n",
            "Epoch: 005, Step: 019, Val Loss: 0.9000\n",
            "Epoch: 005, Step: 020, Val Loss: 0.8623\n",
            "Epoch: 005, Step: 021, Val Loss: 0.8671\n",
            "Epoch: 005, Step: 022, Val Loss: 0.7817\n",
            "Epoch: 005, Step: 023, Val Loss: 0.7508\n",
            "Epoch: 005, Step: 024, Val Loss: 1.0032\n",
            "Epoch: 005, Step: 025, Val Loss: 0.7921\n",
            "Epoch: 005, Step: 026, Val Loss: 0.6960\n",
            "Epoch: 005, Step: 027, Val Loss: 0.8103\n",
            "Epoch: 005, Step: 028, Val Loss: 0.4558\n",
            "Epoch: 005, Step: 029, Val Loss: 0.8608\n",
            "Epoch: 005, Step: 030, Val Loss: 0.7381\n",
            "Epoch: 005, Step: 031, Val Loss: 0.8741\n",
            "Epoch: 005, Step: 032, Val Loss: 0.7495\n",
            "Epoch: 005, Step: 033, Val Loss: 0.6118\n",
            "Epoch: 005, Step: 034, Val Loss: 0.7558\n",
            "Epoch: 005, Step: 035, Val Loss: 0.8191\n",
            "Epoch: 005, Step: 036, Val Loss: 0.8664\n",
            "Epoch: 005, Step: 037, Val Loss: 0.9809\n",
            "Epoch: 005, Step: 038, Val Loss: 0.6823\n",
            "Epoch: 005, Step: 039, Val Loss: 0.5939\n",
            "Epoch: 005, Step: 040, Val Loss: 0.7301\n",
            "Epoch: 005, Step: 041, Val Loss: 0.6609\n",
            "Epoch: 005, Step: 042, Val Loss: 0.7690\n",
            "Epoch: 005, Step: 043, Val Loss: 0.4840\n",
            "Epoch: 005, Step: 044, Val Loss: 0.8691\n",
            "Epoch: 005, Step: 045, Val Loss: 0.7486\n",
            "Epoch: 005, Step: 046, Val Loss: 0.9875\n",
            "Epoch: 005, Step: 047, Val Loss: 0.8758\n",
            "Epoch: 005, Step: 048, Val Loss: 0.7606\n",
            "Epoch: 005, Step: 049, Val Loss: 0.6582\n",
            "Epoch: 005, Step: 050, Val Loss: 0.8598\n",
            "Epoch: 005, Step: 051, Val Loss: 1.0248\n",
            "Epoch: 005, Step: 052, Val Loss: 0.6375\n",
            "Epoch: 005, Step: 053, Val Loss: 0.8021\n",
            "Epoch: 005, Step: 054, Val Loss: 1.0101\n",
            "Epoch: 005, Step: 055, Val Loss: 0.8740\n",
            "Epoch: 005, Step: 056, Val Loss: 0.8689\n",
            "Epoch: 005, Step: 057, Val Loss: 0.3930\n",
            "Epoch: 005, Step: 058, Val Loss: 0.7670\n",
            "Epoch: 006, Step: 000, Loss: 0.6711\n",
            "Epoch: 006, Step: 001, Loss: 1.0434\n",
            "Epoch: 006, Step: 002, Loss: 0.9350\n",
            "Epoch: 006, Step: 003, Loss: 0.6616\n",
            "Epoch: 006, Step: 004, Loss: 0.6471\n",
            "Epoch: 006, Step: 005, Loss: 0.7500\n",
            "Epoch: 006, Step: 006, Loss: 0.6582\n",
            "Epoch: 006, Step: 007, Loss: 0.6151\n",
            "Epoch: 006, Step: 008, Loss: 0.7323\n",
            "Epoch: 006, Step: 009, Loss: 0.7679\n",
            "Epoch: 006, Step: 010, Loss: 0.6409\n",
            "Epoch: 006, Step: 011, Loss: 0.7120\n",
            "Epoch: 006, Step: 012, Loss: 0.6729\n",
            "Epoch: 006, Step: 013, Loss: 0.5500\n",
            "Epoch: 006, Step: 014, Loss: 1.0315\n",
            "Epoch: 006, Step: 015, Loss: 0.9202\n",
            "Epoch: 006, Step: 016, Loss: 1.0205\n",
            "Epoch: 006, Step: 017, Loss: 0.6575\n",
            "Epoch: 006, Step: 018, Loss: 0.5825\n",
            "Epoch: 006, Step: 019, Loss: 0.7944\n",
            "Epoch: 006, Step: 020, Loss: 0.8017\n",
            "Epoch: 006, Step: 021, Loss: 0.6715\n",
            "Epoch: 006, Step: 022, Loss: 0.6726\n",
            "Epoch: 006, Step: 023, Loss: 0.6214\n",
            "Epoch: 006, Step: 024, Loss: 0.7852\n",
            "Epoch: 006, Step: 025, Loss: 0.8746\n",
            "Epoch: 006, Step: 026, Loss: 0.7309\n",
            "Epoch: 006, Step: 027, Loss: 0.6901\n",
            "Epoch: 006, Step: 028, Loss: 0.6908\n",
            "Epoch: 006, Step: 029, Loss: 0.7077\n",
            "Epoch: 006, Step: 030, Loss: 0.6855\n",
            "Epoch: 006, Step: 031, Loss: 0.6898\n",
            "Epoch: 006, Step: 032, Loss: 0.6286\n",
            "Epoch: 006, Step: 033, Loss: 0.6448\n",
            "Epoch: 006, Step: 034, Loss: 0.5706\n",
            "Epoch: 006, Step: 035, Loss: 0.8537\n",
            "Epoch: 006, Step: 036, Loss: 0.6472\n",
            "Epoch: 006, Step: 037, Loss: 0.7785\n",
            "Epoch: 006, Step: 038, Loss: 0.5889\n",
            "Epoch: 006, Step: 039, Loss: 0.7406\n",
            "Epoch: 006, Step: 040, Loss: 0.4523\n",
            "Epoch: 006, Step: 041, Loss: 0.4129\n",
            "Epoch: 006, Step: 042, Loss: 0.7769\n",
            "Epoch: 006, Step: 043, Loss: 0.7837\n",
            "Epoch: 006, Step: 044, Loss: 0.5840\n",
            "Epoch: 006, Step: 045, Loss: 0.5533\n",
            "Epoch: 006, Step: 046, Loss: 0.5483\n",
            "Epoch: 006, Step: 047, Loss: 0.3489\n",
            "Epoch: 006, Step: 048, Loss: 0.7887\n",
            "Epoch: 006, Step: 049, Loss: 0.8309\n",
            "Epoch: 006, Step: 050, Loss: 0.8527\n",
            "Epoch: 006, Step: 051, Loss: 1.0474\n",
            "Epoch: 006, Step: 052, Loss: 1.3022\n",
            "Epoch: 006, Step: 053, Loss: 0.5580\n",
            "Epoch: 006, Step: 054, Loss: 0.5803\n",
            "Epoch: 006, Step: 055, Loss: 0.7542\n",
            "Epoch: 006, Step: 056, Loss: 0.7301\n",
            "Epoch: 006, Step: 057, Loss: 0.8626\n",
            "Epoch: 006, Step: 058, Loss: 0.7319\n",
            "Epoch: 006, Step: 059, Loss: 0.5717\n",
            "Epoch: 006, Step: 060, Loss: 0.6806\n",
            "Epoch: 006, Step: 061, Loss: 0.5508\n",
            "Epoch: 006, Step: 062, Loss: 0.6811\n",
            "Epoch: 006, Step: 063, Loss: 0.6718\n",
            "Epoch: 006, Step: 064, Loss: 0.6232\n",
            "Epoch: 006, Step: 065, Loss: 0.6983\n",
            "Epoch: 006, Step: 066, Loss: 0.6354\n",
            "Epoch: 006, Step: 067, Loss: 0.6418\n",
            "Epoch: 006, Step: 068, Loss: 0.5827\n",
            "Epoch: 006, Step: 069, Loss: 0.8466\n",
            "Epoch: 006, Step: 070, Loss: 0.8404\n",
            "Epoch: 006, Step: 071, Loss: 0.7190\n",
            "Epoch: 006, Step: 072, Loss: 0.7903\n",
            "Epoch: 006, Step: 073, Loss: 0.5451\n",
            "Epoch: 006, Step: 074, Loss: 0.5396\n",
            "Epoch: 006, Step: 075, Loss: 0.7341\n",
            "Epoch: 006, Step: 076, Loss: 0.6292\n",
            "Epoch: 006, Step: 077, Loss: 0.6011\n",
            "Epoch: 006, Step: 078, Loss: 0.5349\n",
            "Epoch: 006, Step: 079, Loss: 0.7157\n",
            "Epoch: 006, Step: 080, Loss: 0.7888\n",
            "Epoch: 006, Step: 081, Loss: 0.6997\n",
            "Epoch: 006, Step: 082, Loss: 0.6978\n",
            "Epoch: 006, Step: 083, Loss: 0.7075\n",
            "Epoch: 006, Step: 084, Loss: 0.6426\n",
            "Epoch: 006, Step: 085, Loss: 0.8797\n",
            "Epoch: 006, Step: 086, Loss: 0.6674\n",
            "Epoch: 006, Step: 087, Loss: 0.6567\n",
            "Epoch: 006, Step: 088, Loss: 0.7244\n",
            "Epoch: 006, Step: 089, Loss: 0.6159\n",
            "Epoch: 006, Step: 090, Loss: 0.6883\n",
            "Epoch: 006, Step: 091, Loss: 0.7819\n",
            "Epoch: 006, Step: 092, Loss: 0.7397\n",
            "Epoch: 006, Step: 093, Loss: 0.7055\n",
            "Epoch: 006, Step: 094, Loss: 0.6352\n",
            "Epoch: 006, Step: 095, Loss: 0.7616\n",
            "Epoch: 006, Step: 096, Loss: 0.6390\n",
            "Epoch: 006, Step: 097, Loss: 0.7232\n",
            "Epoch: 006, Step: 098, Loss: 0.7457\n",
            "Epoch: 006, Step: 099, Loss: 0.7191\n",
            "Epoch: 006, Step: 100, Loss: 0.7427\n",
            "Epoch: 006, Step: 101, Loss: 0.6517\n",
            "Epoch: 006, Step: 102, Loss: 0.6210\n",
            "Epoch: 006, Step: 103, Loss: 0.6647\n",
            "Epoch: 006, Step: 104, Loss: 0.6753\n",
            "Epoch: 006, Step: 105, Loss: 0.7398\n",
            "Epoch: 006, Step: 106, Loss: 0.6800\n",
            "Epoch: 006, Step: 107, Loss: 0.7260\n",
            "Epoch: 006, Step: 108, Loss: 0.7543\n",
            "Epoch: 006, Step: 109, Loss: 0.8255\n",
            "Epoch: 006, Step: 110, Loss: 0.9802\n",
            "Epoch: 006, Step: 111, Loss: 0.7642\n",
            "Epoch: 006, Step: 112, Loss: 0.6573\n",
            "Epoch: 006, Step: 113, Loss: 0.7157\n",
            "Epoch: 006, Step: 114, Loss: 0.6235\n",
            "Epoch: 006, Step: 115, Loss: 0.6047\n",
            "Epoch: 006, Step: 116, Loss: 0.8038\n",
            "Epoch: 006, Step: 117, Loss: 0.8186\n",
            "Epoch: 006, Step: 118, Loss: 0.5864\n",
            "Epoch: 006, Step: 119, Loss: 0.8790\n",
            "Epoch: 006, Step: 120, Loss: 0.5995\n",
            "Epoch: 006, Step: 121, Loss: 0.5876\n",
            "Epoch: 006, Step: 122, Loss: 0.7422\n",
            "Epoch: 006, Step: 123, Loss: 0.7555\n",
            "Epoch: 006, Step: 124, Loss: 0.5240\n",
            "Epoch: 006, Step: 125, Loss: 0.6771\n",
            "Epoch: 006, Step: 126, Loss: 0.5438\n",
            "Epoch: 006, Step: 127, Loss: 0.7596\n",
            "Epoch: 006, Step: 128, Loss: 0.7430\n",
            "Epoch: 006, Step: 129, Loss: 0.6049\n",
            "Epoch: 006, Step: 130, Loss: 0.6135\n",
            "Epoch: 006, Step: 131, Loss: 0.6064\n",
            "Epoch: 006, Step: 132, Loss: 0.8858\n",
            "Epoch: 006, Step: 133, Loss: 0.4316\n",
            "Epoch: 006, Step: 134, Loss: 0.7608\n",
            "Epoch: 006, Step: 135, Loss: 0.5167\n",
            "Epoch: 006, Step: 136, Loss: 0.7510\n",
            "Epoch: 006, Step: 137, Loss: 0.5880\n",
            "Epoch: 006, Step: 138, Loss: 0.5941\n",
            "Epoch: 006, Step: 139, Loss: 0.4416\n",
            "Epoch: 006, Step: 140, Loss: 0.7452\n",
            "Epoch: 006, Step: 141, Loss: 0.8318\n",
            "Epoch: 006, Step: 142, Loss: 0.7456\n",
            "Epoch: 006, Step: 143, Loss: 0.7288\n",
            "Epoch: 006, Step: 144, Loss: 1.0742\n",
            "Epoch: 006, Step: 145, Loss: 0.7439\n",
            "Epoch: 006, Step: 146, Loss: 0.6163\n",
            "Epoch: 006, Step: 147, Loss: 0.4698\n",
            "Epoch: 006, Step: 148, Loss: 0.6706\n",
            "Epoch: 006, Step: 149, Loss: 0.9343\n",
            "Epoch: 006, Step: 150, Loss: 0.6967\n",
            "Epoch: 006, Step: 151, Loss: 0.7153\n",
            "Epoch: 006, Step: 152, Loss: 0.7337\n",
            "Epoch: 006, Step: 153, Loss: 0.6000\n",
            "Epoch: 006, Step: 154, Loss: 0.6121\n",
            "Epoch: 006, Step: 155, Loss: 0.6972\n",
            "Epoch: 006, Step: 156, Loss: 0.6852\n",
            "Epoch: 006, Step: 157, Loss: 0.7669\n",
            "Epoch: 006, Step: 158, Loss: 0.7039\n",
            "Epoch: 006, Step: 159, Loss: 0.6297\n",
            "Epoch: 006, Step: 160, Loss: 0.6275\n",
            "Epoch: 006, Step: 161, Loss: 0.7738\n",
            "Epoch: 006, Step: 162, Loss: 0.6651\n",
            "Epoch: 006, Step: 163, Loss: 0.6092\n",
            "Epoch: 006, Step: 164, Loss: 0.7794\n",
            "Epoch: 006, Step: 165, Loss: 0.5655\n",
            "Epoch: 006, Step: 166, Loss: 0.7034\n",
            "Epoch: 006, Step: 167, Loss: 0.6943\n",
            "Epoch: 006, Step: 168, Loss: 0.6944\n",
            "Epoch: 006, Step: 169, Loss: 0.6246\n",
            "Epoch: 006, Step: 170, Loss: 0.7601\n",
            "Epoch: 006, Step: 171, Loss: 0.6281\n",
            "Epoch: 006, Step: 172, Loss: 0.6660\n",
            "Epoch: 006, Step: 173, Loss: 0.8153\n",
            "Epoch: 006, Step: 174, Loss: 0.8431\n",
            "Epoch: 006, Step: 175, Loss: 0.6522\n",
            "Epoch: 006, Step: 176, Loss: 0.6407\n",
            "Epoch: 006, Step: 177, Loss: 0.6550\n",
            "Epoch: 006, Step: 178, Loss: 0.7297\n",
            "Epoch: 006, Step: 179, Loss: 0.6873\n",
            "Epoch: 006, Step: 180, Loss: 0.6664\n",
            "Epoch: 006, Step: 181, Loss: 0.6191\n",
            "Epoch: 006, Step: 182, Loss: 0.6835\n",
            "Epoch: 006, Step: 183, Loss: 0.7675\n",
            "Epoch: 006, Step: 184, Loss: 0.7011\n",
            "Epoch: 006, Step: 185, Loss: 0.6787\n",
            "Epoch: 006, Step: 186, Loss: 0.7073\n",
            "Epoch: 006, Step: 187, Loss: 0.6926\n",
            "Epoch: 006, Step: 188, Loss: 0.6865\n",
            "Epoch: 006, Step: 189, Loss: 0.6468\n",
            "Epoch: 006, Step: 190, Loss: 0.6843\n",
            "Epoch: 006, Step: 191, Loss: 0.7820\n",
            "Epoch: 006, Step: 192, Loss: 0.7393\n",
            "Epoch: 006, Step: 193, Loss: 0.6948\n",
            "Epoch: 006, Step: 194, Loss: 0.7204\n",
            "Epoch: 006, Step: 195, Loss: 0.7395\n",
            "Epoch: 006, Step: 196, Loss: 0.6559\n",
            "Epoch: 006, Step: 197, Loss: 0.7228\n",
            "Epoch: 006, Step: 198, Loss: 0.7380\n",
            "Epoch: 006, Step: 199, Loss: 0.7664\n",
            "Epoch: 006, Step: 200, Loss: 0.6543\n",
            "Epoch: 006, Step: 201, Loss: 0.6711\n",
            "Epoch: 006, Step: 202, Loss: 0.7422\n",
            "Epoch: 006, Step: 203, Loss: 0.6264\n",
            "Epoch: 006, Step: 204, Loss: 0.6307\n",
            "Epoch: 006, Step: 205, Loss: 0.5885\n",
            "Epoch: 006, Step: 206, Loss: 0.5779\n",
            "Epoch: 006, Step: 000, Val Loss: 0.6267\n",
            "Epoch: 006, Step: 001, Val Loss: 0.7055\n",
            "Epoch: 006, Step: 002, Val Loss: 0.6267\n",
            "Epoch: 006, Step: 003, Val Loss: 0.5864\n",
            "Epoch: 006, Step: 004, Val Loss: 0.7055\n",
            "Epoch: 006, Step: 005, Val Loss: 0.5478\n",
            "Epoch: 006, Step: 006, Val Loss: 0.7055\n",
            "Epoch: 006, Step: 007, Val Loss: 0.6267\n",
            "Epoch: 006, Step: 008, Val Loss: 0.7844\n",
            "Epoch: 006, Step: 009, Val Loss: 0.7844\n",
            "Epoch: 006, Step: 010, Val Loss: 0.8633\n",
            "Epoch: 006, Step: 011, Val Loss: 0.7055\n",
            "Epoch: 006, Step: 012, Val Loss: 0.7055\n",
            "Epoch: 006, Step: 013, Val Loss: 0.7055\n",
            "Epoch: 006, Step: 014, Val Loss: 0.8603\n",
            "Epoch: 006, Step: 015, Val Loss: 0.7055\n",
            "Epoch: 006, Step: 016, Val Loss: 0.7844\n",
            "Epoch: 006, Step: 017, Val Loss: 0.6267\n",
            "Epoch: 006, Step: 018, Val Loss: 0.7165\n",
            "Epoch: 006, Step: 019, Val Loss: 0.6475\n",
            "Epoch: 006, Step: 020, Val Loss: 0.6267\n",
            "Epoch: 006, Step: 021, Val Loss: 0.7844\n",
            "Epoch: 006, Step: 022, Val Loss: 0.7055\n",
            "Epoch: 006, Step: 023, Val Loss: 0.7844\n",
            "Epoch: 006, Step: 024, Val Loss: 0.5478\n",
            "Epoch: 006, Step: 025, Val Loss: 0.6267\n",
            "Epoch: 006, Step: 026, Val Loss: 0.5478\n",
            "Epoch: 006, Step: 027, Val Loss: 0.6267\n",
            "Epoch: 006, Step: 028, Val Loss: 0.5478\n",
            "Epoch: 006, Step: 029, Val Loss: 0.7844\n",
            "Epoch: 006, Step: 030, Val Loss: 0.7055\n",
            "Epoch: 006, Step: 031, Val Loss: 0.8473\n",
            "Epoch: 006, Step: 032, Val Loss: 0.6267\n",
            "Epoch: 006, Step: 033, Val Loss: 0.6267\n",
            "Epoch: 006, Step: 034, Val Loss: 0.7055\n",
            "Epoch: 006, Step: 035, Val Loss: 0.6267\n",
            "Epoch: 006, Step: 036, Val Loss: 0.6267\n",
            "Epoch: 006, Step: 037, Val Loss: 0.7055\n",
            "Epoch: 006, Step: 038, Val Loss: 0.5478\n",
            "Epoch: 006, Step: 039, Val Loss: 0.7055\n",
            "Epoch: 006, Step: 040, Val Loss: 0.6267\n",
            "Epoch: 006, Step: 041, Val Loss: 0.7055\n",
            "Epoch: 006, Step: 042, Val Loss: 0.7844\n",
            "Epoch: 006, Step: 043, Val Loss: 0.7055\n",
            "Epoch: 006, Step: 044, Val Loss: 0.7055\n",
            "Epoch: 006, Step: 045, Val Loss: 0.7055\n",
            "Epoch: 006, Step: 046, Val Loss: 0.5721\n",
            "Epoch: 006, Step: 047, Val Loss: 0.6267\n",
            "Epoch: 006, Step: 048, Val Loss: 0.6506\n",
            "Epoch: 006, Step: 049, Val Loss: 0.7055\n",
            "Epoch: 006, Step: 050, Val Loss: 0.7055\n",
            "Epoch: 006, Step: 051, Val Loss: 0.7844\n",
            "Epoch: 006, Step: 052, Val Loss: 0.6076\n",
            "Epoch: 006, Step: 053, Val Loss: 0.7055\n",
            "Epoch: 006, Step: 054, Val Loss: 0.6267\n",
            "Epoch: 006, Step: 055, Val Loss: 0.6267\n",
            "Epoch: 006, Step: 056, Val Loss: 0.5478\n",
            "Epoch: 006, Step: 057, Val Loss: 0.6267\n",
            "Epoch: 006, Step: 058, Val Loss: 0.7055\n",
            "Epoch: 007, Step: 000, Loss: 0.7245\n",
            "Epoch: 007, Step: 001, Loss: 0.6281\n",
            "Epoch: 007, Step: 002, Loss: 0.5966\n",
            "Epoch: 007, Step: 003, Loss: 0.4874\n",
            "Epoch: 007, Step: 004, Loss: 0.8444\n",
            "Epoch: 007, Step: 005, Loss: 0.6689\n",
            "Epoch: 007, Step: 006, Loss: 0.8280\n",
            "Epoch: 007, Step: 007, Loss: 0.7449\n",
            "Epoch: 007, Step: 008, Loss: 0.5664\n",
            "Epoch: 007, Step: 009, Loss: 0.7694\n",
            "Epoch: 007, Step: 010, Loss: 0.6355\n",
            "Epoch: 007, Step: 011, Loss: 0.5843\n",
            "Epoch: 007, Step: 012, Loss: 0.7739\n",
            "Epoch: 007, Step: 013, Loss: 0.8352\n",
            "Epoch: 007, Step: 014, Loss: 0.7805\n",
            "Epoch: 007, Step: 015, Loss: 0.6477\n",
            "Epoch: 007, Step: 016, Loss: 0.7029\n",
            "Epoch: 007, Step: 017, Loss: 0.6241\n",
            "Epoch: 007, Step: 018, Loss: 0.6592\n",
            "Epoch: 007, Step: 019, Loss: 0.7530\n",
            "Epoch: 007, Step: 020, Loss: 0.5863\n",
            "Epoch: 007, Step: 021, Loss: 0.6813\n",
            "Epoch: 007, Step: 022, Loss: 0.6334\n",
            "Epoch: 007, Step: 023, Loss: 0.8043\n",
            "Epoch: 007, Step: 024, Loss: 0.8165\n",
            "Epoch: 007, Step: 025, Loss: 0.7300\n",
            "Epoch: 007, Step: 026, Loss: 0.6642\n",
            "Epoch: 007, Step: 027, Loss: 0.6183\n",
            "Epoch: 007, Step: 028, Loss: 0.8705\n",
            "Epoch: 007, Step: 029, Loss: 0.7618\n",
            "Epoch: 007, Step: 030, Loss: 0.6776\n",
            "Epoch: 007, Step: 031, Loss: 0.5879\n",
            "Epoch: 007, Step: 032, Loss: 0.6845\n",
            "Epoch: 007, Step: 033, Loss: 0.7885\n",
            "Epoch: 007, Step: 034, Loss: 0.5537\n",
            "Epoch: 007, Step: 035, Loss: 0.6166\n",
            "Epoch: 007, Step: 036, Loss: 0.6096\n",
            "Epoch: 007, Step: 037, Loss: 0.7279\n",
            "Epoch: 007, Step: 038, Loss: 0.7331\n",
            "Epoch: 007, Step: 039, Loss: 0.6739\n",
            "Epoch: 007, Step: 040, Loss: 0.5673\n",
            "Epoch: 007, Step: 041, Loss: 0.5706\n",
            "Epoch: 007, Step: 042, Loss: 0.6453\n",
            "Epoch: 007, Step: 043, Loss: 0.7438\n",
            "Epoch: 007, Step: 044, Loss: 0.5015\n",
            "Epoch: 007, Step: 045, Loss: 0.7921\n",
            "Epoch: 007, Step: 046, Loss: 0.6340\n",
            "Epoch: 007, Step: 047, Loss: 0.6876\n",
            "Epoch: 007, Step: 048, Loss: 0.6114\n",
            "Epoch: 007, Step: 049, Loss: 0.8360\n",
            "Epoch: 007, Step: 050, Loss: 0.6898\n",
            "Epoch: 007, Step: 051, Loss: 0.6976\n",
            "Epoch: 007, Step: 052, Loss: 0.6324\n",
            "Epoch: 007, Step: 053, Loss: 0.8639\n",
            "Epoch: 007, Step: 054, Loss: 0.5841\n",
            "Epoch: 007, Step: 055, Loss: 0.8902\n",
            "Epoch: 007, Step: 056, Loss: 0.6534\n",
            "Epoch: 007, Step: 057, Loss: 0.5828\n",
            "Epoch: 007, Step: 058, Loss: 0.7824\n",
            "Epoch: 007, Step: 059, Loss: 0.6011\n",
            "Epoch: 007, Step: 060, Loss: 0.6022\n",
            "Epoch: 007, Step: 061, Loss: 0.6379\n",
            "Epoch: 007, Step: 062, Loss: 0.5909\n",
            "Epoch: 007, Step: 063, Loss: 0.6895\n",
            "Epoch: 007, Step: 064, Loss: 0.5726\n",
            "Epoch: 007, Step: 065, Loss: 0.5638\n",
            "Epoch: 007, Step: 066, Loss: 0.6152\n",
            "Epoch: 007, Step: 067, Loss: 0.7581\n",
            "Epoch: 007, Step: 068, Loss: 0.7418\n",
            "Epoch: 007, Step: 069, Loss: 0.6324\n",
            "Epoch: 007, Step: 070, Loss: 0.7780\n",
            "Epoch: 007, Step: 071, Loss: 1.0483\n",
            "Epoch: 007, Step: 072, Loss: 0.5588\n",
            "Epoch: 007, Step: 073, Loss: 0.7913\n",
            "Epoch: 007, Step: 074, Loss: 0.6635\n",
            "Epoch: 007, Step: 075, Loss: 0.6199\n",
            "Epoch: 007, Step: 076, Loss: 0.7083\n",
            "Epoch: 007, Step: 077, Loss: 0.7479\n",
            "Epoch: 007, Step: 078, Loss: 0.7518\n",
            "Epoch: 007, Step: 079, Loss: 0.7776\n",
            "Epoch: 007, Step: 080, Loss: 0.5628\n",
            "Epoch: 007, Step: 081, Loss: 0.6821\n",
            "Epoch: 007, Step: 082, Loss: 0.6784\n",
            "Epoch: 007, Step: 083, Loss: 0.6033\n",
            "Epoch: 007, Step: 084, Loss: 0.6876\n",
            "Epoch: 007, Step: 085, Loss: 0.6468\n",
            "Epoch: 007, Step: 086, Loss: 0.7277\n",
            "Epoch: 007, Step: 087, Loss: 0.7018\n",
            "Epoch: 007, Step: 088, Loss: 0.6295\n",
            "Epoch: 007, Step: 089, Loss: 0.6224\n",
            "Epoch: 007, Step: 090, Loss: 0.7049\n",
            "Epoch: 007, Step: 091, Loss: 0.7067\n",
            "Epoch: 007, Step: 092, Loss: 0.6364\n",
            "Epoch: 007, Step: 093, Loss: 0.6102\n",
            "Epoch: 007, Step: 094, Loss: 0.7024\n",
            "Epoch: 007, Step: 095, Loss: 0.6731\n",
            "Epoch: 007, Step: 096, Loss: 0.7149\n",
            "Epoch: 007, Step: 097, Loss: 0.6743\n",
            "Epoch: 007, Step: 098, Loss: 0.6184\n",
            "Epoch: 007, Step: 099, Loss: 0.6252\n",
            "Epoch: 007, Step: 100, Loss: 0.5427\n",
            "Epoch: 007, Step: 101, Loss: 0.5423\n",
            "Epoch: 007, Step: 102, Loss: 0.7279\n",
            "Epoch: 007, Step: 103, Loss: 0.6019\n",
            "Epoch: 007, Step: 104, Loss: 0.6939\n",
            "Epoch: 007, Step: 105, Loss: 0.6230\n",
            "Epoch: 007, Step: 106, Loss: 0.4567\n",
            "Epoch: 007, Step: 107, Loss: 0.6283\n",
            "Epoch: 007, Step: 108, Loss: 0.9334\n",
            "Epoch: 007, Step: 109, Loss: 0.9113\n",
            "Epoch: 007, Step: 110, Loss: 0.7196\n",
            "Epoch: 007, Step: 111, Loss: 0.7600\n",
            "Epoch: 007, Step: 112, Loss: 0.7421\n",
            "Epoch: 007, Step: 113, Loss: 0.6424\n",
            "Epoch: 007, Step: 114, Loss: 0.7322\n",
            "Epoch: 007, Step: 115, Loss: 0.8446\n",
            "Epoch: 007, Step: 116, Loss: 0.8231\n",
            "Epoch: 007, Step: 117, Loss: 0.9254\n",
            "Epoch: 007, Step: 118, Loss: 0.7350\n",
            "Epoch: 007, Step: 119, Loss: 0.6678\n",
            "Epoch: 007, Step: 120, Loss: 0.7304\n",
            "Epoch: 007, Step: 121, Loss: 0.7052\n",
            "Epoch: 007, Step: 122, Loss: 0.8234\n",
            "Epoch: 007, Step: 123, Loss: 0.6698\n",
            "Epoch: 007, Step: 124, Loss: 0.6353\n",
            "Epoch: 007, Step: 125, Loss: 0.6772\n",
            "Epoch: 007, Step: 126, Loss: 0.5993\n",
            "Epoch: 007, Step: 127, Loss: 0.6885\n",
            "Epoch: 007, Step: 128, Loss: 0.6628\n",
            "Epoch: 007, Step: 129, Loss: 0.8997\n",
            "Epoch: 007, Step: 130, Loss: 0.6487\n",
            "Epoch: 007, Step: 131, Loss: 0.6192\n",
            "Epoch: 007, Step: 132, Loss: 0.7605\n",
            "Epoch: 007, Step: 133, Loss: 0.6538\n",
            "Epoch: 007, Step: 134, Loss: 0.7377\n",
            "Epoch: 007, Step: 135, Loss: 0.7197\n",
            "Epoch: 007, Step: 136, Loss: 0.6258\n",
            "Epoch: 007, Step: 137, Loss: 0.6992\n",
            "Epoch: 007, Step: 138, Loss: 0.6625\n",
            "Epoch: 007, Step: 139, Loss: 0.6909\n",
            "Epoch: 007, Step: 140, Loss: 0.6444\n",
            "Epoch: 007, Step: 141, Loss: 0.6361\n",
            "Epoch: 007, Step: 142, Loss: 0.6695\n",
            "Epoch: 007, Step: 143, Loss: 0.6560\n",
            "Epoch: 007, Step: 144, Loss: 0.5584\n",
            "Epoch: 007, Step: 145, Loss: 0.6343\n",
            "Epoch: 007, Step: 146, Loss: 0.7350\n",
            "Epoch: 007, Step: 147, Loss: 0.7571\n",
            "Epoch: 007, Step: 148, Loss: 0.5643\n",
            "Epoch: 007, Step: 149, Loss: 0.5706\n",
            "Epoch: 007, Step: 150, Loss: 0.7776\n",
            "Epoch: 007, Step: 151, Loss: 0.6673\n",
            "Epoch: 007, Step: 152, Loss: 0.5945\n",
            "Epoch: 007, Step: 153, Loss: 0.6546\n",
            "Epoch: 007, Step: 154, Loss: 0.5439\n",
            "Epoch: 007, Step: 155, Loss: 0.5849\n",
            "Epoch: 007, Step: 156, Loss: 0.7034\n",
            "Epoch: 007, Step: 157, Loss: 0.6659\n",
            "Epoch: 007, Step: 158, Loss: 0.6457\n",
            "Epoch: 007, Step: 159, Loss: 0.8138\n",
            "Epoch: 007, Step: 160, Loss: 0.6443\n",
            "Epoch: 007, Step: 161, Loss: 0.5943\n",
            "Epoch: 007, Step: 162, Loss: 0.5448\n",
            "Epoch: 007, Step: 163, Loss: 0.9933\n",
            "Epoch: 007, Step: 164, Loss: 0.8711\n",
            "Epoch: 007, Step: 165, Loss: 0.7030\n",
            "Epoch: 007, Step: 166, Loss: 0.7033\n",
            "Epoch: 007, Step: 167, Loss: 0.9164\n",
            "Epoch: 007, Step: 168, Loss: 0.6378\n",
            "Epoch: 007, Step: 169, Loss: 0.7329\n",
            "Epoch: 007, Step: 170, Loss: 0.6687\n",
            "Epoch: 007, Step: 171, Loss: 0.6491\n",
            "Epoch: 007, Step: 172, Loss: 0.6703\n",
            "Epoch: 007, Step: 173, Loss: 0.5817\n",
            "Epoch: 007, Step: 174, Loss: 0.6107\n",
            "Epoch: 007, Step: 175, Loss: 0.7920\n",
            "Epoch: 007, Step: 176, Loss: 0.7853\n",
            "Epoch: 007, Step: 177, Loss: 0.7554\n",
            "Epoch: 007, Step: 178, Loss: 0.7175\n",
            "Epoch: 007, Step: 179, Loss: 0.6454\n",
            "Epoch: 007, Step: 180, Loss: 0.6652\n",
            "Epoch: 007, Step: 181, Loss: 0.6560\n",
            "Epoch: 007, Step: 182, Loss: 0.5642\n",
            "Epoch: 007, Step: 183, Loss: 0.6742\n",
            "Epoch: 007, Step: 184, Loss: 0.7902\n",
            "Epoch: 007, Step: 185, Loss: 0.6251\n",
            "Epoch: 007, Step: 186, Loss: 0.6061\n",
            "Epoch: 007, Step: 187, Loss: 0.7026\n",
            "Epoch: 007, Step: 188, Loss: 0.6293\n",
            "Epoch: 007, Step: 189, Loss: 0.7372\n",
            "Epoch: 007, Step: 190, Loss: 0.6660\n",
            "Epoch: 007, Step: 191, Loss: 0.6816\n",
            "Epoch: 007, Step: 192, Loss: 0.6681\n",
            "Epoch: 007, Step: 193, Loss: 0.5844\n",
            "Epoch: 007, Step: 194, Loss: 0.5557\n",
            "Epoch: 007, Step: 195, Loss: 0.7027\n",
            "Epoch: 007, Step: 196, Loss: 0.6255\n",
            "Epoch: 007, Step: 197, Loss: 0.6623\n",
            "Epoch: 007, Step: 198, Loss: 0.8505\n",
            "Epoch: 007, Step: 199, Loss: 0.7248\n",
            "Epoch: 007, Step: 200, Loss: 0.6510\n",
            "Epoch: 007, Step: 201, Loss: 0.8692\n",
            "Epoch: 007, Step: 202, Loss: 0.6784\n",
            "Epoch: 007, Step: 203, Loss: 0.8114\n",
            "Epoch: 007, Step: 204, Loss: 0.7299\n",
            "Epoch: 007, Step: 205, Loss: 0.6169\n",
            "Epoch: 007, Step: 206, Loss: 0.9840\n",
            "Epoch: 007, Step: 000, Val Loss: 0.7207\n",
            "Epoch: 007, Step: 001, Val Loss: 0.6712\n",
            "Epoch: 007, Step: 002, Val Loss: 0.8249\n",
            "Epoch: 007, Step: 003, Val Loss: 0.6123\n",
            "Epoch: 007, Step: 004, Val Loss: 0.5779\n",
            "Epoch: 007, Step: 005, Val Loss: 0.7245\n",
            "Epoch: 007, Step: 006, Val Loss: 0.7503\n",
            "Epoch: 007, Step: 007, Val Loss: 0.7673\n",
            "Epoch: 007, Step: 008, Val Loss: 0.8378\n",
            "Epoch: 007, Step: 009, Val Loss: 0.7792\n",
            "Epoch: 007, Step: 010, Val Loss: 0.6165\n",
            "Epoch: 007, Step: 011, Val Loss: 0.6255\n",
            "Epoch: 007, Step: 012, Val Loss: 0.6897\n",
            "Epoch: 007, Step: 013, Val Loss: 0.7880\n",
            "Epoch: 007, Step: 014, Val Loss: 0.6046\n",
            "Epoch: 007, Step: 015, Val Loss: 0.6513\n",
            "Epoch: 007, Step: 016, Val Loss: 0.4904\n",
            "Epoch: 007, Step: 017, Val Loss: 0.7142\n",
            "Epoch: 007, Step: 018, Val Loss: 0.7837\n",
            "Epoch: 007, Step: 019, Val Loss: 0.5716\n",
            "Epoch: 007, Step: 020, Val Loss: 0.8054\n",
            "Epoch: 007, Step: 021, Val Loss: 0.7099\n",
            "Epoch: 007, Step: 022, Val Loss: 0.7215\n",
            "Epoch: 007, Step: 023, Val Loss: 0.5938\n",
            "Epoch: 007, Step: 024, Val Loss: 0.5151\n",
            "Epoch: 007, Step: 025, Val Loss: 0.6921\n",
            "Epoch: 007, Step: 026, Val Loss: 0.6160\n",
            "Epoch: 007, Step: 027, Val Loss: 0.7737\n",
            "Epoch: 007, Step: 028, Val Loss: 0.7412\n",
            "Epoch: 007, Step: 029, Val Loss: 0.5883\n",
            "Epoch: 007, Step: 030, Val Loss: 0.7656\n",
            "Epoch: 007, Step: 031, Val Loss: 0.5166\n",
            "Epoch: 007, Step: 032, Val Loss: 0.5267\n",
            "Epoch: 007, Step: 033, Val Loss: 0.6125\n",
            "Epoch: 007, Step: 034, Val Loss: 0.7123\n",
            "Epoch: 007, Step: 035, Val Loss: 0.6981\n",
            "Epoch: 007, Step: 036, Val Loss: 0.6195\n",
            "Epoch: 007, Step: 037, Val Loss: 0.7088\n",
            "Epoch: 007, Step: 038, Val Loss: 0.7978\n",
            "Epoch: 007, Step: 039, Val Loss: 0.5309\n",
            "Epoch: 007, Step: 040, Val Loss: 0.6864\n",
            "Epoch: 007, Step: 041, Val Loss: 0.6211\n",
            "Epoch: 007, Step: 042, Val Loss: 0.7028\n",
            "Epoch: 007, Step: 043, Val Loss: 0.5474\n",
            "Epoch: 007, Step: 044, Val Loss: 0.7126\n",
            "Epoch: 007, Step: 045, Val Loss: 0.7087\n",
            "Epoch: 007, Step: 046, Val Loss: 0.6221\n",
            "Epoch: 007, Step: 047, Val Loss: 0.5140\n",
            "Epoch: 007, Step: 048, Val Loss: 0.6699\n",
            "Epoch: 007, Step: 049, Val Loss: 0.6118\n",
            "Epoch: 007, Step: 050, Val Loss: 0.6092\n",
            "Epoch: 007, Step: 051, Val Loss: 0.6653\n",
            "Epoch: 007, Step: 052, Val Loss: 0.7799\n",
            "Epoch: 007, Step: 053, Val Loss: 0.8069\n",
            "Epoch: 007, Step: 054, Val Loss: 0.7197\n",
            "Epoch: 007, Step: 055, Val Loss: 0.7186\n",
            "Epoch: 007, Step: 056, Val Loss: 0.6215\n",
            "Epoch: 007, Step: 057, Val Loss: 0.6112\n",
            "Epoch: 007, Step: 058, Val Loss: 0.5886\n",
            "Epoch: 008, Step: 000, Loss: 0.6112\n",
            "Epoch: 008, Step: 001, Loss: 0.6258\n",
            "Epoch: 008, Step: 002, Loss: 0.6373\n",
            "Epoch: 008, Step: 003, Loss: 0.7880\n",
            "Epoch: 008, Step: 004, Loss: 0.7033\n",
            "Epoch: 008, Step: 005, Loss: 0.6130\n",
            "Epoch: 008, Step: 006, Loss: 0.5839\n",
            "Epoch: 008, Step: 007, Loss: 0.6050\n",
            "Epoch: 008, Step: 008, Loss: 0.6672\n",
            "Epoch: 008, Step: 009, Loss: 0.6259\n",
            "Epoch: 008, Step: 010, Loss: 0.6574\n",
            "Epoch: 008, Step: 011, Loss: 0.6205\n",
            "Epoch: 008, Step: 012, Loss: 0.6528\n",
            "Epoch: 008, Step: 013, Loss: 0.5680\n",
            "Epoch: 008, Step: 014, Loss: 0.7390\n",
            "Epoch: 008, Step: 015, Loss: 0.6422\n",
            "Epoch: 008, Step: 016, Loss: 0.5861\n",
            "Epoch: 008, Step: 017, Loss: 0.8148\n",
            "Epoch: 008, Step: 018, Loss: 0.6103\n",
            "Epoch: 008, Step: 019, Loss: 0.6034\n",
            "Epoch: 008, Step: 020, Loss: 0.6780\n",
            "Epoch: 008, Step: 021, Loss: 0.6321\n",
            "Epoch: 008, Step: 022, Loss: 0.6058\n",
            "Epoch: 008, Step: 023, Loss: 0.9112\n",
            "Epoch: 008, Step: 024, Loss: 0.5922\n",
            "Epoch: 008, Step: 025, Loss: 0.5923\n",
            "Epoch: 008, Step: 026, Loss: 0.5181\n",
            "Epoch: 008, Step: 027, Loss: 0.8829\n",
            "Epoch: 008, Step: 028, Loss: 0.5435\n",
            "Epoch: 008, Step: 029, Loss: 0.5520\n",
            "Epoch: 008, Step: 030, Loss: 0.6321\n",
            "Epoch: 008, Step: 031, Loss: 0.6521\n",
            "Epoch: 008, Step: 032, Loss: 0.7275\n",
            "Epoch: 008, Step: 033, Loss: 0.7327\n",
            "Epoch: 008, Step: 034, Loss: 0.5796\n",
            "Epoch: 008, Step: 035, Loss: 0.8604\n",
            "Epoch: 008, Step: 036, Loss: 0.8219\n",
            "Epoch: 008, Step: 037, Loss: 0.8240\n",
            "Epoch: 008, Step: 038, Loss: 0.5901\n",
            "Epoch: 008, Step: 039, Loss: 0.8284\n",
            "Epoch: 008, Step: 040, Loss: 0.5272\n",
            "Epoch: 008, Step: 041, Loss: 0.9329\n",
            "Epoch: 008, Step: 042, Loss: 0.6771\n",
            "Epoch: 008, Step: 043, Loss: 0.9320\n",
            "Epoch: 008, Step: 044, Loss: 0.6181\n",
            "Epoch: 008, Step: 045, Loss: 0.6902\n",
            "Epoch: 008, Step: 046, Loss: 0.7365\n",
            "Epoch: 008, Step: 047, Loss: 0.6887\n",
            "Epoch: 008, Step: 048, Loss: 0.5895\n",
            "Epoch: 008, Step: 049, Loss: 0.7426\n",
            "Epoch: 008, Step: 050, Loss: 0.8059\n",
            "Epoch: 008, Step: 051, Loss: 0.7135\n",
            "Epoch: 008, Step: 052, Loss: 0.7876\n",
            "Epoch: 008, Step: 053, Loss: 0.6885\n",
            "Epoch: 008, Step: 054, Loss: 0.7697\n",
            "Epoch: 008, Step: 055, Loss: 0.5263\n",
            "Epoch: 008, Step: 056, Loss: 1.1718\n",
            "Epoch: 008, Step: 057, Loss: 0.6316\n",
            "Epoch: 008, Step: 058, Loss: 0.6727\n",
            "Epoch: 008, Step: 059, Loss: 0.7953\n",
            "Epoch: 008, Step: 060, Loss: 0.6490\n",
            "Epoch: 008, Step: 061, Loss: 0.5273\n",
            "Epoch: 008, Step: 062, Loss: 0.6624\n",
            "Epoch: 008, Step: 063, Loss: 0.6360\n",
            "Epoch: 008, Step: 064, Loss: 0.5368\n",
            "Epoch: 008, Step: 065, Loss: 0.7224\n",
            "Epoch: 008, Step: 066, Loss: 0.7078\n",
            "Epoch: 008, Step: 067, Loss: 0.6232\n",
            "Epoch: 008, Step: 068, Loss: 0.4961\n",
            "Epoch: 008, Step: 069, Loss: 0.6659\n",
            "Epoch: 008, Step: 070, Loss: 0.5218\n",
            "Epoch: 008, Step: 071, Loss: 0.6839\n",
            "Epoch: 008, Step: 072, Loss: 0.4617\n",
            "Epoch: 008, Step: 073, Loss: 1.0066\n",
            "Epoch: 008, Step: 074, Loss: 0.7214\n",
            "Epoch: 008, Step: 075, Loss: 0.7332\n",
            "Epoch: 008, Step: 076, Loss: 0.5850\n",
            "Epoch: 008, Step: 077, Loss: 0.5839\n",
            "Epoch: 008, Step: 078, Loss: 0.7498\n",
            "Epoch: 008, Step: 079, Loss: 0.7343\n",
            "Epoch: 008, Step: 080, Loss: 0.9139\n",
            "Epoch: 008, Step: 081, Loss: 0.4741\n",
            "Epoch: 008, Step: 082, Loss: 0.5079\n",
            "Epoch: 008, Step: 083, Loss: 0.8392\n",
            "Epoch: 008, Step: 084, Loss: 0.8056\n",
            "Epoch: 008, Step: 085, Loss: 0.6882\n",
            "Epoch: 008, Step: 086, Loss: 0.4364\n",
            "Epoch: 008, Step: 087, Loss: 0.6636\n",
            "Epoch: 008, Step: 088, Loss: 0.6991\n",
            "Epoch: 008, Step: 089, Loss: 0.6004\n",
            "Epoch: 008, Step: 090, Loss: 0.6537\n",
            "Epoch: 008, Step: 091, Loss: 0.6942\n",
            "Epoch: 008, Step: 092, Loss: 0.4900\n",
            "Epoch: 008, Step: 093, Loss: 0.7308\n",
            "Epoch: 008, Step: 094, Loss: 0.6348\n",
            "Epoch: 008, Step: 095, Loss: 0.6495\n",
            "Epoch: 008, Step: 096, Loss: 0.5462\n",
            "Epoch: 008, Step: 097, Loss: 0.5282\n",
            "Epoch: 008, Step: 098, Loss: 0.5962\n",
            "Epoch: 008, Step: 099, Loss: 0.5563\n",
            "Epoch: 008, Step: 100, Loss: 0.5592\n",
            "Epoch: 008, Step: 101, Loss: 0.6202\n",
            "Epoch: 008, Step: 102, Loss: 0.8185\n",
            "Epoch: 008, Step: 103, Loss: 0.6563\n",
            "Epoch: 008, Step: 104, Loss: 0.5630\n",
            "Epoch: 008, Step: 105, Loss: 0.4652\n",
            "Epoch: 008, Step: 106, Loss: 0.4743\n",
            "Epoch: 008, Step: 107, Loss: 0.6995\n",
            "Epoch: 008, Step: 108, Loss: 0.5983\n",
            "Epoch: 008, Step: 109, Loss: 0.4978\n",
            "Epoch: 008, Step: 110, Loss: 0.3551\n",
            "Epoch: 008, Step: 111, Loss: 1.0259\n",
            "Epoch: 008, Step: 112, Loss: 0.6067\n",
            "Epoch: 008, Step: 113, Loss: 0.6004\n",
            "Epoch: 008, Step: 114, Loss: 1.0879\n",
            "Epoch: 008, Step: 115, Loss: 0.8321\n",
            "Epoch: 008, Step: 116, Loss: 0.6780\n",
            "Epoch: 008, Step: 117, Loss: 0.5999\n",
            "Epoch: 008, Step: 118, Loss: 0.4947\n",
            "Epoch: 008, Step: 119, Loss: 0.5820\n",
            "Epoch: 008, Step: 120, Loss: 0.5144\n",
            "Epoch: 008, Step: 121, Loss: 0.5525\n",
            "Epoch: 008, Step: 122, Loss: 0.6925\n",
            "Epoch: 008, Step: 123, Loss: 0.8723\n",
            "Epoch: 008, Step: 124, Loss: 0.9351\n",
            "Epoch: 008, Step: 125, Loss: 0.9849\n",
            "Epoch: 008, Step: 126, Loss: 0.7438\n",
            "Epoch: 008, Step: 127, Loss: 0.7942\n",
            "Epoch: 008, Step: 128, Loss: 0.5522\n",
            "Epoch: 008, Step: 129, Loss: 0.3973\n",
            "Epoch: 008, Step: 130, Loss: 1.1177\n",
            "Epoch: 008, Step: 131, Loss: 0.5898\n",
            "Epoch: 008, Step: 132, Loss: 0.3900\n",
            "Epoch: 008, Step: 133, Loss: 0.9591\n",
            "Epoch: 008, Step: 134, Loss: 0.5795\n",
            "Epoch: 008, Step: 135, Loss: 0.7052\n",
            "Epoch: 008, Step: 136, Loss: 0.7855\n",
            "Epoch: 008, Step: 137, Loss: 0.7969\n",
            "Epoch: 008, Step: 138, Loss: 0.5047\n",
            "Epoch: 008, Step: 139, Loss: 0.7520\n",
            "Epoch: 008, Step: 140, Loss: 0.5868\n",
            "Epoch: 008, Step: 141, Loss: 0.7902\n",
            "Epoch: 008, Step: 142, Loss: 0.7712\n",
            "Epoch: 008, Step: 143, Loss: 0.6436\n",
            "Epoch: 008, Step: 144, Loss: 0.7292\n",
            "Epoch: 008, Step: 145, Loss: 0.6071\n",
            "Epoch: 008, Step: 146, Loss: 0.6762\n",
            "Epoch: 008, Step: 147, Loss: 0.6521\n",
            "Epoch: 008, Step: 148, Loss: 0.6608\n",
            "Epoch: 008, Step: 149, Loss: 0.6812\n",
            "Epoch: 008, Step: 150, Loss: 0.6645\n",
            "Epoch: 008, Step: 151, Loss: 0.6614\n",
            "Epoch: 008, Step: 152, Loss: 0.8434\n",
            "Epoch: 008, Step: 153, Loss: 0.6586\n",
            "Epoch: 008, Step: 154, Loss: 0.6807\n",
            "Epoch: 008, Step: 155, Loss: 0.5978\n",
            "Epoch: 008, Step: 156, Loss: 0.6610\n",
            "Epoch: 008, Step: 157, Loss: 0.6481\n",
            "Epoch: 008, Step: 158, Loss: 0.8098\n",
            "Epoch: 008, Step: 159, Loss: 0.5999\n",
            "Epoch: 008, Step: 160, Loss: 0.6414\n",
            "Epoch: 008, Step: 161, Loss: 0.6495\n",
            "Epoch: 008, Step: 162, Loss: 0.6128\n",
            "Epoch: 008, Step: 163, Loss: 0.5156\n",
            "Epoch: 008, Step: 164, Loss: 0.6328\n",
            "Epoch: 008, Step: 165, Loss: 0.4890\n",
            "Epoch: 008, Step: 166, Loss: 0.5938\n",
            "Epoch: 008, Step: 167, Loss: 0.6043\n",
            "Epoch: 008, Step: 168, Loss: 0.3957\n",
            "Epoch: 008, Step: 169, Loss: 0.5546\n",
            "Epoch: 008, Step: 170, Loss: 0.6122\n",
            "Epoch: 008, Step: 171, Loss: 0.3968\n",
            "Epoch: 008, Step: 172, Loss: 0.8808\n",
            "Epoch: 008, Step: 173, Loss: 0.6224\n",
            "Epoch: 008, Step: 174, Loss: 0.8517\n",
            "Epoch: 008, Step: 175, Loss: 0.7516\n",
            "Epoch: 008, Step: 176, Loss: 0.6998\n",
            "Epoch: 008, Step: 177, Loss: 0.7198\n",
            "Epoch: 008, Step: 178, Loss: 0.3832\n",
            "Epoch: 008, Step: 179, Loss: 0.6127\n",
            "Epoch: 008, Step: 180, Loss: 0.6010\n",
            "Epoch: 008, Step: 181, Loss: 0.6502\n",
            "Epoch: 008, Step: 182, Loss: 1.0035\n",
            "Epoch: 008, Step: 183, Loss: 0.8629\n",
            "Epoch: 008, Step: 184, Loss: 0.4906\n",
            "Epoch: 008, Step: 185, Loss: 0.6021\n",
            "Epoch: 008, Step: 186, Loss: 0.4837\n",
            "Epoch: 008, Step: 187, Loss: 0.4338\n",
            "Epoch: 008, Step: 188, Loss: 0.4724\n",
            "Epoch: 008, Step: 189, Loss: 0.9697\n",
            "Epoch: 008, Step: 190, Loss: 0.4871\n",
            "Epoch: 008, Step: 191, Loss: 0.9653\n",
            "Epoch: 008, Step: 192, Loss: 0.7596\n",
            "Epoch: 008, Step: 193, Loss: 0.8665\n",
            "Epoch: 008, Step: 194, Loss: 0.9017\n",
            "Epoch: 008, Step: 195, Loss: 0.8069\n",
            "Epoch: 008, Step: 196, Loss: 0.9648\n",
            "Epoch: 008, Step: 197, Loss: 0.5938\n",
            "Epoch: 008, Step: 198, Loss: 0.7528\n",
            "Epoch: 008, Step: 199, Loss: 0.7311\n",
            "Epoch: 008, Step: 200, Loss: 0.9982\n",
            "Epoch: 008, Step: 201, Loss: 0.7340\n",
            "Epoch: 008, Step: 202, Loss: 0.6760\n",
            "Epoch: 008, Step: 203, Loss: 0.7736\n",
            "Epoch: 008, Step: 204, Loss: 0.7187\n",
            "Epoch: 008, Step: 205, Loss: 0.7012\n",
            "Epoch: 008, Step: 206, Loss: 0.6149\n",
            "Epoch: 008, Step: 000, Val Loss: 0.6214\n",
            "Epoch: 008, Step: 001, Val Loss: 0.6505\n",
            "Epoch: 008, Step: 002, Val Loss: 0.5718\n",
            "Epoch: 008, Step: 003, Val Loss: 0.7293\n",
            "Epoch: 008, Step: 004, Val Loss: 0.7956\n",
            "Epoch: 008, Step: 005, Val Loss: 0.6388\n",
            "Epoch: 008, Step: 006, Val Loss: 0.6172\n",
            "Epoch: 008, Step: 007, Val Loss: 0.5796\n",
            "Epoch: 008, Step: 008, Val Loss: 0.7525\n",
            "Epoch: 008, Step: 009, Val Loss: 0.6240\n",
            "Epoch: 008, Step: 010, Val Loss: 0.5683\n",
            "Epoch: 008, Step: 011, Val Loss: 0.7836\n",
            "Epoch: 008, Step: 012, Val Loss: 0.6964\n",
            "Epoch: 008, Step: 013, Val Loss: 0.6544\n",
            "Epoch: 008, Step: 014, Val Loss: 0.6265\n",
            "Epoch: 008, Step: 015, Val Loss: 0.4756\n",
            "Epoch: 008, Step: 016, Val Loss: 0.6336\n",
            "Epoch: 008, Step: 017, Val Loss: 0.8940\n",
            "Epoch: 008, Step: 018, Val Loss: 0.6432\n",
            "Epoch: 008, Step: 019, Val Loss: 0.6452\n",
            "Epoch: 008, Step: 020, Val Loss: 0.6742\n",
            "Epoch: 008, Step: 021, Val Loss: 0.6493\n",
            "Epoch: 008, Step: 022, Val Loss: 0.6400\n",
            "Epoch: 008, Step: 023, Val Loss: 0.6880\n",
            "Epoch: 008, Step: 024, Val Loss: 0.6347\n",
            "Epoch: 008, Step: 025, Val Loss: 0.5520\n",
            "Epoch: 008, Step: 026, Val Loss: 0.6420\n",
            "Epoch: 008, Step: 027, Val Loss: 0.6731\n",
            "Epoch: 008, Step: 028, Val Loss: 0.7402\n",
            "Epoch: 008, Step: 029, Val Loss: 0.6589\n",
            "Epoch: 008, Step: 030, Val Loss: 0.6299\n",
            "Epoch: 008, Step: 031, Val Loss: 0.5906\n",
            "Epoch: 008, Step: 032, Val Loss: 0.6986\n",
            "Epoch: 008, Step: 033, Val Loss: 0.7491\n",
            "Epoch: 008, Step: 034, Val Loss: 0.6393\n",
            "Epoch: 008, Step: 035, Val Loss: 0.6181\n",
            "Epoch: 008, Step: 036, Val Loss: 0.6930\n",
            "Epoch: 008, Step: 037, Val Loss: 0.7088\n",
            "Epoch: 008, Step: 038, Val Loss: 0.6115\n",
            "Epoch: 008, Step: 039, Val Loss: 0.6910\n",
            "Epoch: 008, Step: 040, Val Loss: 0.7008\n",
            "Epoch: 008, Step: 041, Val Loss: 0.5928\n",
            "Epoch: 008, Step: 042, Val Loss: 0.7190\n",
            "Epoch: 008, Step: 043, Val Loss: 0.7241\n",
            "Epoch: 008, Step: 044, Val Loss: 0.6585\n",
            "Epoch: 008, Step: 045, Val Loss: 0.7280\n",
            "Epoch: 008, Step: 046, Val Loss: 0.7539\n",
            "Epoch: 008, Step: 047, Val Loss: 0.5993\n",
            "Epoch: 008, Step: 048, Val Loss: 0.8924\n",
            "Epoch: 008, Step: 049, Val Loss: 0.6433\n",
            "Epoch: 008, Step: 050, Val Loss: 0.5776\n",
            "Epoch: 008, Step: 051, Val Loss: 0.7167\n",
            "Epoch: 008, Step: 052, Val Loss: 0.6461\n",
            "Epoch: 008, Step: 053, Val Loss: 0.6715\n",
            "Epoch: 008, Step: 054, Val Loss: 0.5602\n",
            "Epoch: 008, Step: 055, Val Loss: 0.7175\n",
            "Epoch: 008, Step: 056, Val Loss: 0.6616\n",
            "Epoch: 008, Step: 057, Val Loss: 0.8332\n",
            "Epoch: 008, Step: 058, Val Loss: 0.6627\n",
            "Epoch: 009, Step: 000, Loss: 0.5901\n",
            "Epoch: 009, Step: 001, Loss: 0.6886\n",
            "Epoch: 009, Step: 002, Loss: 0.6994\n",
            "Epoch: 009, Step: 003, Loss: 0.6936\n",
            "Epoch: 009, Step: 004, Loss: 0.7023\n",
            "Epoch: 009, Step: 005, Loss: 0.8560\n",
            "Epoch: 009, Step: 006, Loss: 0.6345\n",
            "Epoch: 009, Step: 007, Loss: 0.7295\n",
            "Epoch: 009, Step: 008, Loss: 0.6067\n",
            "Epoch: 009, Step: 009, Loss: 0.6862\n",
            "Epoch: 009, Step: 010, Loss: 0.6998\n",
            "Epoch: 009, Step: 011, Loss: 0.5587\n",
            "Epoch: 009, Step: 012, Loss: 0.6021\n",
            "Epoch: 009, Step: 013, Loss: 0.6791\n",
            "Epoch: 009, Step: 014, Loss: 0.7831\n",
            "Epoch: 009, Step: 015, Loss: 0.6702\n",
            "Epoch: 009, Step: 016, Loss: 0.5970\n",
            "Epoch: 009, Step: 017, Loss: 0.5081\n",
            "Epoch: 009, Step: 018, Loss: 0.7252\n",
            "Epoch: 009, Step: 019, Loss: 0.5797\n",
            "Epoch: 009, Step: 020, Loss: 0.2256\n",
            "Epoch: 009, Step: 021, Loss: 0.7406\n",
            "Epoch: 009, Step: 022, Loss: 0.4859\n",
            "Epoch: 009, Step: 023, Loss: 0.6442\n",
            "Epoch: 009, Step: 024, Loss: 0.5306\n",
            "Epoch: 009, Step: 025, Loss: 0.7502\n",
            "Epoch: 009, Step: 026, Loss: 0.5389\n",
            "Epoch: 009, Step: 027, Loss: 0.6715\n",
            "Epoch: 009, Step: 028, Loss: 0.6300\n",
            "Epoch: 009, Step: 029, Loss: 0.5545\n",
            "Epoch: 009, Step: 030, Loss: 0.4381\n",
            "Epoch: 009, Step: 031, Loss: 0.8811\n",
            "Epoch: 009, Step: 032, Loss: 0.6769\n",
            "Epoch: 009, Step: 033, Loss: 0.6659\n",
            "Epoch: 009, Step: 034, Loss: 0.6613\n",
            "Epoch: 009, Step: 035, Loss: 0.3636\n",
            "Epoch: 009, Step: 036, Loss: 1.1152\n",
            "Epoch: 009, Step: 037, Loss: 0.5892\n",
            "Epoch: 009, Step: 038, Loss: 0.6131\n",
            "Epoch: 009, Step: 039, Loss: 0.6841\n",
            "Epoch: 009, Step: 040, Loss: 0.4831\n",
            "Epoch: 009, Step: 041, Loss: 0.7095\n",
            "Epoch: 009, Step: 042, Loss: 0.5793\n",
            "Epoch: 009, Step: 043, Loss: 0.6049\n",
            "Epoch: 009, Step: 044, Loss: 0.8324\n",
            "Epoch: 009, Step: 045, Loss: 0.6126\n",
            "Epoch: 009, Step: 046, Loss: 0.5933\n",
            "Epoch: 009, Step: 047, Loss: 0.3635\n",
            "Epoch: 009, Step: 048, Loss: 0.6719\n",
            "Epoch: 009, Step: 049, Loss: 0.6307\n",
            "Epoch: 009, Step: 050, Loss: 0.5682\n",
            "Epoch: 009, Step: 051, Loss: 0.5828\n",
            "Epoch: 009, Step: 052, Loss: 0.9050\n",
            "Epoch: 009, Step: 053, Loss: 0.4780\n",
            "Epoch: 009, Step: 054, Loss: 0.8572\n",
            "Epoch: 009, Step: 055, Loss: 0.4439\n",
            "Epoch: 009, Step: 056, Loss: 0.5396\n",
            "Epoch: 009, Step: 057, Loss: 0.3684\n",
            "Epoch: 009, Step: 058, Loss: 0.8129\n",
            "Epoch: 009, Step: 059, Loss: 0.5727\n",
            "Epoch: 009, Step: 060, Loss: 0.5747\n",
            "Epoch: 009, Step: 061, Loss: 0.6283\n",
            "Epoch: 009, Step: 062, Loss: 1.1054\n",
            "Epoch: 009, Step: 063, Loss: 0.9829\n",
            "Epoch: 009, Step: 064, Loss: 0.4921\n",
            "Epoch: 009, Step: 065, Loss: 0.3264\n",
            "Epoch: 009, Step: 066, Loss: 0.3399\n",
            "Epoch: 009, Step: 067, Loss: 0.5451\n",
            "Epoch: 009, Step: 068, Loss: 0.3193\n",
            "Epoch: 009, Step: 069, Loss: 0.3642\n",
            "Epoch: 009, Step: 070, Loss: 0.7274\n",
            "Epoch: 009, Step: 071, Loss: 0.9146\n",
            "Epoch: 009, Step: 072, Loss: 0.7603\n",
            "Epoch: 009, Step: 073, Loss: 0.9916\n",
            "Epoch: 009, Step: 074, Loss: 0.6952\n",
            "Epoch: 009, Step: 075, Loss: 0.4008\n",
            "Epoch: 009, Step: 076, Loss: 1.0586\n",
            "Epoch: 009, Step: 077, Loss: 0.5087\n",
            "Epoch: 009, Step: 078, Loss: 0.6092\n",
            "Epoch: 009, Step: 079, Loss: 0.5608\n",
            "Epoch: 009, Step: 080, Loss: 0.7538\n",
            "Epoch: 009, Step: 081, Loss: 0.7930\n",
            "Epoch: 009, Step: 082, Loss: 0.7244\n",
            "Epoch: 009, Step: 083, Loss: 0.6566\n",
            "Epoch: 009, Step: 084, Loss: 0.6080\n",
            "Epoch: 009, Step: 085, Loss: 0.7713\n",
            "Epoch: 009, Step: 086, Loss: 0.8807\n",
            "Epoch: 009, Step: 087, Loss: 0.5670\n",
            "Epoch: 009, Step: 088, Loss: 0.6279\n",
            "Epoch: 009, Step: 089, Loss: 0.5321\n",
            "Epoch: 009, Step: 090, Loss: 0.8498\n",
            "Epoch: 009, Step: 091, Loss: 0.6999\n",
            "Epoch: 009, Step: 092, Loss: 0.5611\n",
            "Epoch: 009, Step: 093, Loss: 0.7494\n",
            "Epoch: 009, Step: 094, Loss: 0.5616\n",
            "Epoch: 009, Step: 095, Loss: 0.5233\n",
            "Epoch: 009, Step: 096, Loss: 0.5906\n",
            "Epoch: 009, Step: 097, Loss: 0.6721\n",
            "Epoch: 009, Step: 098, Loss: 0.6695\n",
            "Epoch: 009, Step: 099, Loss: 0.7897\n",
            "Epoch: 009, Step: 100, Loss: 0.5498\n",
            "Epoch: 009, Step: 101, Loss: 0.7327\n",
            "Epoch: 009, Step: 102, Loss: 0.6857\n",
            "Epoch: 009, Step: 103, Loss: 0.5587\n",
            "Epoch: 009, Step: 104, Loss: 0.5092\n",
            "Epoch: 009, Step: 105, Loss: 0.5336\n",
            "Epoch: 009, Step: 106, Loss: 0.7370\n",
            "Epoch: 009, Step: 107, Loss: 0.6085\n",
            "Epoch: 009, Step: 108, Loss: 0.7790\n",
            "Epoch: 009, Step: 109, Loss: 0.5303\n",
            "Epoch: 009, Step: 110, Loss: 0.5509\n",
            "Epoch: 009, Step: 111, Loss: 0.6642\n",
            "Epoch: 009, Step: 112, Loss: 0.4022\n",
            "Epoch: 009, Step: 113, Loss: 0.8398\n",
            "Epoch: 009, Step: 114, Loss: 0.5211\n",
            "Epoch: 009, Step: 115, Loss: 0.8206\n",
            "Epoch: 009, Step: 116, Loss: 0.6576\n",
            "Epoch: 009, Step: 117, Loss: 0.7210\n",
            "Epoch: 009, Step: 118, Loss: 0.6647\n",
            "Epoch: 009, Step: 119, Loss: 0.6015\n",
            "Epoch: 009, Step: 120, Loss: 0.4865\n",
            "Epoch: 009, Step: 121, Loss: 0.7520\n",
            "Epoch: 009, Step: 122, Loss: 0.4606\n",
            "Epoch: 009, Step: 123, Loss: 0.8057\n",
            "Epoch: 009, Step: 124, Loss: 0.4735\n",
            "Epoch: 009, Step: 125, Loss: 0.3207\n",
            "Epoch: 009, Step: 126, Loss: 0.7320\n",
            "Epoch: 009, Step: 127, Loss: 0.7718\n",
            "Epoch: 009, Step: 128, Loss: 0.9780\n",
            "Epoch: 009, Step: 129, Loss: 0.7180\n",
            "Epoch: 009, Step: 130, Loss: 0.6684\n",
            "Epoch: 009, Step: 131, Loss: 0.7060\n",
            "Epoch: 009, Step: 132, Loss: 0.6256\n",
            "Epoch: 009, Step: 133, Loss: 0.9884\n",
            "Epoch: 009, Step: 134, Loss: 0.8750\n",
            "Epoch: 009, Step: 135, Loss: 0.8009\n",
            "Epoch: 009, Step: 136, Loss: 0.5918\n",
            "Epoch: 009, Step: 137, Loss: 0.5551\n",
            "Epoch: 009, Step: 138, Loss: 0.7870\n",
            "Epoch: 009, Step: 139, Loss: 0.5929\n",
            "Epoch: 009, Step: 140, Loss: 0.6421\n",
            "Epoch: 009, Step: 141, Loss: 0.6816\n",
            "Epoch: 009, Step: 142, Loss: 0.4744\n",
            "Epoch: 009, Step: 143, Loss: 0.5327\n",
            "Epoch: 009, Step: 144, Loss: 0.7507\n",
            "Epoch: 009, Step: 145, Loss: 0.9704\n",
            "Epoch: 009, Step: 146, Loss: 0.7474\n",
            "Epoch: 009, Step: 147, Loss: 0.4606\n",
            "Epoch: 009, Step: 148, Loss: 0.6654\n",
            "Epoch: 009, Step: 149, Loss: 0.4256\n",
            "Epoch: 009, Step: 150, Loss: 0.5650\n",
            "Epoch: 009, Step: 151, Loss: 0.4482\n",
            "Epoch: 009, Step: 152, Loss: 0.5646\n",
            "Epoch: 009, Step: 153, Loss: 0.6080\n",
            "Epoch: 009, Step: 154, Loss: 0.6915\n",
            "Epoch: 009, Step: 155, Loss: 0.3030\n",
            "Epoch: 009, Step: 156, Loss: 0.7647\n",
            "Epoch: 009, Step: 157, Loss: 1.0293\n",
            "Epoch: 009, Step: 158, Loss: 0.4565\n",
            "Epoch: 009, Step: 159, Loss: 0.9763\n",
            "Epoch: 009, Step: 160, Loss: 0.9054\n",
            "Epoch: 009, Step: 161, Loss: 0.5632\n",
            "Epoch: 009, Step: 162, Loss: 0.9496\n",
            "Epoch: 009, Step: 163, Loss: 0.9327\n",
            "Epoch: 009, Step: 164, Loss: 0.5358\n",
            "Epoch: 009, Step: 165, Loss: 0.6615\n",
            "Epoch: 009, Step: 166, Loss: 0.7562\n",
            "Epoch: 009, Step: 167, Loss: 0.7864\n",
            "Epoch: 009, Step: 168, Loss: 0.6376\n",
            "Epoch: 009, Step: 169, Loss: 0.6879\n",
            "Epoch: 009, Step: 170, Loss: 0.6256\n",
            "Epoch: 009, Step: 171, Loss: 0.6755\n",
            "Epoch: 009, Step: 172, Loss: 0.6512\n",
            "Epoch: 009, Step: 173, Loss: 0.6562\n",
            "Epoch: 009, Step: 174, Loss: 0.7447\n",
            "Epoch: 009, Step: 175, Loss: 0.6338\n",
            "Epoch: 009, Step: 176, Loss: 0.7115\n",
            "Epoch: 009, Step: 177, Loss: 1.5811\n",
            "Epoch: 009, Step: 178, Loss: 0.7197\n",
            "Epoch: 009, Step: 179, Loss: 0.6103\n",
            "Epoch: 009, Step: 180, Loss: 0.5336\n",
            "Epoch: 009, Step: 181, Loss: 0.7259\n",
            "Epoch: 009, Step: 182, Loss: 0.7105\n",
            "Epoch: 009, Step: 183, Loss: 0.6919\n",
            "Epoch: 009, Step: 184, Loss: 0.7385\n",
            "Epoch: 009, Step: 185, Loss: 0.8940\n",
            "Epoch: 009, Step: 186, Loss: 0.6453\n",
            "Epoch: 009, Step: 187, Loss: 0.7883\n",
            "Epoch: 009, Step: 188, Loss: 0.6425\n",
            "Epoch: 009, Step: 189, Loss: 0.6377\n",
            "Epoch: 009, Step: 190, Loss: 0.8023\n",
            "Epoch: 009, Step: 191, Loss: 0.6551\n",
            "Epoch: 009, Step: 192, Loss: 0.7201\n",
            "Epoch: 009, Step: 193, Loss: 0.6047\n",
            "Epoch: 009, Step: 194, Loss: 0.7427\n",
            "Epoch: 009, Step: 195, Loss: 0.7273\n",
            "Epoch: 009, Step: 196, Loss: 0.6423\n",
            "Epoch: 009, Step: 197, Loss: 0.8244\n",
            "Epoch: 009, Step: 198, Loss: 0.7238\n",
            "Epoch: 009, Step: 199, Loss: 0.7187\n",
            "Epoch: 009, Step: 200, Loss: 0.6859\n",
            "Epoch: 009, Step: 201, Loss: 0.6785\n",
            "Epoch: 009, Step: 202, Loss: 0.6913\n",
            "Epoch: 009, Step: 203, Loss: 0.6737\n",
            "Epoch: 009, Step: 204, Loss: 0.6526\n",
            "Epoch: 009, Step: 205, Loss: 0.5587\n",
            "Epoch: 009, Step: 206, Loss: 0.6912\n",
            "Epoch: 009, Step: 000, Val Loss: 0.6257\n",
            "Epoch: 009, Step: 001, Val Loss: 0.6802\n",
            "Epoch: 009, Step: 002, Val Loss: 0.6170\n",
            "Epoch: 009, Step: 003, Val Loss: 0.6876\n",
            "Epoch: 009, Step: 004, Val Loss: 0.6650\n",
            "Epoch: 009, Step: 005, Val Loss: 0.6859\n",
            "Epoch: 009, Step: 006, Val Loss: 0.6155\n",
            "Epoch: 009, Step: 007, Val Loss: 0.7211\n",
            "Epoch: 009, Step: 008, Val Loss: 0.6900\n",
            "Epoch: 009, Step: 009, Val Loss: 0.6730\n",
            "Epoch: 009, Step: 010, Val Loss: 0.6323\n",
            "Epoch: 009, Step: 011, Val Loss: 0.6538\n",
            "Epoch: 009, Step: 012, Val Loss: 0.7602\n",
            "Epoch: 009, Step: 013, Val Loss: 0.6483\n",
            "Epoch: 009, Step: 014, Val Loss: 0.6485\n",
            "Epoch: 009, Step: 015, Val Loss: 0.7958\n",
            "Epoch: 009, Step: 016, Val Loss: 0.6544\n",
            "Epoch: 009, Step: 017, Val Loss: 0.7035\n",
            "Epoch: 009, Step: 018, Val Loss: 0.6903\n",
            "Epoch: 009, Step: 019, Val Loss: 0.6793\n",
            "Epoch: 009, Step: 020, Val Loss: 0.6460\n",
            "Epoch: 009, Step: 021, Val Loss: 0.6282\n",
            "Epoch: 009, Step: 022, Val Loss: 0.6449\n",
            "Epoch: 009, Step: 023, Val Loss: 0.6694\n",
            "Epoch: 009, Step: 024, Val Loss: 0.6842\n",
            "Epoch: 009, Step: 025, Val Loss: 0.6711\n",
            "Epoch: 009, Step: 026, Val Loss: 0.6610\n",
            "Epoch: 009, Step: 027, Val Loss: 0.6925\n",
            "Epoch: 009, Step: 028, Val Loss: 0.6938\n",
            "Epoch: 009, Step: 029, Val Loss: 0.6440\n",
            "Epoch: 009, Step: 030, Val Loss: 0.6585\n",
            "Epoch: 009, Step: 031, Val Loss: 0.6337\n",
            "Epoch: 009, Step: 032, Val Loss: 0.6224\n",
            "Epoch: 009, Step: 033, Val Loss: 0.6412\n",
            "Epoch: 009, Step: 034, Val Loss: 0.6574\n",
            "Epoch: 009, Step: 035, Val Loss: 0.6998\n",
            "Epoch: 009, Step: 036, Val Loss: 0.7464\n",
            "Epoch: 009, Step: 037, Val Loss: 0.6817\n",
            "Epoch: 009, Step: 038, Val Loss: 0.7007\n",
            "Epoch: 009, Step: 039, Val Loss: 0.6283\n",
            "Epoch: 009, Step: 040, Val Loss: 0.7042\n",
            "Epoch: 009, Step: 041, Val Loss: 0.6516\n",
            "Epoch: 009, Step: 042, Val Loss: 0.6797\n",
            "Epoch: 009, Step: 043, Val Loss: 0.6965\n",
            "Epoch: 009, Step: 044, Val Loss: 0.7194\n",
            "Epoch: 009, Step: 045, Val Loss: 0.6298\n",
            "Epoch: 009, Step: 046, Val Loss: 0.7183\n",
            "Epoch: 009, Step: 047, Val Loss: 0.6930\n",
            "Epoch: 009, Step: 048, Val Loss: 0.7007\n",
            "Epoch: 009, Step: 049, Val Loss: 0.6256\n",
            "Epoch: 009, Step: 050, Val Loss: 0.7219\n",
            "Epoch: 009, Step: 051, Val Loss: 0.6708\n",
            "Epoch: 009, Step: 052, Val Loss: 0.6636\n",
            "Epoch: 009, Step: 053, Val Loss: 0.7328\n",
            "Epoch: 009, Step: 054, Val Loss: 0.7002\n",
            "Epoch: 009, Step: 055, Val Loss: 0.6828\n",
            "Epoch: 009, Step: 056, Val Loss: 0.7161\n",
            "Epoch: 009, Step: 057, Val Loss: 0.6667\n",
            "Epoch: 009, Step: 058, Val Loss: 0.7044\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch_geometric.data import Batch"
      ],
      "metadata": {
        "id": "4l3KorsfkCBJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def test(model, test_loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for i, batch in enumerate(test_loader):\n",
        "            out = model(batch.x, batch.edge_index, batch.batch)\n",
        "\n",
        "            pred = out.argmax(dim=1)\n",
        "\n",
        "            predictions.extend(pred.cpu().numpy())\n",
        "            true_labels.extend(batch.y.cpu().numpy())\n",
        "\n",
        "    return predictions, true_labels"
      ],
      "metadata": {
        "id": "PMtAG9HVflOn"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = DataLoader(test_set, batch_size=2, shuffle=True)"
      ],
      "metadata": {
        "id": "4_kgiV8jlYAr"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "-GL6yIb9olYj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_y, preds = test(model, test_loader, criterion)"
      ],
      "metadata": {
        "id": "kdAHa9vcovNa"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(test_y, preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5io80ytkRPX",
        "outputId": "969e69fe-68af-40f4-c2d4-0d0bf5a657e7"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.67      0.78       105\n",
            "           1       0.19      0.67      0.29        12\n",
            "\n",
            "    accuracy                           0.67       117\n",
            "   macro avg       0.57      0.67      0.54       117\n",
            "weighted avg       0.87      0.67      0.73       117\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise-2 - Graph Classification with GCN"
      ],
      "metadata": {
        "id": "AkOg2BgslsfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        self.conv1 = GCNConv(-1, 64)\n",
        "        self.conv2 = GCNConv(-1, 128)\n",
        "        self.conv3 = GCNConv(-1, 64)\n",
        "\n",
        "        self.linear1 = Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        # 1. Obtain node embeddings\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = x.relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        # 2. Readout layer\n",
        "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
        "\n",
        "        # 3. Apply a final classifier\n",
        "        x = dropout(x, p=0.5, training=self.training)\n",
        "        x = self.linear1(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "kdpPNNMPlx4W"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = GCN(num_classes=2).to('cuda')"
      ],
      "metadata": {
        "id": "uyVV9jfqmHSC"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model2, train_loader, val_loader, optimizer, criterion, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCf8hYN0mYA5",
        "outputId": "2467e77e-33a1-400b-c914-076a0d858cd9"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch: 000, Step: 310, Loss: 0.7282\n",
            "Epoch: 000, Step: 311, Loss: 0.7222\n",
            "Epoch: 000, Step: 312, Loss: 0.7235\n",
            "Epoch: 000, Step: 313, Loss: 0.6974\n",
            "Epoch: 000, Step: 314, Loss: 0.6926\n",
            "Epoch: 000, Step: 315, Loss: 0.7252\n",
            "Epoch: 000, Step: 316, Loss: 0.6961\n",
            "Epoch: 000, Step: 317, Loss: 0.7192\n",
            "Epoch: 000, Step: 318, Loss: 0.6871\n",
            "Epoch: 000, Step: 319, Loss: 0.7259\n",
            "Epoch: 000, Step: 320, Loss: 0.6903\n",
            "Epoch: 000, Step: 321, Loss: 0.7234\n",
            "Epoch: 000, Step: 322, Loss: 0.7238\n",
            "Epoch: 000, Step: 323, Loss: 0.7315\n",
            "Epoch: 000, Step: 324, Loss: 0.7231\n",
            "Epoch: 000, Step: 325, Loss: 0.6902\n",
            "Epoch: 000, Step: 326, Loss: 0.6610\n",
            "Epoch: 000, Step: 327, Loss: 0.7237\n",
            "Epoch: 000, Step: 328, Loss: 0.6920\n",
            "Epoch: 000, Step: 329, Loss: 0.6955\n",
            "Epoch: 000, Step: 330, Loss: 0.6981\n",
            "Epoch: 000, Step: 331, Loss: 0.6976\n",
            "Epoch: 000, Step: 332, Loss: 0.7266\n",
            "Epoch: 000, Step: 333, Loss: 0.6908\n",
            "Epoch: 000, Step: 334, Loss: 0.6681\n",
            "Epoch: 000, Step: 335, Loss: 0.7253\n",
            "Epoch: 000, Step: 336, Loss: 0.7325\n",
            "Epoch: 000, Step: 337, Loss: 0.7252\n",
            "Epoch: 000, Step: 338, Loss: 0.7001\n",
            "Epoch: 000, Step: 339, Loss: 0.6952\n",
            "Epoch: 000, Step: 340, Loss: 0.6618\n",
            "Epoch: 000, Step: 341, Loss: 0.6597\n",
            "Epoch: 000, Step: 342, Loss: 0.7281\n",
            "Epoch: 000, Step: 343, Loss: 0.6931\n",
            "Epoch: 000, Step: 344, Loss: 0.6946\n",
            "Epoch: 000, Step: 345, Loss: 0.7319\n",
            "Epoch: 000, Step: 346, Loss: 0.7261\n",
            "Epoch: 000, Step: 347, Loss: 0.6870\n",
            "Epoch: 000, Step: 348, Loss: 0.6865\n",
            "Epoch: 000, Step: 349, Loss: 0.6919\n",
            "Epoch: 000, Step: 350, Loss: 0.6934\n",
            "Epoch: 000, Step: 351, Loss: 0.6873\n",
            "Epoch: 000, Step: 352, Loss: 0.7245\n",
            "Epoch: 000, Step: 353, Loss: 0.6952\n",
            "Epoch: 000, Step: 354, Loss: 0.6957\n",
            "Epoch: 000, Step: 355, Loss: 0.6581\n",
            "Epoch: 000, Step: 356, Loss: 0.6909\n",
            "Epoch: 000, Step: 357, Loss: 0.7301\n",
            "Epoch: 000, Step: 358, Loss: 0.7290\n",
            "Epoch: 000, Step: 359, Loss: 0.6596\n",
            "Epoch: 000, Step: 360, Loss: 0.7046\n",
            "Epoch: 000, Step: 361, Loss: 0.7328\n",
            "Epoch: 000, Step: 362, Loss: 0.6931\n",
            "Epoch: 000, Step: 363, Loss: 0.6908\n",
            "Epoch: 000, Step: 364, Loss: 0.6876\n",
            "Epoch: 000, Step: 365, Loss: 0.7247\n",
            "Epoch: 000, Step: 366, Loss: 0.6907\n",
            "Epoch: 000, Step: 367, Loss: 0.6960\n",
            "Epoch: 000, Step: 368, Loss: 0.7010\n",
            "Epoch: 000, Step: 369, Loss: 0.7275\n",
            "Epoch: 000, Step: 370, Loss: 0.7239\n",
            "Epoch: 000, Step: 371, Loss: 0.6890\n",
            "Epoch: 000, Step: 372, Loss: 0.6931\n",
            "Epoch: 000, Step: 373, Loss: 0.6864\n",
            "Epoch: 000, Step: 374, Loss: 0.6863\n",
            "Epoch: 000, Step: 375, Loss: 0.6982\n",
            "Epoch: 000, Step: 376, Loss: 0.6943\n",
            "Epoch: 000, Step: 377, Loss: 0.6941\n",
            "Epoch: 000, Step: 378, Loss: 0.6919\n",
            "Epoch: 000, Step: 379, Loss: 0.6609\n",
            "Epoch: 000, Step: 380, Loss: 0.6578\n",
            "Epoch: 000, Step: 381, Loss: 0.6897\n",
            "Epoch: 000, Step: 382, Loss: 0.6960\n",
            "Epoch: 000, Step: 383, Loss: 0.7243\n",
            "Epoch: 000, Step: 384, Loss: 0.6954\n",
            "Epoch: 000, Step: 385, Loss: 0.6625\n",
            "Epoch: 000, Step: 386, Loss: 0.6598\n",
            "Epoch: 000, Step: 387, Loss: 0.7216\n",
            "Epoch: 000, Step: 388, Loss: 0.6947\n",
            "Epoch: 000, Step: 389, Loss: 0.6570\n",
            "Epoch: 000, Step: 390, Loss: 0.6925\n",
            "Epoch: 000, Step: 391, Loss: 0.6911\n",
            "Epoch: 000, Step: 392, Loss: 0.6658\n",
            "Epoch: 000, Step: 393, Loss: 0.6952\n",
            "Epoch: 000, Step: 394, Loss: 0.6960\n",
            "Epoch: 000, Step: 395, Loss: 0.7219\n",
            "Epoch: 000, Step: 396, Loss: 0.7216\n",
            "Epoch: 000, Step: 397, Loss: 0.7032\n",
            "Epoch: 000, Step: 398, Loss: 0.7231\n",
            "Epoch: 000, Step: 399, Loss: 0.6585\n",
            "Epoch: 000, Step: 400, Loss: 0.6960\n",
            "Epoch: 000, Step: 401, Loss: 0.6993\n",
            "Epoch: 000, Step: 402, Loss: 0.7243\n",
            "Epoch: 000, Step: 403, Loss: 0.6627\n",
            "Epoch: 000, Step: 404, Loss: 0.6587\n",
            "Epoch: 000, Step: 405, Loss: 0.7037\n",
            "Epoch: 000, Step: 406, Loss: 0.6943\n",
            "Epoch: 000, Step: 407, Loss: 0.7291\n",
            "Epoch: 000, Step: 408, Loss: 0.7238\n",
            "Epoch: 000, Step: 409, Loss: 0.7259\n",
            "Epoch: 000, Step: 410, Loss: 0.6629\n",
            "Epoch: 000, Step: 411, Loss: 0.7215\n",
            "Epoch: 000, Step: 412, Loss: 0.6609\n",
            "Epoch: 000, Step: 000, Val Loss: 0.7263\n",
            "Epoch: 000, Step: 001, Val Loss: 0.7267\n",
            "Epoch: 000, Step: 002, Val Loss: 0.7260\n",
            "Epoch: 000, Step: 003, Val Loss: 0.7258\n",
            "Epoch: 000, Step: 004, Val Loss: 0.7261\n",
            "Epoch: 000, Step: 005, Val Loss: 0.6949\n",
            "Epoch: 000, Step: 006, Val Loss: 0.7258\n",
            "Epoch: 000, Step: 007, Val Loss: 0.6939\n",
            "Epoch: 000, Step: 008, Val Loss: 0.6941\n",
            "Epoch: 000, Step: 009, Val Loss: 0.6944\n",
            "Epoch: 000, Step: 010, Val Loss: 0.6945\n",
            "Epoch: 000, Step: 011, Val Loss: 0.6604\n",
            "Epoch: 000, Step: 012, Val Loss: 0.7255\n",
            "Epoch: 000, Step: 013, Val Loss: 0.6613\n",
            "Epoch: 000, Step: 014, Val Loss: 0.6938\n",
            "Epoch: 000, Step: 015, Val Loss: 0.6604\n",
            "Epoch: 000, Step: 016, Val Loss: 0.6630\n",
            "Epoch: 000, Step: 017, Val Loss: 0.7269\n",
            "Epoch: 000, Step: 018, Val Loss: 0.6930\n",
            "Epoch: 000, Step: 019, Val Loss: 0.6938\n",
            "Epoch: 000, Step: 020, Val Loss: 0.7260\n",
            "Epoch: 000, Step: 021, Val Loss: 0.6609\n",
            "Epoch: 000, Step: 022, Val Loss: 0.6931\n",
            "Epoch: 000, Step: 023, Val Loss: 0.6921\n",
            "Epoch: 000, Step: 024, Val Loss: 0.7263\n",
            "Epoch: 000, Step: 025, Val Loss: 0.6937\n",
            "Epoch: 000, Step: 026, Val Loss: 0.7261\n",
            "Epoch: 000, Step: 027, Val Loss: 0.7257\n",
            "Epoch: 000, Step: 028, Val Loss: 0.7255\n",
            "Epoch: 000, Step: 029, Val Loss: 0.6949\n",
            "Epoch: 000, Step: 030, Val Loss: 0.6940\n",
            "Epoch: 000, Step: 031, Val Loss: 0.7255\n",
            "Epoch: 000, Step: 032, Val Loss: 0.6952\n",
            "Epoch: 000, Step: 033, Val Loss: 0.7264\n",
            "Epoch: 000, Step: 034, Val Loss: 0.6937\n",
            "Epoch: 000, Step: 035, Val Loss: 0.7266\n",
            "Epoch: 000, Step: 036, Val Loss: 0.7266\n",
            "Epoch: 000, Step: 037, Val Loss: 0.6589\n",
            "Epoch: 000, Step: 038, Val Loss: 0.6923\n",
            "Epoch: 000, Step: 039, Val Loss: 0.6934\n",
            "Epoch: 000, Step: 040, Val Loss: 0.6929\n",
            "Epoch: 000, Step: 041, Val Loss: 0.7264\n",
            "Epoch: 000, Step: 042, Val Loss: 0.6928\n",
            "Epoch: 000, Step: 043, Val Loss: 0.6610\n",
            "Epoch: 000, Step: 044, Val Loss: 0.6935\n",
            "Epoch: 000, Step: 045, Val Loss: 0.6934\n",
            "Epoch: 000, Step: 046, Val Loss: 0.7261\n",
            "Epoch: 000, Step: 047, Val Loss: 0.6931\n",
            "Epoch: 000, Step: 048, Val Loss: 0.6603\n",
            "Epoch: 000, Step: 049, Val Loss: 0.7263\n",
            "Epoch: 000, Step: 050, Val Loss: 0.6610\n",
            "Epoch: 000, Step: 051, Val Loss: 0.7269\n",
            "Epoch: 000, Step: 052, Val Loss: 0.7274\n",
            "Epoch: 000, Step: 053, Val Loss: 0.6619\n",
            "Epoch: 000, Step: 054, Val Loss: 0.6611\n",
            "Epoch: 000, Step: 055, Val Loss: 0.7264\n",
            "Epoch: 000, Step: 056, Val Loss: 0.6936\n",
            "Epoch: 000, Step: 057, Val Loss: 0.6610\n",
            "Epoch: 000, Step: 058, Val Loss: 0.7252\n",
            "Epoch: 000, Step: 059, Val Loss: 0.7252\n",
            "Epoch: 000, Step: 060, Val Loss: 0.7259\n",
            "Epoch: 000, Step: 061, Val Loss: 0.6934\n",
            "Epoch: 000, Step: 062, Val Loss: 0.6609\n",
            "Epoch: 000, Step: 063, Val Loss: 0.6934\n",
            "Epoch: 000, Step: 064, Val Loss: 0.6919\n",
            "Epoch: 000, Step: 065, Val Loss: 0.6935\n",
            "Epoch: 000, Step: 066, Val Loss: 0.7272\n",
            "Epoch: 000, Step: 067, Val Loss: 0.6929\n",
            "Epoch: 000, Step: 068, Val Loss: 0.6585\n",
            "Epoch: 000, Step: 069, Val Loss: 0.6932\n",
            "Epoch: 000, Step: 070, Val Loss: 0.7255\n",
            "Epoch: 000, Step: 071, Val Loss: 0.6947\n",
            "Epoch: 000, Step: 072, Val Loss: 0.6939\n",
            "Epoch: 000, Step: 073, Val Loss: 0.6937\n",
            "Epoch: 000, Step: 074, Val Loss: 0.7266\n",
            "Epoch: 000, Step: 075, Val Loss: 0.6600\n",
            "Epoch: 000, Step: 076, Val Loss: 0.6611\n",
            "Epoch: 000, Step: 077, Val Loss: 0.7260\n",
            "Epoch: 000, Step: 078, Val Loss: 0.7258\n",
            "Epoch: 000, Step: 079, Val Loss: 0.7269\n",
            "Epoch: 000, Step: 080, Val Loss: 0.7258\n",
            "Epoch: 000, Step: 081, Val Loss: 0.6600\n",
            "Epoch: 000, Step: 082, Val Loss: 0.7262\n",
            "Epoch: 000, Step: 083, Val Loss: 0.6598\n",
            "Epoch: 000, Step: 084, Val Loss: 0.6935\n",
            "Epoch: 000, Step: 085, Val Loss: 0.6941\n",
            "Epoch: 000, Step: 086, Val Loss: 0.7259\n",
            "Epoch: 000, Step: 087, Val Loss: 0.6933\n",
            "Epoch: 000, Step: 088, Val Loss: 0.6935\n",
            "Epoch: 000, Step: 089, Val Loss: 0.6930\n",
            "Epoch: 000, Step: 090, Val Loss: 0.7261\n",
            "Epoch: 000, Step: 091, Val Loss: 0.6956\n",
            "Epoch: 000, Step: 092, Val Loss: 0.6937\n",
            "Epoch: 000, Step: 093, Val Loss: 0.6942\n",
            "Epoch: 000, Step: 094, Val Loss: 0.6927\n",
            "Epoch: 000, Step: 095, Val Loss: 0.7260\n",
            "Epoch: 000, Step: 096, Val Loss: 0.6930\n",
            "Epoch: 000, Step: 097, Val Loss: 0.7263\n",
            "Epoch: 000, Step: 098, Val Loss: 0.6610\n",
            "Epoch: 000, Step: 099, Val Loss: 0.6620\n",
            "Epoch: 000, Step: 100, Val Loss: 0.7259\n",
            "Epoch: 000, Step: 101, Val Loss: 0.6935\n",
            "Epoch: 000, Step: 102, Val Loss: 0.6611\n",
            "Epoch: 000, Step: 103, Val Loss: 0.6928\n",
            "Epoch: 000, Step: 104, Val Loss: 0.6938\n",
            "Epoch: 000, Step: 105, Val Loss: 0.7251\n",
            "Epoch: 000, Step: 106, Val Loss: 0.7272\n",
            "Epoch: 000, Step: 107, Val Loss: 0.6606\n",
            "Epoch: 000, Step: 108, Val Loss: 0.7259\n",
            "Epoch: 000, Step: 109, Val Loss: 0.7275\n",
            "Epoch: 000, Step: 110, Val Loss: 0.7263\n",
            "Epoch: 000, Step: 111, Val Loss: 0.7266\n",
            "Epoch: 000, Step: 112, Val Loss: 0.6624\n",
            "Epoch: 000, Step: 113, Val Loss: 0.6937\n",
            "Epoch: 000, Step: 114, Val Loss: 0.6935\n",
            "Epoch: 000, Step: 115, Val Loss: 0.7268\n",
            "Epoch: 000, Step: 116, Val Loss: 0.6934\n",
            "Epoch: 000, Step: 117, Val Loss: 0.6946\n",
            "Epoch: 001, Step: 000, Loss: 0.6955\n",
            "Epoch: 001, Step: 001, Loss: 0.7225\n",
            "Epoch: 001, Step: 002, Loss: 0.6959\n",
            "Epoch: 001, Step: 003, Loss: 0.6942\n",
            "Epoch: 001, Step: 004, Loss: 0.6976\n",
            "Epoch: 001, Step: 005, Loss: 0.6901\n",
            "Epoch: 001, Step: 006, Loss: 0.6547\n",
            "Epoch: 001, Step: 007, Loss: 0.6911\n",
            "Epoch: 001, Step: 008, Loss: 0.6965\n",
            "Epoch: 001, Step: 009, Loss: 0.7252\n",
            "Epoch: 001, Step: 010, Loss: 0.6946\n",
            "Epoch: 001, Step: 011, Loss: 0.6893\n",
            "Epoch: 001, Step: 012, Loss: 0.7193\n",
            "Epoch: 001, Step: 013, Loss: 0.6948\n",
            "Epoch: 001, Step: 014, Loss: 0.6973\n",
            "Epoch: 001, Step: 015, Loss: 0.6888\n",
            "Epoch: 001, Step: 016, Loss: 0.6625\n",
            "Epoch: 001, Step: 017, Loss: 0.6582\n",
            "Epoch: 001, Step: 018, Loss: 0.7335\n",
            "Epoch: 001, Step: 019, Loss: 0.6942\n",
            "Epoch: 001, Step: 020, Loss: 0.6934\n",
            "Epoch: 001, Step: 021, Loss: 0.6888\n",
            "Epoch: 001, Step: 022, Loss: 0.6920\n",
            "Epoch: 001, Step: 023, Loss: 0.6957\n",
            "Epoch: 001, Step: 024, Loss: 0.7239\n",
            "Epoch: 001, Step: 025, Loss: 0.6929\n",
            "Epoch: 001, Step: 026, Loss: 0.6676\n",
            "Epoch: 001, Step: 027, Loss: 0.6929\n",
            "Epoch: 001, Step: 028, Loss: 0.7261\n",
            "Epoch: 001, Step: 029, Loss: 0.7256\n",
            "Epoch: 001, Step: 030, Loss: 0.7235\n",
            "Epoch: 001, Step: 031, Loss: 0.7005\n",
            "Epoch: 001, Step: 032, Loss: 0.7237\n",
            "Epoch: 001, Step: 033, Loss: 0.7232\n",
            "Epoch: 001, Step: 034, Loss: 0.7266\n",
            "Epoch: 001, Step: 035, Loss: 0.7282\n",
            "Epoch: 001, Step: 036, Loss: 0.6986\n",
            "Epoch: 001, Step: 037, Loss: 0.6988\n",
            "Epoch: 001, Step: 038, Loss: 0.6933\n",
            "Epoch: 001, Step: 039, Loss: 0.6924\n",
            "Epoch: 001, Step: 040, Loss: 0.6885\n",
            "Epoch: 001, Step: 041, Loss: 0.7261\n",
            "Epoch: 001, Step: 042, Loss: 0.6816\n",
            "Epoch: 001, Step: 043, Loss: 0.6594\n",
            "Epoch: 001, Step: 044, Loss: 0.6954\n",
            "Epoch: 001, Step: 045, Loss: 0.7251\n",
            "Epoch: 001, Step: 046, Loss: 0.7274\n",
            "Epoch: 001, Step: 047, Loss: 0.6659\n",
            "Epoch: 001, Step: 048, Loss: 0.6901\n",
            "Epoch: 001, Step: 049, Loss: 0.6883\n",
            "Epoch: 001, Step: 050, Loss: 0.7246\n",
            "Epoch: 001, Step: 051, Loss: 0.6669\n",
            "Epoch: 001, Step: 052, Loss: 0.6910\n",
            "Epoch: 001, Step: 053, Loss: 0.6958\n",
            "Epoch: 001, Step: 054, Loss: 0.6602\n",
            "Epoch: 001, Step: 055, Loss: 0.6969\n",
            "Epoch: 001, Step: 056, Loss: 0.6929\n",
            "Epoch: 001, Step: 057, Loss: 0.6629\n",
            "Epoch: 001, Step: 058, Loss: 0.6932\n",
            "Epoch: 001, Step: 059, Loss: 0.6933\n",
            "Epoch: 001, Step: 060, Loss: 0.7299\n",
            "Epoch: 001, Step: 061, Loss: 0.6894\n",
            "Epoch: 001, Step: 062, Loss: 0.6923\n",
            "Epoch: 001, Step: 063, Loss: 0.6976\n",
            "Epoch: 001, Step: 064, Loss: 0.6574\n",
            "Epoch: 001, Step: 065, Loss: 0.6925\n",
            "Epoch: 001, Step: 066, Loss: 0.6991\n",
            "Epoch: 001, Step: 067, Loss: 0.7264\n",
            "Epoch: 001, Step: 068, Loss: 0.6881\n",
            "Epoch: 001, Step: 069, Loss: 0.6928\n",
            "Epoch: 001, Step: 070, Loss: 0.6938\n",
            "Epoch: 001, Step: 071, Loss: 0.6660\n",
            "Epoch: 001, Step: 072, Loss: 0.7307\n",
            "Epoch: 001, Step: 073, Loss: 0.6567\n",
            "Epoch: 001, Step: 074, Loss: 0.7302\n",
            "Epoch: 001, Step: 075, Loss: 0.7294\n",
            "Epoch: 001, Step: 076, Loss: 0.7259\n",
            "Epoch: 001, Step: 077, Loss: 0.6897\n",
            "Epoch: 001, Step: 078, Loss: 0.6928\n",
            "Epoch: 001, Step: 079, Loss: 0.6906\n",
            "Epoch: 001, Step: 080, Loss: 0.7222\n",
            "Epoch: 001, Step: 081, Loss: 0.6587\n",
            "Epoch: 001, Step: 082, Loss: 0.6666\n",
            "Epoch: 001, Step: 083, Loss: 0.6894\n",
            "Epoch: 001, Step: 084, Loss: 0.6620\n",
            "Epoch: 001, Step: 085, Loss: 0.7281\n",
            "Epoch: 001, Step: 086, Loss: 0.6611\n",
            "Epoch: 001, Step: 087, Loss: 0.7228\n",
            "Epoch: 001, Step: 088, Loss: 0.6994\n",
            "Epoch: 001, Step: 089, Loss: 0.6968\n",
            "Epoch: 001, Step: 090, Loss: 0.7239\n",
            "Epoch: 001, Step: 091, Loss: 0.6974\n",
            "Epoch: 001, Step: 092, Loss: 0.7225\n",
            "Epoch: 001, Step: 093, Loss: 0.6624\n",
            "Epoch: 001, Step: 094, Loss: 0.7022\n",
            "Epoch: 001, Step: 095, Loss: 0.6583\n",
            "Epoch: 001, Step: 096, Loss: 0.7276\n",
            "Epoch: 001, Step: 097, Loss: 0.6961\n",
            "Epoch: 001, Step: 098, Loss: 0.6607\n",
            "Epoch: 001, Step: 099, Loss: 0.6963\n",
            "Epoch: 001, Step: 100, Loss: 0.6894\n",
            "Epoch: 001, Step: 101, Loss: 0.6906\n",
            "Epoch: 001, Step: 102, Loss: 0.6924\n",
            "Epoch: 001, Step: 103, Loss: 0.7235\n",
            "Epoch: 001, Step: 104, Loss: 0.6950\n",
            "Epoch: 001, Step: 105, Loss: 0.6875\n",
            "Epoch: 001, Step: 106, Loss: 0.6698\n",
            "Epoch: 001, Step: 107, Loss: 0.6912\n",
            "Epoch: 001, Step: 108, Loss: 0.7294\n",
            "Epoch: 001, Step: 109, Loss: 0.6944\n",
            "Epoch: 001, Step: 110, Loss: 0.7276\n",
            "Epoch: 001, Step: 111, Loss: 0.6952\n",
            "Epoch: 001, Step: 112, Loss: 0.6952\n",
            "Epoch: 001, Step: 113, Loss: 0.7311\n",
            "Epoch: 001, Step: 114, Loss: 0.6849\n",
            "Epoch: 001, Step: 115, Loss: 0.7231\n",
            "Epoch: 001, Step: 116, Loss: 0.6918\n",
            "Epoch: 001, Step: 117, Loss: 0.7008\n",
            "Epoch: 001, Step: 118, Loss: 0.6961\n",
            "Epoch: 001, Step: 119, Loss: 0.6969\n",
            "Epoch: 001, Step: 120, Loss: 0.6614\n",
            "Epoch: 001, Step: 121, Loss: 0.6608\n",
            "Epoch: 001, Step: 122, Loss: 0.7178\n",
            "Epoch: 001, Step: 123, Loss: 0.6668\n",
            "Epoch: 001, Step: 124, Loss: 0.6630\n",
            "Epoch: 001, Step: 125, Loss: 0.7243\n",
            "Epoch: 001, Step: 126, Loss: 0.7195\n",
            "Epoch: 001, Step: 127, Loss: 0.7243\n",
            "Epoch: 001, Step: 128, Loss: 0.6927\n",
            "Epoch: 001, Step: 129, Loss: 0.6999\n",
            "Epoch: 001, Step: 130, Loss: 0.7241\n",
            "Epoch: 001, Step: 131, Loss: 0.7283\n",
            "Epoch: 001, Step: 132, Loss: 0.6966\n",
            "Epoch: 001, Step: 133, Loss: 0.6676\n",
            "Epoch: 001, Step: 134, Loss: 0.7004\n",
            "Epoch: 001, Step: 135, Loss: 0.7234\n",
            "Epoch: 001, Step: 136, Loss: 0.6915\n",
            "Epoch: 001, Step: 137, Loss: 0.6884\n",
            "Epoch: 001, Step: 138, Loss: 0.7170\n",
            "Epoch: 001, Step: 139, Loss: 0.7256\n",
            "Epoch: 001, Step: 140, Loss: 0.7305\n",
            "Epoch: 001, Step: 141, Loss: 0.6568\n",
            "Epoch: 001, Step: 142, Loss: 0.6655\n",
            "Epoch: 001, Step: 143, Loss: 0.7256\n",
            "Epoch: 001, Step: 144, Loss: 0.6880\n",
            "Epoch: 001, Step: 145, Loss: 0.7223\n",
            "Epoch: 001, Step: 146, Loss: 0.6945\n",
            "Epoch: 001, Step: 147, Loss: 0.6973\n",
            "Epoch: 001, Step: 148, Loss: 0.7293\n",
            "Epoch: 001, Step: 149, Loss: 0.7229\n",
            "Epoch: 001, Step: 150, Loss: 0.7211\n",
            "Epoch: 001, Step: 151, Loss: 0.6917\n",
            "Epoch: 001, Step: 152, Loss: 0.6969\n",
            "Epoch: 001, Step: 153, Loss: 0.6830\n",
            "Epoch: 001, Step: 154, Loss: 0.6990\n",
            "Epoch: 001, Step: 155, Loss: 0.6610\n",
            "Epoch: 001, Step: 156, Loss: 0.6879\n",
            "Epoch: 001, Step: 157, Loss: 0.7280\n",
            "Epoch: 001, Step: 158, Loss: 0.7312\n",
            "Epoch: 001, Step: 159, Loss: 0.6956\n",
            "Epoch: 001, Step: 160, Loss: 0.6941\n",
            "Epoch: 001, Step: 161, Loss: 0.7248\n",
            "Epoch: 001, Step: 162, Loss: 0.7229\n",
            "Epoch: 001, Step: 163, Loss: 0.6900\n",
            "Epoch: 001, Step: 164, Loss: 0.6891\n",
            "Epoch: 001, Step: 165, Loss: 0.7237\n",
            "Epoch: 001, Step: 166, Loss: 0.7234\n",
            "Epoch: 001, Step: 167, Loss: 0.6689\n",
            "Epoch: 001, Step: 168, Loss: 0.7237\n",
            "Epoch: 001, Step: 169, Loss: 0.7183\n",
            "Epoch: 001, Step: 170, Loss: 0.6946\n",
            "Epoch: 001, Step: 171, Loss: 0.6919\n",
            "Epoch: 001, Step: 172, Loss: 0.7233\n",
            "Epoch: 001, Step: 173, Loss: 0.6582\n",
            "Epoch: 001, Step: 174, Loss: 0.6945\n",
            "Epoch: 001, Step: 175, Loss: 0.7325\n",
            "Epoch: 001, Step: 176, Loss: 0.6865\n",
            "Epoch: 001, Step: 177, Loss: 0.7366\n",
            "Epoch: 001, Step: 178, Loss: 0.7369\n",
            "Epoch: 001, Step: 179, Loss: 0.7301\n",
            "Epoch: 001, Step: 180, Loss: 0.6901\n",
            "Epoch: 001, Step: 181, Loss: 0.6952\n",
            "Epoch: 001, Step: 182, Loss: 0.6935\n",
            "Epoch: 001, Step: 183, Loss: 0.7279\n",
            "Epoch: 001, Step: 184, Loss: 0.6650\n",
            "Epoch: 001, Step: 185, Loss: 0.7296\n",
            "Epoch: 001, Step: 186, Loss: 0.6935\n",
            "Epoch: 001, Step: 187, Loss: 0.6992\n",
            "Epoch: 001, Step: 188, Loss: 0.7314\n",
            "Epoch: 001, Step: 189, Loss: 0.6649\n",
            "Epoch: 001, Step: 190, Loss: 0.7195\n",
            "Epoch: 001, Step: 191, Loss: 0.6924\n",
            "Epoch: 001, Step: 192, Loss: 0.6917\n",
            "Epoch: 001, Step: 193, Loss: 0.6963\n",
            "Epoch: 001, Step: 194, Loss: 0.7243\n",
            "Epoch: 001, Step: 195, Loss: 0.6924\n",
            "Epoch: 001, Step: 196, Loss: 0.7255\n",
            "Epoch: 001, Step: 197, Loss: 0.6914\n",
            "Epoch: 001, Step: 198, Loss: 0.6908\n",
            "Epoch: 001, Step: 199, Loss: 0.7271\n",
            "Epoch: 001, Step: 200, Loss: 0.7328\n",
            "Epoch: 001, Step: 201, Loss: 0.6991\n",
            "Epoch: 001, Step: 202, Loss: 0.6609\n",
            "Epoch: 001, Step: 203, Loss: 0.6848\n",
            "Epoch: 001, Step: 204, Loss: 0.6864\n",
            "Epoch: 001, Step: 205, Loss: 0.6960\n",
            "Epoch: 001, Step: 206, Loss: 0.6606\n",
            "Epoch: 001, Step: 207, Loss: 0.7250\n",
            "Epoch: 001, Step: 208, Loss: 0.6937\n",
            "Epoch: 001, Step: 209, Loss: 0.7265\n",
            "Epoch: 001, Step: 210, Loss: 0.7260\n",
            "Epoch: 001, Step: 211, Loss: 0.6924\n",
            "Epoch: 001, Step: 212, Loss: 0.6605\n",
            "Epoch: 001, Step: 213, Loss: 0.6887\n",
            "Epoch: 001, Step: 214, Loss: 0.7306\n",
            "Epoch: 001, Step: 215, Loss: 0.7315\n",
            "Epoch: 001, Step: 216, Loss: 0.6998\n",
            "Epoch: 001, Step: 217, Loss: 0.7236\n",
            "Epoch: 001, Step: 218, Loss: 0.6940\n",
            "Epoch: 001, Step: 219, Loss: 0.7219\n",
            "Epoch: 001, Step: 220, Loss: 0.7282\n",
            "Epoch: 001, Step: 221, Loss: 0.6966\n",
            "Epoch: 001, Step: 222, Loss: 0.6986\n",
            "Epoch: 001, Step: 223, Loss: 0.6644\n",
            "Epoch: 001, Step: 224, Loss: 0.6890\n",
            "Epoch: 001, Step: 225, Loss: 0.7319\n",
            "Epoch: 001, Step: 226, Loss: 0.7283\n",
            "Epoch: 001, Step: 227, Loss: 0.6925\n",
            "Epoch: 001, Step: 228, Loss: 0.6872\n",
            "Epoch: 001, Step: 229, Loss: 0.6980\n",
            "Epoch: 001, Step: 230, Loss: 0.6948\n",
            "Epoch: 001, Step: 231, Loss: 0.6935\n",
            "Epoch: 001, Step: 232, Loss: 0.6900\n",
            "Epoch: 001, Step: 233, Loss: 0.7231\n",
            "Epoch: 001, Step: 234, Loss: 0.7216\n",
            "Epoch: 001, Step: 235, Loss: 0.7268\n",
            "Epoch: 001, Step: 236, Loss: 0.6562\n",
            "Epoch: 001, Step: 237, Loss: 0.7357\n",
            "Epoch: 001, Step: 238, Loss: 0.6902\n",
            "Epoch: 001, Step: 239, Loss: 0.7360\n",
            "Epoch: 001, Step: 240, Loss: 0.7225\n",
            "Epoch: 001, Step: 241, Loss: 0.7302\n",
            "Epoch: 001, Step: 242, Loss: 0.7305\n",
            "Epoch: 001, Step: 243, Loss: 0.6951\n",
            "Epoch: 001, Step: 244, Loss: 0.7317\n",
            "Epoch: 001, Step: 245, Loss: 0.6932\n",
            "Epoch: 001, Step: 246, Loss: 0.6897\n",
            "Epoch: 001, Step: 247, Loss: 0.6879\n",
            "Epoch: 001, Step: 248, Loss: 0.6928\n",
            "Epoch: 001, Step: 249, Loss: 0.6890\n",
            "Epoch: 001, Step: 250, Loss: 0.6918\n",
            "Epoch: 001, Step: 251, Loss: 0.6907\n",
            "Epoch: 001, Step: 252, Loss: 0.6936\n",
            "Epoch: 001, Step: 253, Loss: 0.6526\n",
            "Epoch: 001, Step: 254, Loss: 0.7231\n",
            "Epoch: 001, Step: 255, Loss: 0.6885\n",
            "Epoch: 001, Step: 256, Loss: 0.6903\n",
            "Epoch: 001, Step: 257, Loss: 0.7008\n",
            "Epoch: 001, Step: 258, Loss: 0.6991\n",
            "Epoch: 001, Step: 259, Loss: 0.6944\n",
            "Epoch: 001, Step: 260, Loss: 0.6879\n",
            "Epoch: 001, Step: 261, Loss: 0.6898\n",
            "Epoch: 001, Step: 262, Loss: 0.7255\n",
            "Epoch: 001, Step: 263, Loss: 0.6939\n",
            "Epoch: 001, Step: 264, Loss: 0.7233\n",
            "Epoch: 001, Step: 265, Loss: 0.7221\n",
            "Epoch: 001, Step: 266, Loss: 0.6937\n",
            "Epoch: 001, Step: 267, Loss: 0.6578\n",
            "Epoch: 001, Step: 268, Loss: 0.6661\n",
            "Epoch: 001, Step: 269, Loss: 0.6957\n",
            "Epoch: 001, Step: 270, Loss: 0.6653\n",
            "Epoch: 001, Step: 271, Loss: 0.6635\n",
            "Epoch: 001, Step: 272, Loss: 0.7293\n",
            "Epoch: 001, Step: 273, Loss: 0.7007\n",
            "Epoch: 001, Step: 274, Loss: 0.6543\n",
            "Epoch: 001, Step: 275, Loss: 0.6923\n",
            "Epoch: 001, Step: 276, Loss: 0.6884\n",
            "Epoch: 001, Step: 277, Loss: 0.7021\n",
            "Epoch: 001, Step: 278, Loss: 0.7223\n",
            "Epoch: 001, Step: 279, Loss: 0.6931\n",
            "Epoch: 001, Step: 280, Loss: 0.6893\n",
            "Epoch: 001, Step: 281, Loss: 0.6965\n",
            "Epoch: 001, Step: 282, Loss: 0.6969\n",
            "Epoch: 001, Step: 283, Loss: 0.6975\n",
            "Epoch: 001, Step: 284, Loss: 0.6865\n",
            "Epoch: 001, Step: 285, Loss: 0.7233\n",
            "Epoch: 001, Step: 286, Loss: 0.6967\n",
            "Epoch: 001, Step: 287, Loss: 0.6611\n",
            "Epoch: 001, Step: 288, Loss: 0.6651\n",
            "Epoch: 001, Step: 289, Loss: 0.6945\n",
            "Epoch: 001, Step: 290, Loss: 0.6862\n",
            "Epoch: 001, Step: 291, Loss: 0.7282\n",
            "Epoch: 001, Step: 292, Loss: 0.7275\n",
            "Epoch: 001, Step: 293, Loss: 0.6866\n",
            "Epoch: 001, Step: 294, Loss: 0.6974\n",
            "Epoch: 001, Step: 295, Loss: 0.7264\n",
            "Epoch: 001, Step: 296, Loss: 0.6895\n",
            "Epoch: 001, Step: 297, Loss: 0.6915\n",
            "Epoch: 001, Step: 298, Loss: 0.7015\n",
            "Epoch: 001, Step: 299, Loss: 0.6940\n",
            "Epoch: 001, Step: 300, Loss: 0.7236\n",
            "Epoch: 001, Step: 301, Loss: 0.7262\n",
            "Epoch: 001, Step: 302, Loss: 0.6581\n",
            "Epoch: 001, Step: 303, Loss: 0.6927\n",
            "Epoch: 001, Step: 304, Loss: 0.7261\n",
            "Epoch: 001, Step: 305, Loss: 0.7298\n",
            "Epoch: 001, Step: 306, Loss: 0.6892\n",
            "Epoch: 001, Step: 307, Loss: 0.7233\n",
            "Epoch: 001, Step: 308, Loss: 0.7320\n",
            "Epoch: 001, Step: 309, Loss: 0.6905\n",
            "Epoch: 001, Step: 310, Loss: 0.6875\n",
            "Epoch: 001, Step: 311, Loss: 0.6952\n",
            "Epoch: 001, Step: 312, Loss: 0.6607\n",
            "Epoch: 001, Step: 313, Loss: 0.6974\n",
            "Epoch: 001, Step: 314, Loss: 0.7028\n",
            "Epoch: 001, Step: 315, Loss: 0.7220\n",
            "Epoch: 001, Step: 316, Loss: 0.6939\n",
            "Epoch: 001, Step: 317, Loss: 0.6898\n",
            "Epoch: 001, Step: 318, Loss: 0.6583\n",
            "Epoch: 001, Step: 319, Loss: 0.6976\n",
            "Epoch: 001, Step: 320, Loss: 0.7316\n",
            "Epoch: 001, Step: 321, Loss: 0.6884\n",
            "Epoch: 001, Step: 322, Loss: 0.6954\n",
            "Epoch: 001, Step: 323, Loss: 0.7271\n",
            "Epoch: 001, Step: 324, Loss: 0.6924\n",
            "Epoch: 001, Step: 325, Loss: 0.6958\n",
            "Epoch: 001, Step: 326, Loss: 0.7300\n",
            "Epoch: 001, Step: 327, Loss: 0.6614\n",
            "Epoch: 001, Step: 328, Loss: 0.7271\n",
            "Epoch: 001, Step: 329, Loss: 0.7339\n",
            "Epoch: 001, Step: 330, Loss: 0.6928\n",
            "Epoch: 001, Step: 331, Loss: 0.6625\n",
            "Epoch: 001, Step: 332, Loss: 0.6928\n",
            "Epoch: 001, Step: 333, Loss: 0.7310\n",
            "Epoch: 001, Step: 334, Loss: 0.6571\n",
            "Epoch: 001, Step: 335, Loss: 0.7213\n",
            "Epoch: 001, Step: 336, Loss: 0.6592\n",
            "Epoch: 001, Step: 337, Loss: 0.7197\n",
            "Epoch: 001, Step: 338, Loss: 0.7015\n",
            "Epoch: 001, Step: 339, Loss: 0.6966\n",
            "Epoch: 001, Step: 340, Loss: 0.7312\n",
            "Epoch: 001, Step: 341, Loss: 0.6892\n",
            "Epoch: 001, Step: 342, Loss: 0.7289\n",
            "Epoch: 001, Step: 343, Loss: 0.7318\n",
            "Epoch: 001, Step: 344, Loss: 0.6969\n",
            "Epoch: 001, Step: 345, Loss: 0.6933\n",
            "Epoch: 001, Step: 346, Loss: 0.7315\n",
            "Epoch: 001, Step: 347, Loss: 0.7297\n",
            "Epoch: 001, Step: 348, Loss: 0.7262\n",
            "Epoch: 001, Step: 349, Loss: 0.7270\n",
            "Epoch: 001, Step: 350, Loss: 0.6966\n",
            "Epoch: 001, Step: 351, Loss: 0.7308\n",
            "Epoch: 001, Step: 352, Loss: 0.6895\n",
            "Epoch: 001, Step: 353, Loss: 0.7270\n",
            "Epoch: 001, Step: 354, Loss: 0.7293\n",
            "Epoch: 001, Step: 355, Loss: 0.7297\n",
            "Epoch: 001, Step: 356, Loss: 0.6897\n",
            "Epoch: 001, Step: 357, Loss: 0.6897\n",
            "Epoch: 001, Step: 358, Loss: 0.6868\n",
            "Epoch: 001, Step: 359, Loss: 0.7203\n",
            "Epoch: 001, Step: 360, Loss: 0.6943\n",
            "Epoch: 001, Step: 361, Loss: 0.7302\n",
            "Epoch: 001, Step: 362, Loss: 0.7222\n",
            "Epoch: 001, Step: 363, Loss: 0.6978\n",
            "Epoch: 001, Step: 364, Loss: 0.7240\n",
            "Epoch: 001, Step: 365, Loss: 0.7335\n",
            "Epoch: 001, Step: 366, Loss: 0.6929\n",
            "Epoch: 001, Step: 367, Loss: 0.7251\n",
            "Epoch: 001, Step: 368, Loss: 0.7259\n",
            "Epoch: 001, Step: 369, Loss: 0.6940\n",
            "Epoch: 001, Step: 370, Loss: 0.6612\n",
            "Epoch: 001, Step: 371, Loss: 0.7281\n",
            "Epoch: 001, Step: 372, Loss: 0.6498\n",
            "Epoch: 001, Step: 373, Loss: 0.6574\n",
            "Epoch: 001, Step: 374, Loss: 0.6956\n",
            "Epoch: 001, Step: 375, Loss: 0.6583\n",
            "Epoch: 001, Step: 376, Loss: 0.6949\n",
            "Epoch: 001, Step: 377, Loss: 0.6950\n",
            "Epoch: 001, Step: 378, Loss: 0.6572\n",
            "Epoch: 001, Step: 379, Loss: 0.6944\n",
            "Epoch: 001, Step: 380, Loss: 0.6594\n",
            "Epoch: 001, Step: 381, Loss: 0.6998\n",
            "Epoch: 001, Step: 382, Loss: 0.6968\n",
            "Epoch: 001, Step: 383, Loss: 0.6963\n",
            "Epoch: 001, Step: 384, Loss: 0.6867\n",
            "Epoch: 001, Step: 385, Loss: 0.6899\n",
            "Epoch: 001, Step: 386, Loss: 0.6645\n",
            "Epoch: 001, Step: 387, Loss: 0.6670\n",
            "Epoch: 001, Step: 388, Loss: 0.6986\n",
            "Epoch: 001, Step: 389, Loss: 0.6906\n",
            "Epoch: 001, Step: 390, Loss: 0.6899\n",
            "Epoch: 001, Step: 391, Loss: 0.6848\n",
            "Epoch: 001, Step: 392, Loss: 0.6911\n",
            "Epoch: 001, Step: 393, Loss: 0.6940\n",
            "Epoch: 001, Step: 394, Loss: 0.6975\n",
            "Epoch: 001, Step: 395, Loss: 0.6897\n",
            "Epoch: 001, Step: 396, Loss: 0.7286\n",
            "Epoch: 001, Step: 397, Loss: 0.6948\n",
            "Epoch: 001, Step: 398, Loss: 0.7238\n",
            "Epoch: 001, Step: 399, Loss: 0.6950\n",
            "Epoch: 001, Step: 400, Loss: 0.7224\n",
            "Epoch: 001, Step: 401, Loss: 0.6847\n",
            "Epoch: 001, Step: 402, Loss: 0.6885\n",
            "Epoch: 001, Step: 403, Loss: 0.7005\n",
            "Epoch: 001, Step: 404, Loss: 0.6944\n",
            "Epoch: 001, Step: 405, Loss: 0.7272\n",
            "Epoch: 001, Step: 406, Loss: 0.6908\n",
            "Epoch: 001, Step: 407, Loss: 0.6566\n",
            "Epoch: 001, Step: 408, Loss: 0.6594\n",
            "Epoch: 001, Step: 409, Loss: 0.6962\n",
            "Epoch: 001, Step: 410, Loss: 0.6622\n",
            "Epoch: 001, Step: 411, Loss: 0.6663\n",
            "Epoch: 001, Step: 412, Loss: 0.6546\n",
            "Epoch: 001, Step: 000, Val Loss: 0.6935\n",
            "Epoch: 001, Step: 001, Val Loss: 0.6933\n",
            "Epoch: 001, Step: 002, Val Loss: 0.7253\n",
            "Epoch: 001, Step: 003, Val Loss: 0.6934\n",
            "Epoch: 001, Step: 004, Val Loss: 0.6596\n",
            "Epoch: 001, Step: 005, Val Loss: 0.6940\n",
            "Epoch: 001, Step: 006, Val Loss: 0.6623\n",
            "Epoch: 001, Step: 007, Val Loss: 0.7261\n",
            "Epoch: 001, Step: 008, Val Loss: 0.7261\n",
            "Epoch: 001, Step: 009, Val Loss: 0.7262\n",
            "Epoch: 001, Step: 010, Val Loss: 0.6611\n",
            "Epoch: 001, Step: 011, Val Loss: 0.7265\n",
            "Epoch: 001, Step: 012, Val Loss: 0.6922\n",
            "Epoch: 001, Step: 013, Val Loss: 0.6598\n",
            "Epoch: 001, Step: 014, Val Loss: 0.6938\n",
            "Epoch: 001, Step: 015, Val Loss: 0.6956\n",
            "Epoch: 001, Step: 016, Val Loss: 0.6608\n",
            "Epoch: 001, Step: 017, Val Loss: 0.6601\n",
            "Epoch: 001, Step: 018, Val Loss: 0.6931\n",
            "Epoch: 001, Step: 019, Val Loss: 0.7260\n",
            "Epoch: 001, Step: 020, Val Loss: 0.6935\n",
            "Epoch: 001, Step: 021, Val Loss: 0.7261\n",
            "Epoch: 001, Step: 022, Val Loss: 0.7254\n",
            "Epoch: 001, Step: 023, Val Loss: 0.6945\n",
            "Epoch: 001, Step: 024, Val Loss: 0.7272\n",
            "Epoch: 001, Step: 025, Val Loss: 0.6604\n",
            "Epoch: 001, Step: 026, Val Loss: 0.6946\n",
            "Epoch: 001, Step: 027, Val Loss: 0.6933\n",
            "Epoch: 001, Step: 028, Val Loss: 0.6926\n",
            "Epoch: 001, Step: 029, Val Loss: 0.7267\n",
            "Epoch: 001, Step: 030, Val Loss: 0.6938\n",
            "Epoch: 001, Step: 031, Val Loss: 0.6938\n",
            "Epoch: 001, Step: 032, Val Loss: 0.7260\n",
            "Epoch: 001, Step: 033, Val Loss: 0.6928\n",
            "Epoch: 001, Step: 034, Val Loss: 0.6937\n",
            "Epoch: 001, Step: 035, Val Loss: 0.6926\n",
            "Epoch: 001, Step: 036, Val Loss: 0.6935\n",
            "Epoch: 001, Step: 037, Val Loss: 0.7260\n",
            "Epoch: 001, Step: 038, Val Loss: 0.6919\n",
            "Epoch: 001, Step: 039, Val Loss: 0.6946\n",
            "Epoch: 001, Step: 040, Val Loss: 0.6936\n",
            "Epoch: 001, Step: 041, Val Loss: 0.6942\n",
            "Epoch: 001, Step: 042, Val Loss: 0.6927\n",
            "Epoch: 001, Step: 043, Val Loss: 0.7254\n",
            "Epoch: 001, Step: 044, Val Loss: 0.6924\n",
            "Epoch: 001, Step: 045, Val Loss: 0.7268\n",
            "Epoch: 001, Step: 046, Val Loss: 0.7264\n",
            "Epoch: 001, Step: 047, Val Loss: 0.7263\n",
            "Epoch: 001, Step: 048, Val Loss: 0.7272\n",
            "Epoch: 001, Step: 049, Val Loss: 0.6932\n",
            "Epoch: 001, Step: 050, Val Loss: 0.6943\n",
            "Epoch: 001, Step: 051, Val Loss: 0.6934\n",
            "Epoch: 001, Step: 052, Val Loss: 0.6941\n",
            "Epoch: 001, Step: 053, Val Loss: 0.6943\n",
            "Epoch: 001, Step: 054, Val Loss: 0.6605\n",
            "Epoch: 001, Step: 055, Val Loss: 0.6935\n",
            "Epoch: 001, Step: 056, Val Loss: 0.7261\n",
            "Epoch: 001, Step: 057, Val Loss: 0.6604\n",
            "Epoch: 001, Step: 058, Val Loss: 0.7260\n",
            "Epoch: 001, Step: 059, Val Loss: 0.6609\n",
            "Epoch: 001, Step: 060, Val Loss: 0.6934\n",
            "Epoch: 001, Step: 061, Val Loss: 0.6942\n",
            "Epoch: 001, Step: 062, Val Loss: 0.7273\n",
            "Epoch: 001, Step: 063, Val Loss: 0.6928\n",
            "Epoch: 001, Step: 064, Val Loss: 0.6919\n",
            "Epoch: 001, Step: 065, Val Loss: 0.7262\n",
            "Epoch: 001, Step: 066, Val Loss: 0.6602\n",
            "Epoch: 001, Step: 067, Val Loss: 0.6606\n",
            "Epoch: 001, Step: 068, Val Loss: 0.6937\n",
            "Epoch: 001, Step: 069, Val Loss: 0.6940\n",
            "Epoch: 001, Step: 070, Val Loss: 0.6950\n",
            "Epoch: 001, Step: 071, Val Loss: 0.6936\n",
            "Epoch: 001, Step: 072, Val Loss: 0.6941\n",
            "Epoch: 001, Step: 073, Val Loss: 0.7270\n",
            "Epoch: 001, Step: 074, Val Loss: 0.7255\n",
            "Epoch: 001, Step: 075, Val Loss: 0.7255\n",
            "Epoch: 001, Step: 076, Val Loss: 0.6610\n",
            "Epoch: 001, Step: 077, Val Loss: 0.6933\n",
            "Epoch: 001, Step: 078, Val Loss: 0.7268\n",
            "Epoch: 001, Step: 079, Val Loss: 0.7269\n",
            "Epoch: 001, Step: 080, Val Loss: 0.6933\n",
            "Epoch: 001, Step: 081, Val Loss: 0.7260\n",
            "Epoch: 001, Step: 082, Val Loss: 0.6932\n",
            "Epoch: 001, Step: 083, Val Loss: 0.6603\n",
            "Epoch: 001, Step: 084, Val Loss: 0.7257\n",
            "Epoch: 001, Step: 085, Val Loss: 0.6940\n",
            "Epoch: 001, Step: 086, Val Loss: 0.6932\n",
            "Epoch: 001, Step: 087, Val Loss: 0.6929\n",
            "Epoch: 001, Step: 088, Val Loss: 0.6919\n",
            "Epoch: 001, Step: 089, Val Loss: 0.6940\n",
            "Epoch: 001, Step: 090, Val Loss: 0.7257\n",
            "Epoch: 001, Step: 091, Val Loss: 0.7260\n",
            "Epoch: 001, Step: 092, Val Loss: 0.7276\n",
            "Epoch: 001, Step: 093, Val Loss: 0.6615\n",
            "Epoch: 001, Step: 094, Val Loss: 0.6940\n",
            "Epoch: 001, Step: 095, Val Loss: 0.6934\n",
            "Epoch: 001, Step: 096, Val Loss: 0.6941\n",
            "Epoch: 001, Step: 097, Val Loss: 0.7268\n",
            "Epoch: 001, Step: 098, Val Loss: 0.6937\n",
            "Epoch: 001, Step: 099, Val Loss: 0.6950\n",
            "Epoch: 001, Step: 100, Val Loss: 0.6944\n",
            "Epoch: 001, Step: 101, Val Loss: 0.7255\n",
            "Epoch: 001, Step: 102, Val Loss: 0.6940\n",
            "Epoch: 001, Step: 103, Val Loss: 0.7260\n",
            "Epoch: 001, Step: 104, Val Loss: 0.7257\n",
            "Epoch: 001, Step: 105, Val Loss: 0.7270\n",
            "Epoch: 001, Step: 106, Val Loss: 0.6931\n",
            "Epoch: 001, Step: 107, Val Loss: 0.6936\n",
            "Epoch: 001, Step: 108, Val Loss: 0.6942\n",
            "Epoch: 001, Step: 109, Val Loss: 0.6943\n",
            "Epoch: 001, Step: 110, Val Loss: 0.6923\n",
            "Epoch: 001, Step: 111, Val Loss: 0.6937\n",
            "Epoch: 001, Step: 112, Val Loss: 0.6909\n",
            "Epoch: 001, Step: 113, Val Loss: 0.6957\n",
            "Epoch: 001, Step: 114, Val Loss: 0.7267\n",
            "Epoch: 001, Step: 115, Val Loss: 0.7253\n",
            "Epoch: 001, Step: 116, Val Loss: 0.6944\n",
            "Epoch: 001, Step: 117, Val Loss: 0.6943\n",
            "Epoch: 002, Step: 000, Loss: 0.6946\n",
            "Epoch: 002, Step: 001, Loss: 0.6931\n",
            "Epoch: 002, Step: 002, Loss: 0.6905\n",
            "Epoch: 002, Step: 003, Loss: 0.6943\n",
            "Epoch: 002, Step: 004, Loss: 0.6948\n",
            "Epoch: 002, Step: 005, Loss: 0.6899\n",
            "Epoch: 002, Step: 006, Loss: 0.7239\n",
            "Epoch: 002, Step: 007, Loss: 0.6891\n",
            "Epoch: 002, Step: 008, Loss: 0.6622\n",
            "Epoch: 002, Step: 009, Loss: 0.6585\n",
            "Epoch: 002, Step: 010, Loss: 0.6874\n",
            "Epoch: 002, Step: 011, Loss: 0.7251\n",
            "Epoch: 002, Step: 012, Loss: 0.6964\n",
            "Epoch: 002, Step: 013, Loss: 0.6634\n",
            "Epoch: 002, Step: 014, Loss: 0.6605\n",
            "Epoch: 002, Step: 015, Loss: 0.6960\n",
            "Epoch: 002, Step: 016, Loss: 0.6667\n",
            "Epoch: 002, Step: 017, Loss: 0.6909\n",
            "Epoch: 002, Step: 018, Loss: 0.7271\n",
            "Epoch: 002, Step: 019, Loss: 0.7233\n",
            "Epoch: 002, Step: 020, Loss: 0.6946\n",
            "Epoch: 002, Step: 021, Loss: 0.6927\n",
            "Epoch: 002, Step: 022, Loss: 0.6880\n",
            "Epoch: 002, Step: 023, Loss: 0.6953\n",
            "Epoch: 002, Step: 024, Loss: 0.6943\n",
            "Epoch: 002, Step: 025, Loss: 0.6908\n",
            "Epoch: 002, Step: 026, Loss: 0.6862\n",
            "Epoch: 002, Step: 027, Loss: 0.7239\n",
            "Epoch: 002, Step: 028, Loss: 0.6927\n",
            "Epoch: 002, Step: 029, Loss: 0.6950\n",
            "Epoch: 002, Step: 030, Loss: 0.6631\n",
            "Epoch: 002, Step: 031, Loss: 0.6947\n",
            "Epoch: 002, Step: 032, Loss: 0.6568\n",
            "Epoch: 002, Step: 033, Loss: 0.6946\n",
            "Epoch: 002, Step: 034, Loss: 0.6924\n",
            "Epoch: 002, Step: 035, Loss: 0.7231\n",
            "Epoch: 002, Step: 036, Loss: 0.6947\n",
            "Epoch: 002, Step: 037, Loss: 0.6890\n",
            "Epoch: 002, Step: 038, Loss: 0.6926\n",
            "Epoch: 002, Step: 039, Loss: 0.7008\n",
            "Epoch: 002, Step: 040, Loss: 0.7204\n",
            "Epoch: 002, Step: 041, Loss: 0.6978\n",
            "Epoch: 002, Step: 042, Loss: 0.6930\n",
            "Epoch: 002, Step: 043, Loss: 0.7271\n",
            "Epoch: 002, Step: 044, Loss: 0.6956\n",
            "Epoch: 002, Step: 045, Loss: 0.6931\n",
            "Epoch: 002, Step: 046, Loss: 0.6829\n",
            "Epoch: 002, Step: 047, Loss: 0.6946\n",
            "Epoch: 002, Step: 048, Loss: 0.6968\n",
            "Epoch: 002, Step: 049, Loss: 0.7268\n",
            "Epoch: 002, Step: 050, Loss: 0.6947\n",
            "Epoch: 002, Step: 051, Loss: 0.6929\n",
            "Epoch: 002, Step: 052, Loss: 0.6617\n",
            "Epoch: 002, Step: 053, Loss: 0.7321\n",
            "Epoch: 002, Step: 054, Loss: 0.6915\n",
            "Epoch: 002, Step: 055, Loss: 0.6914\n",
            "Epoch: 002, Step: 056, Loss: 0.7147\n",
            "Epoch: 002, Step: 057, Loss: 0.6545\n",
            "Epoch: 002, Step: 058, Loss: 0.7226\n",
            "Epoch: 002, Step: 059, Loss: 0.6900\n",
            "Epoch: 002, Step: 060, Loss: 0.7272\n",
            "Epoch: 002, Step: 061, Loss: 0.7223\n",
            "Epoch: 002, Step: 062, Loss: 0.7313\n",
            "Epoch: 002, Step: 063, Loss: 0.6634\n",
            "Epoch: 002, Step: 064, Loss: 0.6583\n",
            "Epoch: 002, Step: 065, Loss: 0.6887\n",
            "Epoch: 002, Step: 066, Loss: 0.6941\n",
            "Epoch: 002, Step: 067, Loss: 0.6932\n",
            "Epoch: 002, Step: 068, Loss: 0.7222\n",
            "Epoch: 002, Step: 069, Loss: 0.7255\n",
            "Epoch: 002, Step: 070, Loss: 0.6604\n",
            "Epoch: 002, Step: 071, Loss: 0.7244\n",
            "Epoch: 002, Step: 072, Loss: 0.7280\n",
            "Epoch: 002, Step: 073, Loss: 0.6979\n",
            "Epoch: 002, Step: 074, Loss: 0.6905\n",
            "Epoch: 002, Step: 075, Loss: 0.7324\n",
            "Epoch: 002, Step: 076, Loss: 0.6625\n",
            "Epoch: 002, Step: 077, Loss: 0.6905\n",
            "Epoch: 002, Step: 078, Loss: 0.7276\n",
            "Epoch: 002, Step: 079, Loss: 0.7344\n",
            "Epoch: 002, Step: 080, Loss: 0.7265\n",
            "Epoch: 002, Step: 081, Loss: 0.6950\n",
            "Epoch: 002, Step: 082, Loss: 0.6862\n",
            "Epoch: 002, Step: 083, Loss: 0.7292\n",
            "Epoch: 002, Step: 084, Loss: 0.7251\n",
            "Epoch: 002, Step: 085, Loss: 0.6570\n",
            "Epoch: 002, Step: 086, Loss: 0.7246\n",
            "Epoch: 002, Step: 087, Loss: 0.6925\n",
            "Epoch: 002, Step: 088, Loss: 0.6948\n",
            "Epoch: 002, Step: 089, Loss: 0.7006\n",
            "Epoch: 002, Step: 090, Loss: 0.6932\n",
            "Epoch: 002, Step: 091, Loss: 0.7304\n",
            "Epoch: 002, Step: 092, Loss: 0.7220\n",
            "Epoch: 002, Step: 093, Loss: 0.6541\n",
            "Epoch: 002, Step: 094, Loss: 0.6868\n",
            "Epoch: 002, Step: 095, Loss: 0.6604\n",
            "Epoch: 002, Step: 096, Loss: 0.6927\n",
            "Epoch: 002, Step: 097, Loss: 0.6899\n",
            "Epoch: 002, Step: 098, Loss: 0.6976\n",
            "Epoch: 002, Step: 099, Loss: 0.6941\n",
            "Epoch: 002, Step: 100, Loss: 0.7255\n",
            "Epoch: 002, Step: 101, Loss: 0.6952\n",
            "Epoch: 002, Step: 102, Loss: 0.7315\n",
            "Epoch: 002, Step: 103, Loss: 0.6981\n",
            "Epoch: 002, Step: 104, Loss: 0.7277\n",
            "Epoch: 002, Step: 105, Loss: 0.6932\n",
            "Epoch: 002, Step: 106, Loss: 0.7265\n",
            "Epoch: 002, Step: 107, Loss: 0.7256\n",
            "Epoch: 002, Step: 108, Loss: 0.6968\n",
            "Epoch: 002, Step: 109, Loss: 0.6920\n",
            "Epoch: 002, Step: 110, Loss: 0.6627\n",
            "Epoch: 002, Step: 111, Loss: 0.6561\n",
            "Epoch: 002, Step: 112, Loss: 0.6630\n",
            "Epoch: 002, Step: 113, Loss: 0.6982\n",
            "Epoch: 002, Step: 114, Loss: 0.7205\n",
            "Epoch: 002, Step: 115, Loss: 0.6566\n",
            "Epoch: 002, Step: 116, Loss: 0.7235\n",
            "Epoch: 002, Step: 117, Loss: 0.7217\n",
            "Epoch: 002, Step: 118, Loss: 0.7216\n",
            "Epoch: 002, Step: 119, Loss: 0.7341\n",
            "Epoch: 002, Step: 120, Loss: 0.6659\n",
            "Epoch: 002, Step: 121, Loss: 0.7325\n",
            "Epoch: 002, Step: 122, Loss: 0.6609\n",
            "Epoch: 002, Step: 123, Loss: 0.6868\n",
            "Epoch: 002, Step: 124, Loss: 0.6914\n",
            "Epoch: 002, Step: 125, Loss: 0.7003\n",
            "Epoch: 002, Step: 126, Loss: 0.7316\n",
            "Epoch: 002, Step: 127, Loss: 0.6594\n",
            "Epoch: 002, Step: 128, Loss: 0.6878\n",
            "Epoch: 002, Step: 129, Loss: 0.6905\n",
            "Epoch: 002, Step: 130, Loss: 0.7255\n",
            "Epoch: 002, Step: 131, Loss: 0.6618\n",
            "Epoch: 002, Step: 132, Loss: 0.6938\n",
            "Epoch: 002, Step: 133, Loss: 0.6885\n",
            "Epoch: 002, Step: 134, Loss: 0.7246\n",
            "Epoch: 002, Step: 135, Loss: 0.6891\n",
            "Epoch: 002, Step: 136, Loss: 0.6915\n",
            "Epoch: 002, Step: 137, Loss: 0.6978\n",
            "Epoch: 002, Step: 138, Loss: 0.7234\n",
            "Epoch: 002, Step: 139, Loss: 0.7217\n",
            "Epoch: 002, Step: 140, Loss: 0.6925\n",
            "Epoch: 002, Step: 141, Loss: 0.6596\n",
            "Epoch: 002, Step: 142, Loss: 0.7270\n",
            "Epoch: 002, Step: 143, Loss: 0.6877\n",
            "Epoch: 002, Step: 144, Loss: 0.6943\n",
            "Epoch: 002, Step: 145, Loss: 0.7327\n",
            "Epoch: 002, Step: 146, Loss: 0.7243\n",
            "Epoch: 002, Step: 147, Loss: 0.6568\n",
            "Epoch: 002, Step: 148, Loss: 0.6667\n",
            "Epoch: 002, Step: 149, Loss: 0.6905\n",
            "Epoch: 002, Step: 150, Loss: 0.6958\n",
            "Epoch: 002, Step: 151, Loss: 0.6515\n",
            "Epoch: 002, Step: 152, Loss: 0.6940\n",
            "Epoch: 002, Step: 153, Loss: 0.6998\n",
            "Epoch: 002, Step: 154, Loss: 0.7278\n",
            "Epoch: 002, Step: 155, Loss: 0.6961\n",
            "Epoch: 002, Step: 156, Loss: 0.6950\n",
            "Epoch: 002, Step: 157, Loss: 0.7307\n",
            "Epoch: 002, Step: 158, Loss: 0.7249\n",
            "Epoch: 002, Step: 159, Loss: 0.7331\n",
            "Epoch: 002, Step: 160, Loss: 0.6631\n",
            "Epoch: 002, Step: 161, Loss: 0.7250\n",
            "Epoch: 002, Step: 162, Loss: 0.6956\n",
            "Epoch: 002, Step: 163, Loss: 0.6972\n",
            "Epoch: 002, Step: 164, Loss: 0.6611\n",
            "Epoch: 002, Step: 165, Loss: 0.6983\n",
            "Epoch: 002, Step: 166, Loss: 0.6916\n",
            "Epoch: 002, Step: 167, Loss: 0.6599\n",
            "Epoch: 002, Step: 168, Loss: 0.6980\n",
            "Epoch: 002, Step: 169, Loss: 0.6962\n",
            "Epoch: 002, Step: 170, Loss: 0.6608\n",
            "Epoch: 002, Step: 171, Loss: 0.6876\n",
            "Epoch: 002, Step: 172, Loss: 0.6964\n",
            "Epoch: 002, Step: 173, Loss: 0.6940\n",
            "Epoch: 002, Step: 174, Loss: 0.7276\n",
            "Epoch: 002, Step: 175, Loss: 0.6869\n",
            "Epoch: 002, Step: 176, Loss: 0.7047\n",
            "Epoch: 002, Step: 177, Loss: 0.7256\n",
            "Epoch: 002, Step: 178, Loss: 0.7288\n",
            "Epoch: 002, Step: 179, Loss: 0.6958\n",
            "Epoch: 002, Step: 180, Loss: 0.6676\n",
            "Epoch: 002, Step: 181, Loss: 0.7266\n",
            "Epoch: 002, Step: 182, Loss: 0.7254\n",
            "Epoch: 002, Step: 183, Loss: 0.7247\n",
            "Epoch: 002, Step: 184, Loss: 0.7249\n",
            "Epoch: 002, Step: 185, Loss: 0.7261\n",
            "Epoch: 002, Step: 186, Loss: 0.7234\n",
            "Epoch: 002, Step: 187, Loss: 0.6582\n",
            "Epoch: 002, Step: 188, Loss: 0.6587\n",
            "Epoch: 002, Step: 189, Loss: 0.6992\n",
            "Epoch: 002, Step: 190, Loss: 0.6950\n",
            "Epoch: 002, Step: 191, Loss: 0.7292\n",
            "Epoch: 002, Step: 192, Loss: 0.6928\n",
            "Epoch: 002, Step: 193, Loss: 0.6996\n",
            "Epoch: 002, Step: 194, Loss: 0.6675\n",
            "Epoch: 002, Step: 195, Loss: 0.6964\n",
            "Epoch: 002, Step: 196, Loss: 0.7299\n",
            "Epoch: 002, Step: 197, Loss: 0.6902\n",
            "Epoch: 002, Step: 198, Loss: 0.7305\n",
            "Epoch: 002, Step: 199, Loss: 0.7243\n",
            "Epoch: 002, Step: 200, Loss: 0.7235\n",
            "Epoch: 002, Step: 201, Loss: 0.6917\n",
            "Epoch: 002, Step: 202, Loss: 0.6988\n",
            "Epoch: 002, Step: 203, Loss: 0.7292\n",
            "Epoch: 002, Step: 204, Loss: 0.6878\n",
            "Epoch: 002, Step: 205, Loss: 0.7246\n",
            "Epoch: 002, Step: 206, Loss: 0.6989\n",
            "Epoch: 002, Step: 207, Loss: 0.6600\n",
            "Epoch: 002, Step: 208, Loss: 0.6984\n",
            "Epoch: 002, Step: 209, Loss: 0.6638\n",
            "Epoch: 002, Step: 210, Loss: 0.7004\n",
            "Epoch: 002, Step: 211, Loss: 0.6588\n",
            "Epoch: 002, Step: 212, Loss: 0.7012\n",
            "Epoch: 002, Step: 213, Loss: 0.7190\n",
            "Epoch: 002, Step: 214, Loss: 0.7303\n",
            "Epoch: 002, Step: 215, Loss: 0.6942\n",
            "Epoch: 002, Step: 216, Loss: 0.6569\n",
            "Epoch: 002, Step: 217, Loss: 0.6962\n",
            "Epoch: 002, Step: 218, Loss: 0.7225\n",
            "Epoch: 002, Step: 219, Loss: 0.7292\n",
            "Epoch: 002, Step: 220, Loss: 0.7248\n",
            "Epoch: 002, Step: 221, Loss: 0.6859\n",
            "Epoch: 002, Step: 222, Loss: 0.6992\n",
            "Epoch: 002, Step: 223, Loss: 0.7239\n",
            "Epoch: 002, Step: 224, Loss: 0.7249\n",
            "Epoch: 002, Step: 225, Loss: 0.7328\n",
            "Epoch: 002, Step: 226, Loss: 0.6840\n",
            "Epoch: 002, Step: 227, Loss: 0.7272\n",
            "Epoch: 002, Step: 228, Loss: 0.6958\n",
            "Epoch: 002, Step: 229, Loss: 0.6948\n",
            "Epoch: 002, Step: 230, Loss: 0.6895\n",
            "Epoch: 002, Step: 231, Loss: 0.7289\n",
            "Epoch: 002, Step: 232, Loss: 0.7218\n",
            "Epoch: 002, Step: 233, Loss: 0.6988\n",
            "Epoch: 002, Step: 234, Loss: 0.6678\n",
            "Epoch: 002, Step: 235, Loss: 0.7220\n",
            "Epoch: 002, Step: 236, Loss: 0.6929\n",
            "Epoch: 002, Step: 237, Loss: 0.6984\n",
            "Epoch: 002, Step: 238, Loss: 0.6526\n",
            "Epoch: 002, Step: 239, Loss: 0.7301\n",
            "Epoch: 002, Step: 240, Loss: 0.6646\n",
            "Epoch: 002, Step: 241, Loss: 0.6611\n",
            "Epoch: 002, Step: 242, Loss: 0.6940\n",
            "Epoch: 002, Step: 243, Loss: 0.6956\n",
            "Epoch: 002, Step: 244, Loss: 0.7235\n",
            "Epoch: 002, Step: 245, Loss: 0.6917\n",
            "Epoch: 002, Step: 246, Loss: 0.7246\n",
            "Epoch: 002, Step: 247, Loss: 0.7014\n",
            "Epoch: 002, Step: 248, Loss: 0.6922\n",
            "Epoch: 002, Step: 249, Loss: 0.6921\n",
            "Epoch: 002, Step: 250, Loss: 0.6857\n",
            "Epoch: 002, Step: 251, Loss: 0.7007\n",
            "Epoch: 002, Step: 252, Loss: 0.6636\n",
            "Epoch: 002, Step: 253, Loss: 0.7005\n",
            "Epoch: 002, Step: 254, Loss: 0.7193\n",
            "Epoch: 002, Step: 255, Loss: 0.6909\n",
            "Epoch: 002, Step: 256, Loss: 0.7303\n",
            "Epoch: 002, Step: 257, Loss: 0.6896\n",
            "Epoch: 002, Step: 258, Loss: 0.7186\n",
            "Epoch: 002, Step: 259, Loss: 0.6934\n",
            "Epoch: 002, Step: 260, Loss: 0.7009\n",
            "Epoch: 002, Step: 261, Loss: 0.7258\n",
            "Epoch: 002, Step: 262, Loss: 0.7285\n",
            "Epoch: 002, Step: 263, Loss: 0.7294\n",
            "Epoch: 002, Step: 264, Loss: 0.6914\n",
            "Epoch: 002, Step: 265, Loss: 0.7030\n",
            "Epoch: 002, Step: 266, Loss: 0.6953\n",
            "Epoch: 002, Step: 267, Loss: 0.7288\n",
            "Epoch: 002, Step: 268, Loss: 0.7252\n",
            "Epoch: 002, Step: 269, Loss: 0.6999\n",
            "Epoch: 002, Step: 270, Loss: 0.6604\n",
            "Epoch: 002, Step: 271, Loss: 0.7205\n",
            "Epoch: 002, Step: 272, Loss: 0.6873\n",
            "Epoch: 002, Step: 273, Loss: 0.6893\n",
            "Epoch: 002, Step: 274, Loss: 0.7003\n",
            "Epoch: 002, Step: 275, Loss: 0.6971\n",
            "Epoch: 002, Step: 276, Loss: 0.7252\n",
            "Epoch: 002, Step: 277, Loss: 0.6980\n",
            "Epoch: 002, Step: 278, Loss: 0.6937\n",
            "Epoch: 002, Step: 279, Loss: 0.6890\n",
            "Epoch: 002, Step: 280, Loss: 0.6556\n",
            "Epoch: 002, Step: 281, Loss: 0.7020\n",
            "Epoch: 002, Step: 282, Loss: 0.7241\n",
            "Epoch: 002, Step: 283, Loss: 0.7253\n",
            "Epoch: 002, Step: 284, Loss: 0.7271\n",
            "Epoch: 002, Step: 285, Loss: 0.7288\n",
            "Epoch: 002, Step: 286, Loss: 0.6924\n",
            "Epoch: 002, Step: 287, Loss: 0.7275\n",
            "Epoch: 002, Step: 288, Loss: 0.6908\n",
            "Epoch: 002, Step: 289, Loss: 0.7308\n",
            "Epoch: 002, Step: 290, Loss: 0.6972\n",
            "Epoch: 002, Step: 291, Loss: 0.6967\n",
            "Epoch: 002, Step: 292, Loss: 0.7195\n",
            "Epoch: 002, Step: 293, Loss: 0.7265\n",
            "Epoch: 002, Step: 294, Loss: 0.6591\n",
            "Epoch: 002, Step: 295, Loss: 0.6565\n",
            "Epoch: 002, Step: 296, Loss: 0.6986\n",
            "Epoch: 002, Step: 297, Loss: 0.7207\n",
            "Epoch: 002, Step: 298, Loss: 0.7261\n",
            "Epoch: 002, Step: 299, Loss: 0.6613\n",
            "Epoch: 002, Step: 300, Loss: 0.7049\n",
            "Epoch: 002, Step: 301, Loss: 0.6923\n",
            "Epoch: 002, Step: 302, Loss: 0.6932\n",
            "Epoch: 002, Step: 303, Loss: 0.6642\n",
            "Epoch: 002, Step: 304, Loss: 0.7311\n",
            "Epoch: 002, Step: 305, Loss: 0.6561\n",
            "Epoch: 002, Step: 306, Loss: 0.6910\n",
            "Epoch: 002, Step: 307, Loss: 0.6632\n",
            "Epoch: 002, Step: 308, Loss: 0.7268\n",
            "Epoch: 002, Step: 309, Loss: 0.7223\n",
            "Epoch: 002, Step: 310, Loss: 0.7342\n",
            "Epoch: 002, Step: 311, Loss: 0.6962\n",
            "Epoch: 002, Step: 312, Loss: 0.6695\n",
            "Epoch: 002, Step: 313, Loss: 0.7250\n",
            "Epoch: 002, Step: 314, Loss: 0.6899\n",
            "Epoch: 002, Step: 315, Loss: 0.7221\n",
            "Epoch: 002, Step: 316, Loss: 0.6970\n",
            "Epoch: 002, Step: 317, Loss: 0.6960\n",
            "Epoch: 002, Step: 318, Loss: 0.7246\n",
            "Epoch: 002, Step: 319, Loss: 0.7277\n",
            "Epoch: 002, Step: 320, Loss: 0.6895\n",
            "Epoch: 002, Step: 321, Loss: 0.7277\n",
            "Epoch: 002, Step: 322, Loss: 0.6608\n",
            "Epoch: 002, Step: 323, Loss: 0.6949\n",
            "Epoch: 002, Step: 324, Loss: 0.7251\n",
            "Epoch: 002, Step: 325, Loss: 0.7227\n",
            "Epoch: 002, Step: 326, Loss: 0.6625\n",
            "Epoch: 002, Step: 327, Loss: 0.6653\n",
            "Epoch: 002, Step: 328, Loss: 0.7233\n",
            "Epoch: 002, Step: 329, Loss: 0.6988\n",
            "Epoch: 002, Step: 330, Loss: 0.6606\n",
            "Epoch: 002, Step: 331, Loss: 0.7212\n",
            "Epoch: 002, Step: 332, Loss: 0.6960\n",
            "Epoch: 002, Step: 333, Loss: 0.7260\n",
            "Epoch: 002, Step: 334, Loss: 0.6907\n",
            "Epoch: 002, Step: 335, Loss: 0.6562\n",
            "Epoch: 002, Step: 336, Loss: 0.6932\n",
            "Epoch: 002, Step: 337, Loss: 0.6933\n",
            "Epoch: 002, Step: 338, Loss: 0.6619\n",
            "Epoch: 002, Step: 339, Loss: 0.6623\n",
            "Epoch: 002, Step: 340, Loss: 0.7223\n",
            "Epoch: 002, Step: 341, Loss: 0.7241\n",
            "Epoch: 002, Step: 342, Loss: 0.6633\n",
            "Epoch: 002, Step: 343, Loss: 0.6863\n",
            "Epoch: 002, Step: 344, Loss: 0.6601\n",
            "Epoch: 002, Step: 345, Loss: 0.6958\n",
            "Epoch: 002, Step: 346, Loss: 0.6593\n",
            "Epoch: 002, Step: 347, Loss: 0.6923\n",
            "Epoch: 002, Step: 348, Loss: 0.6957\n",
            "Epoch: 002, Step: 349, Loss: 0.6639\n",
            "Epoch: 002, Step: 350, Loss: 0.7306\n",
            "Epoch: 002, Step: 351, Loss: 0.6910\n",
            "Epoch: 002, Step: 352, Loss: 0.6932\n",
            "Epoch: 002, Step: 353, Loss: 0.6901\n",
            "Epoch: 002, Step: 354, Loss: 0.7281\n",
            "Epoch: 002, Step: 355, Loss: 0.7219\n",
            "Epoch: 002, Step: 356, Loss: 0.6913\n",
            "Epoch: 002, Step: 357, Loss: 0.7198\n",
            "Epoch: 002, Step: 358, Loss: 0.7162\n",
            "Epoch: 002, Step: 359, Loss: 0.7192\n",
            "Epoch: 002, Step: 360, Loss: 0.6937\n",
            "Epoch: 002, Step: 361, Loss: 0.6564\n",
            "Epoch: 002, Step: 362, Loss: 0.6915\n",
            "Epoch: 002, Step: 363, Loss: 0.6949\n",
            "Epoch: 002, Step: 364, Loss: 0.6658\n",
            "Epoch: 002, Step: 365, Loss: 0.7297\n",
            "Epoch: 002, Step: 366, Loss: 0.7357\n",
            "Epoch: 002, Step: 367, Loss: 0.7224\n",
            "Epoch: 002, Step: 368, Loss: 0.7296\n",
            "Epoch: 002, Step: 369, Loss: 0.6938\n",
            "Epoch: 002, Step: 370, Loss: 0.6985\n",
            "Epoch: 002, Step: 371, Loss: 0.6995\n",
            "Epoch: 002, Step: 372, Loss: 0.6652\n",
            "Epoch: 002, Step: 373, Loss: 0.7327\n",
            "Epoch: 002, Step: 374, Loss: 0.6908\n",
            "Epoch: 002, Step: 375, Loss: 0.6616\n",
            "Epoch: 002, Step: 376, Loss: 0.7237\n",
            "Epoch: 002, Step: 377, Loss: 0.7234\n",
            "Epoch: 002, Step: 378, Loss: 0.6933\n",
            "Epoch: 002, Step: 379, Loss: 0.7254\n",
            "Epoch: 002, Step: 380, Loss: 0.7191\n",
            "Epoch: 002, Step: 381, Loss: 0.6647\n",
            "Epoch: 002, Step: 382, Loss: 0.6922\n",
            "Epoch: 002, Step: 383, Loss: 0.6910\n",
            "Epoch: 002, Step: 384, Loss: 0.7248\n",
            "Epoch: 002, Step: 385, Loss: 0.7002\n",
            "Epoch: 002, Step: 386, Loss: 0.7274\n",
            "Epoch: 002, Step: 387, Loss: 0.7317\n",
            "Epoch: 002, Step: 388, Loss: 0.6988\n",
            "Epoch: 002, Step: 389, Loss: 0.6977\n",
            "Epoch: 002, Step: 390, Loss: 0.6934\n",
            "Epoch: 002, Step: 391, Loss: 0.7283\n",
            "Epoch: 002, Step: 392, Loss: 0.6934\n",
            "Epoch: 002, Step: 393, Loss: 0.6574\n",
            "Epoch: 002, Step: 394, Loss: 0.6957\n",
            "Epoch: 002, Step: 395, Loss: 0.6996\n",
            "Epoch: 002, Step: 396, Loss: 0.6644\n",
            "Epoch: 002, Step: 397, Loss: 0.7298\n",
            "Epoch: 002, Step: 398, Loss: 0.7252\n",
            "Epoch: 002, Step: 399, Loss: 0.7378\n",
            "Epoch: 002, Step: 400, Loss: 0.7344\n",
            "Epoch: 002, Step: 401, Loss: 0.7305\n",
            "Epoch: 002, Step: 402, Loss: 0.7282\n",
            "Epoch: 002, Step: 403, Loss: 0.7274\n",
            "Epoch: 002, Step: 404, Loss: 0.7332\n",
            "Epoch: 002, Step: 405, Loss: 0.6635\n",
            "Epoch: 002, Step: 406, Loss: 0.6657\n",
            "Epoch: 002, Step: 407, Loss: 0.6608\n",
            "Epoch: 002, Step: 408, Loss: 0.7229\n",
            "Epoch: 002, Step: 409, Loss: 0.6657\n",
            "Epoch: 002, Step: 410, Loss: 0.6928\n",
            "Epoch: 002, Step: 411, Loss: 0.6918\n",
            "Epoch: 002, Step: 412, Loss: 0.7179\n",
            "Epoch: 002, Step: 000, Val Loss: 0.7256\n",
            "Epoch: 002, Step: 001, Val Loss: 0.7259\n",
            "Epoch: 002, Step: 002, Val Loss: 0.6929\n",
            "Epoch: 002, Step: 003, Val Loss: 0.6935\n",
            "Epoch: 002, Step: 004, Val Loss: 0.6947\n",
            "Epoch: 002, Step: 005, Val Loss: 0.6935\n",
            "Epoch: 002, Step: 006, Val Loss: 0.7273\n",
            "Epoch: 002, Step: 007, Val Loss: 0.6925\n",
            "Epoch: 002, Step: 008, Val Loss: 0.6938\n",
            "Epoch: 002, Step: 009, Val Loss: 0.6931\n",
            "Epoch: 002, Step: 010, Val Loss: 0.7276\n",
            "Epoch: 002, Step: 011, Val Loss: 0.7265\n",
            "Epoch: 002, Step: 012, Val Loss: 0.6934\n",
            "Epoch: 002, Step: 013, Val Loss: 0.6933\n",
            "Epoch: 002, Step: 014, Val Loss: 0.6934\n",
            "Epoch: 002, Step: 015, Val Loss: 0.6618\n",
            "Epoch: 002, Step: 016, Val Loss: 0.6933\n",
            "Epoch: 002, Step: 017, Val Loss: 0.6940\n",
            "Epoch: 002, Step: 018, Val Loss: 0.6938\n",
            "Epoch: 002, Step: 019, Val Loss: 0.7261\n",
            "Epoch: 002, Step: 020, Val Loss: 0.6943\n",
            "Epoch: 002, Step: 021, Val Loss: 0.6944\n",
            "Epoch: 002, Step: 022, Val Loss: 0.6940\n",
            "Epoch: 002, Step: 023, Val Loss: 0.6940\n",
            "Epoch: 002, Step: 024, Val Loss: 0.6916\n",
            "Epoch: 002, Step: 025, Val Loss: 0.7256\n",
            "Epoch: 002, Step: 026, Val Loss: 0.6937\n",
            "Epoch: 002, Step: 027, Val Loss: 0.7267\n",
            "Epoch: 002, Step: 028, Val Loss: 0.6913\n",
            "Epoch: 002, Step: 029, Val Loss: 0.6945\n",
            "Epoch: 002, Step: 030, Val Loss: 0.6936\n",
            "Epoch: 002, Step: 031, Val Loss: 0.6934\n",
            "Epoch: 002, Step: 032, Val Loss: 0.6932\n",
            "Epoch: 002, Step: 033, Val Loss: 0.6930\n",
            "Epoch: 002, Step: 034, Val Loss: 0.7249\n",
            "Epoch: 002, Step: 035, Val Loss: 0.6609\n",
            "Epoch: 002, Step: 036, Val Loss: 0.6944\n",
            "Epoch: 002, Step: 037, Val Loss: 0.7257\n",
            "Epoch: 002, Step: 038, Val Loss: 0.6934\n",
            "Epoch: 002, Step: 039, Val Loss: 0.6607\n",
            "Epoch: 002, Step: 040, Val Loss: 0.6946\n",
            "Epoch: 002, Step: 041, Val Loss: 0.7255\n",
            "Epoch: 002, Step: 042, Val Loss: 0.6939\n",
            "Epoch: 002, Step: 043, Val Loss: 0.6607\n",
            "Epoch: 002, Step: 044, Val Loss: 0.6951\n",
            "Epoch: 002, Step: 045, Val Loss: 0.7267\n",
            "Epoch: 002, Step: 046, Val Loss: 0.7263\n",
            "Epoch: 002, Step: 047, Val Loss: 0.7242\n",
            "Epoch: 002, Step: 048, Val Loss: 0.7274\n",
            "Epoch: 002, Step: 049, Val Loss: 0.6941\n",
            "Epoch: 002, Step: 050, Val Loss: 0.6602\n",
            "Epoch: 002, Step: 051, Val Loss: 0.6604\n",
            "Epoch: 002, Step: 052, Val Loss: 0.6610\n",
            "Epoch: 002, Step: 053, Val Loss: 0.7264\n",
            "Epoch: 002, Step: 054, Val Loss: 0.7252\n",
            "Epoch: 002, Step: 055, Val Loss: 0.7260\n",
            "Epoch: 002, Step: 056, Val Loss: 0.7262\n",
            "Epoch: 002, Step: 057, Val Loss: 0.6604\n",
            "Epoch: 002, Step: 058, Val Loss: 0.6940\n",
            "Epoch: 002, Step: 059, Val Loss: 0.6945\n",
            "Epoch: 002, Step: 060, Val Loss: 0.6955\n",
            "Epoch: 002, Step: 061, Val Loss: 0.7265\n",
            "Epoch: 002, Step: 062, Val Loss: 0.6934\n",
            "Epoch: 002, Step: 063, Val Loss: 0.7272\n",
            "Epoch: 002, Step: 064, Val Loss: 0.6933\n",
            "Epoch: 002, Step: 065, Val Loss: 0.6613\n",
            "Epoch: 002, Step: 066, Val Loss: 0.7262\n",
            "Epoch: 002, Step: 067, Val Loss: 0.7257\n",
            "Epoch: 002, Step: 068, Val Loss: 0.6921\n",
            "Epoch: 002, Step: 069, Val Loss: 0.6940\n",
            "Epoch: 002, Step: 070, Val Loss: 0.6910\n",
            "Epoch: 002, Step: 071, Val Loss: 0.6960\n",
            "Epoch: 002, Step: 072, Val Loss: 0.6934\n",
            "Epoch: 002, Step: 073, Val Loss: 0.7279\n",
            "Epoch: 002, Step: 074, Val Loss: 0.6938\n",
            "Epoch: 002, Step: 075, Val Loss: 0.6928\n",
            "Epoch: 002, Step: 076, Val Loss: 0.6933\n",
            "Epoch: 002, Step: 077, Val Loss: 0.6925\n",
            "Epoch: 002, Step: 078, Val Loss: 0.6938\n",
            "Epoch: 002, Step: 079, Val Loss: 0.6931\n",
            "Epoch: 002, Step: 080, Val Loss: 0.7269\n",
            "Epoch: 002, Step: 081, Val Loss: 0.6940\n",
            "Epoch: 002, Step: 082, Val Loss: 0.7276\n",
            "Epoch: 002, Step: 083, Val Loss: 0.7261\n",
            "Epoch: 002, Step: 084, Val Loss: 0.6936\n",
            "Epoch: 002, Step: 085, Val Loss: 0.6935\n",
            "Epoch: 002, Step: 086, Val Loss: 0.6614\n",
            "Epoch: 002, Step: 087, Val Loss: 0.7271\n",
            "Epoch: 002, Step: 088, Val Loss: 0.6608\n",
            "Epoch: 002, Step: 089, Val Loss: 0.7267\n",
            "Epoch: 002, Step: 090, Val Loss: 0.7259\n",
            "Epoch: 002, Step: 091, Val Loss: 0.6612\n",
            "Epoch: 002, Step: 092, Val Loss: 0.7269\n",
            "Epoch: 002, Step: 093, Val Loss: 0.6933\n",
            "Epoch: 002, Step: 094, Val Loss: 0.6621\n",
            "Epoch: 002, Step: 095, Val Loss: 0.7268\n",
            "Epoch: 002, Step: 096, Val Loss: 0.7266\n",
            "Epoch: 002, Step: 097, Val Loss: 0.6617\n",
            "Epoch: 002, Step: 098, Val Loss: 0.6936\n",
            "Epoch: 002, Step: 099, Val Loss: 0.7256\n",
            "Epoch: 002, Step: 100, Val Loss: 0.6928\n",
            "Epoch: 002, Step: 101, Val Loss: 0.6923\n",
            "Epoch: 002, Step: 102, Val Loss: 0.6934\n",
            "Epoch: 002, Step: 103, Val Loss: 0.6596\n",
            "Epoch: 002, Step: 104, Val Loss: 0.7264\n",
            "Epoch: 002, Step: 105, Val Loss: 0.7262\n",
            "Epoch: 002, Step: 106, Val Loss: 0.6936\n",
            "Epoch: 002, Step: 107, Val Loss: 0.7258\n",
            "Epoch: 002, Step: 108, Val Loss: 0.6608\n",
            "Epoch: 002, Step: 109, Val Loss: 0.6919\n",
            "Epoch: 002, Step: 110, Val Loss: 0.6593\n",
            "Epoch: 002, Step: 111, Val Loss: 0.7268\n",
            "Epoch: 002, Step: 112, Val Loss: 0.7258\n",
            "Epoch: 002, Step: 113, Val Loss: 0.6934\n",
            "Epoch: 002, Step: 114, Val Loss: 0.6935\n",
            "Epoch: 002, Step: 115, Val Loss: 0.7255\n",
            "Epoch: 002, Step: 116, Val Loss: 0.6939\n",
            "Epoch: 002, Step: 117, Val Loss: 0.6929\n",
            "Epoch: 003, Step: 000, Loss: 0.6933\n",
            "Epoch: 003, Step: 001, Loss: 0.7286\n",
            "Epoch: 003, Step: 002, Loss: 0.7203\n",
            "Epoch: 003, Step: 003, Loss: 0.6976\n",
            "Epoch: 003, Step: 004, Loss: 0.7015\n",
            "Epoch: 003, Step: 005, Loss: 0.6926\n",
            "Epoch: 003, Step: 006, Loss: 0.6591\n",
            "Epoch: 003, Step: 007, Loss: 0.7248\n",
            "Epoch: 003, Step: 008, Loss: 0.7351\n",
            "Epoch: 003, Step: 009, Loss: 0.6917\n",
            "Epoch: 003, Step: 010, Loss: 0.6953\n",
            "Epoch: 003, Step: 011, Loss: 0.6888\n",
            "Epoch: 003, Step: 012, Loss: 0.7280\n",
            "Epoch: 003, Step: 013, Loss: 0.6600\n",
            "Epoch: 003, Step: 014, Loss: 0.7330\n",
            "Epoch: 003, Step: 015, Loss: 0.6892\n",
            "Epoch: 003, Step: 016, Loss: 0.7301\n",
            "Epoch: 003, Step: 017, Loss: 0.7225\n",
            "Epoch: 003, Step: 018, Loss: 0.6902\n",
            "Epoch: 003, Step: 019, Loss: 0.6928\n",
            "Epoch: 003, Step: 020, Loss: 0.7278\n",
            "Epoch: 003, Step: 021, Loss: 0.7228\n",
            "Epoch: 003, Step: 022, Loss: 0.7254\n",
            "Epoch: 003, Step: 023, Loss: 0.6615\n",
            "Epoch: 003, Step: 024, Loss: 0.6633\n",
            "Epoch: 003, Step: 025, Loss: 0.6938\n",
            "Epoch: 003, Step: 026, Loss: 0.7263\n",
            "Epoch: 003, Step: 027, Loss: 0.7016\n",
            "Epoch: 003, Step: 028, Loss: 0.6950\n",
            "Epoch: 003, Step: 029, Loss: 0.6887\n",
            "Epoch: 003, Step: 030, Loss: 0.6944\n",
            "Epoch: 003, Step: 031, Loss: 0.6927\n",
            "Epoch: 003, Step: 032, Loss: 0.6954\n",
            "Epoch: 003, Step: 033, Loss: 0.7276\n",
            "Epoch: 003, Step: 034, Loss: 0.7005\n",
            "Epoch: 003, Step: 035, Loss: 0.7228\n",
            "Epoch: 003, Step: 036, Loss: 0.6590\n",
            "Epoch: 003, Step: 037, Loss: 0.6953\n",
            "Epoch: 003, Step: 038, Loss: 0.6902\n",
            "Epoch: 003, Step: 039, Loss: 0.7305\n",
            "Epoch: 003, Step: 040, Loss: 0.6565\n",
            "Epoch: 003, Step: 041, Loss: 0.6928\n",
            "Epoch: 003, Step: 042, Loss: 0.7226\n",
            "Epoch: 003, Step: 043, Loss: 0.6673\n",
            "Epoch: 003, Step: 044, Loss: 0.6913\n",
            "Epoch: 003, Step: 045, Loss: 0.7001\n",
            "Epoch: 003, Step: 046, Loss: 0.7273\n",
            "Epoch: 003, Step: 047, Loss: 0.7345\n",
            "Epoch: 003, Step: 048, Loss: 0.7206\n",
            "Epoch: 003, Step: 049, Loss: 0.6969\n",
            "Epoch: 003, Step: 050, Loss: 0.6984\n",
            "Epoch: 003, Step: 051, Loss: 0.7235\n",
            "Epoch: 003, Step: 052, Loss: 0.7279\n",
            "Epoch: 003, Step: 053, Loss: 0.6943\n",
            "Epoch: 003, Step: 054, Loss: 0.7245\n",
            "Epoch: 003, Step: 055, Loss: 0.6903\n",
            "Epoch: 003, Step: 056, Loss: 0.6946\n",
            "Epoch: 003, Step: 057, Loss: 0.7259\n",
            "Epoch: 003, Step: 058, Loss: 0.7238\n",
            "Epoch: 003, Step: 059, Loss: 0.7238\n",
            "Epoch: 003, Step: 060, Loss: 0.7285\n",
            "Epoch: 003, Step: 061, Loss: 0.7242\n",
            "Epoch: 003, Step: 062, Loss: 0.6640\n",
            "Epoch: 003, Step: 063, Loss: 0.7244\n",
            "Epoch: 003, Step: 064, Loss: 0.6851\n",
            "Epoch: 003, Step: 065, Loss: 0.6933\n",
            "Epoch: 003, Step: 066, Loss: 0.6624\n",
            "Epoch: 003, Step: 067, Loss: 0.6900\n",
            "Epoch: 003, Step: 068, Loss: 0.7293\n",
            "Epoch: 003, Step: 069, Loss: 0.6600\n",
            "Epoch: 003, Step: 070, Loss: 0.7000\n",
            "Epoch: 003, Step: 071, Loss: 0.6977\n",
            "Epoch: 003, Step: 072, Loss: 0.7294\n",
            "Epoch: 003, Step: 073, Loss: 0.6572\n",
            "Epoch: 003, Step: 074, Loss: 0.6617\n",
            "Epoch: 003, Step: 075, Loss: 0.6665\n",
            "Epoch: 003, Step: 076, Loss: 0.6948\n",
            "Epoch: 003, Step: 077, Loss: 0.6896\n",
            "Epoch: 003, Step: 078, Loss: 0.7294\n",
            "Epoch: 003, Step: 079, Loss: 0.6945\n",
            "Epoch: 003, Step: 080, Loss: 0.7230\n",
            "Epoch: 003, Step: 081, Loss: 0.7012\n",
            "Epoch: 003, Step: 082, Loss: 0.6874\n",
            "Epoch: 003, Step: 083, Loss: 0.6639\n",
            "Epoch: 003, Step: 084, Loss: 0.6903\n",
            "Epoch: 003, Step: 085, Loss: 0.6901\n",
            "Epoch: 003, Step: 086, Loss: 0.6881\n",
            "Epoch: 003, Step: 087, Loss: 0.6964\n",
            "Epoch: 003, Step: 088, Loss: 0.6942\n",
            "Epoch: 003, Step: 089, Loss: 0.6925\n",
            "Epoch: 003, Step: 090, Loss: 0.7191\n",
            "Epoch: 003, Step: 091, Loss: 0.6924\n",
            "Epoch: 003, Step: 092, Loss: 0.6589\n",
            "Epoch: 003, Step: 093, Loss: 0.7287\n",
            "Epoch: 003, Step: 094, Loss: 0.6954\n",
            "Epoch: 003, Step: 095, Loss: 0.6978\n",
            "Epoch: 003, Step: 096, Loss: 0.7243\n",
            "Epoch: 003, Step: 097, Loss: 0.6676\n",
            "Epoch: 003, Step: 098, Loss: 0.6938\n",
            "Epoch: 003, Step: 099, Loss: 0.7001\n",
            "Epoch: 003, Step: 100, Loss: 0.6977\n",
            "Epoch: 003, Step: 101, Loss: 0.6852\n",
            "Epoch: 003, Step: 102, Loss: 0.7353\n",
            "Epoch: 003, Step: 103, Loss: 0.6580\n",
            "Epoch: 003, Step: 104, Loss: 0.6931\n",
            "Epoch: 003, Step: 105, Loss: 0.6594\n",
            "Epoch: 003, Step: 106, Loss: 0.6970\n",
            "Epoch: 003, Step: 107, Loss: 0.6921\n",
            "Epoch: 003, Step: 108, Loss: 0.7311\n",
            "Epoch: 003, Step: 109, Loss: 0.6845\n",
            "Epoch: 003, Step: 110, Loss: 0.7268\n",
            "Epoch: 003, Step: 111, Loss: 0.6961\n",
            "Epoch: 003, Step: 112, Loss: 0.7250\n",
            "Epoch: 003, Step: 113, Loss: 0.7219\n",
            "Epoch: 003, Step: 114, Loss: 0.6626\n",
            "Epoch: 003, Step: 115, Loss: 0.6980\n",
            "Epoch: 003, Step: 116, Loss: 0.7241\n",
            "Epoch: 003, Step: 117, Loss: 0.6929\n",
            "Epoch: 003, Step: 118, Loss: 0.6974\n",
            "Epoch: 003, Step: 119, Loss: 0.7263\n",
            "Epoch: 003, Step: 120, Loss: 0.6579\n",
            "Epoch: 003, Step: 121, Loss: 0.7268\n",
            "Epoch: 003, Step: 122, Loss: 0.6975\n",
            "Epoch: 003, Step: 123, Loss: 0.7220\n",
            "Epoch: 003, Step: 124, Loss: 0.7292\n",
            "Epoch: 003, Step: 125, Loss: 0.7304\n",
            "Epoch: 003, Step: 126, Loss: 0.6558\n",
            "Epoch: 003, Step: 127, Loss: 0.6581\n",
            "Epoch: 003, Step: 128, Loss: 0.7254\n",
            "Epoch: 003, Step: 129, Loss: 0.6568\n",
            "Epoch: 003, Step: 130, Loss: 0.6652\n",
            "Epoch: 003, Step: 131, Loss: 0.6901\n",
            "Epoch: 003, Step: 132, Loss: 0.6916\n",
            "Epoch: 003, Step: 133, Loss: 0.7089\n",
            "Epoch: 003, Step: 134, Loss: 0.7217\n",
            "Epoch: 003, Step: 135, Loss: 0.6607\n",
            "Epoch: 003, Step: 136, Loss: 0.6893\n",
            "Epoch: 003, Step: 137, Loss: 0.6581\n",
            "Epoch: 003, Step: 138, Loss: 0.6952\n",
            "Epoch: 003, Step: 139, Loss: 0.6682\n",
            "Epoch: 003, Step: 140, Loss: 0.6899\n",
            "Epoch: 003, Step: 141, Loss: 0.7273\n",
            "Epoch: 003, Step: 142, Loss: 0.7287\n",
            "Epoch: 003, Step: 143, Loss: 0.6676\n",
            "Epoch: 003, Step: 144, Loss: 0.6979\n",
            "Epoch: 003, Step: 145, Loss: 0.6974\n",
            "Epoch: 003, Step: 146, Loss: 0.7211\n",
            "Epoch: 003, Step: 147, Loss: 0.6595\n",
            "Epoch: 003, Step: 148, Loss: 0.7231\n",
            "Epoch: 003, Step: 149, Loss: 0.7322\n",
            "Epoch: 003, Step: 150, Loss: 0.6989\n",
            "Epoch: 003, Step: 151, Loss: 0.6606\n",
            "Epoch: 003, Step: 152, Loss: 0.7269\n",
            "Epoch: 003, Step: 153, Loss: 0.7245\n",
            "Epoch: 003, Step: 154, Loss: 0.6904\n",
            "Epoch: 003, Step: 155, Loss: 0.6936\n",
            "Epoch: 003, Step: 156, Loss: 0.6635\n",
            "Epoch: 003, Step: 157, Loss: 0.7272\n",
            "Epoch: 003, Step: 158, Loss: 0.7266\n",
            "Epoch: 003, Step: 159, Loss: 0.6968\n",
            "Epoch: 003, Step: 160, Loss: 0.7250\n",
            "Epoch: 003, Step: 161, Loss: 0.7266\n",
            "Epoch: 003, Step: 162, Loss: 0.6588\n",
            "Epoch: 003, Step: 163, Loss: 0.6922\n",
            "Epoch: 003, Step: 164, Loss: 0.7213\n",
            "Epoch: 003, Step: 165, Loss: 0.6566\n",
            "Epoch: 003, Step: 166, Loss: 0.6917\n",
            "Epoch: 003, Step: 167, Loss: 0.6922\n",
            "Epoch: 003, Step: 168, Loss: 0.6901\n",
            "Epoch: 003, Step: 169, Loss: 0.6979\n",
            "Epoch: 003, Step: 170, Loss: 0.7246\n",
            "Epoch: 003, Step: 171, Loss: 0.6958\n",
            "Epoch: 003, Step: 172, Loss: 0.7204\n",
            "Epoch: 003, Step: 173, Loss: 0.7059\n",
            "Epoch: 003, Step: 174, Loss: 0.7281\n",
            "Epoch: 003, Step: 175, Loss: 0.7298\n",
            "Epoch: 003, Step: 176, Loss: 0.6929\n",
            "Epoch: 003, Step: 177, Loss: 0.7223\n",
            "Epoch: 003, Step: 178, Loss: 0.7280\n",
            "Epoch: 003, Step: 179, Loss: 0.6903\n",
            "Epoch: 003, Step: 180, Loss: 0.7297\n",
            "Epoch: 003, Step: 181, Loss: 0.7262\n",
            "Epoch: 003, Step: 182, Loss: 0.6886\n",
            "Epoch: 003, Step: 183, Loss: 0.7327\n",
            "Epoch: 003, Step: 184, Loss: 0.7252\n",
            "Epoch: 003, Step: 185, Loss: 0.6947\n",
            "Epoch: 003, Step: 186, Loss: 0.6980\n",
            "Epoch: 003, Step: 187, Loss: 0.7209\n",
            "Epoch: 003, Step: 188, Loss: 0.6890\n",
            "Epoch: 003, Step: 189, Loss: 0.6667\n",
            "Epoch: 003, Step: 190, Loss: 0.6922\n",
            "Epoch: 003, Step: 191, Loss: 0.6625\n",
            "Epoch: 003, Step: 192, Loss: 0.6989\n",
            "Epoch: 003, Step: 193, Loss: 0.7254\n",
            "Epoch: 003, Step: 194, Loss: 0.6982\n",
            "Epoch: 003, Step: 195, Loss: 0.7295\n",
            "Epoch: 003, Step: 196, Loss: 0.6930\n",
            "Epoch: 003, Step: 197, Loss: 0.6843\n",
            "Epoch: 003, Step: 198, Loss: 0.7240\n",
            "Epoch: 003, Step: 199, Loss: 0.7261\n",
            "Epoch: 003, Step: 200, Loss: 0.6980\n",
            "Epoch: 003, Step: 201, Loss: 0.6924\n",
            "Epoch: 003, Step: 202, Loss: 0.7263\n",
            "Epoch: 003, Step: 203, Loss: 0.6927\n",
            "Epoch: 003, Step: 204, Loss: 0.6586\n",
            "Epoch: 003, Step: 205, Loss: 0.7228\n",
            "Epoch: 003, Step: 206, Loss: 0.6619\n",
            "Epoch: 003, Step: 207, Loss: 0.7318\n",
            "Epoch: 003, Step: 208, Loss: 0.7231\n",
            "Epoch: 003, Step: 209, Loss: 0.6944\n",
            "Epoch: 003, Step: 210, Loss: 0.6883\n",
            "Epoch: 003, Step: 211, Loss: 0.7202\n",
            "Epoch: 003, Step: 212, Loss: 0.7006\n",
            "Epoch: 003, Step: 213, Loss: 0.6641\n",
            "Epoch: 003, Step: 214, Loss: 0.6881\n",
            "Epoch: 003, Step: 215, Loss: 0.6965\n",
            "Epoch: 003, Step: 216, Loss: 0.6962\n",
            "Epoch: 003, Step: 217, Loss: 0.6948\n",
            "Epoch: 003, Step: 218, Loss: 0.7251\n",
            "Epoch: 003, Step: 219, Loss: 0.6952\n",
            "Epoch: 003, Step: 220, Loss: 0.6617\n",
            "Epoch: 003, Step: 221, Loss: 0.7229\n",
            "Epoch: 003, Step: 222, Loss: 0.6914\n",
            "Epoch: 003, Step: 223, Loss: 0.6617\n",
            "Epoch: 003, Step: 224, Loss: 0.6985\n",
            "Epoch: 003, Step: 225, Loss: 0.6678\n",
            "Epoch: 003, Step: 226, Loss: 0.6961\n",
            "Epoch: 003, Step: 227, Loss: 0.6996\n",
            "Epoch: 003, Step: 228, Loss: 0.7239\n",
            "Epoch: 003, Step: 229, Loss: 0.7294\n",
            "Epoch: 003, Step: 230, Loss: 0.6996\n",
            "Epoch: 003, Step: 231, Loss: 0.6938\n",
            "Epoch: 003, Step: 232, Loss: 0.6615\n",
            "Epoch: 003, Step: 233, Loss: 0.6892\n",
            "Epoch: 003, Step: 234, Loss: 0.6627\n",
            "Epoch: 003, Step: 235, Loss: 0.6653\n",
            "Epoch: 003, Step: 236, Loss: 0.6623\n",
            "Epoch: 003, Step: 237, Loss: 0.6925\n",
            "Epoch: 003, Step: 238, Loss: 0.6572\n",
            "Epoch: 003, Step: 239, Loss: 0.6670\n",
            "Epoch: 003, Step: 240, Loss: 0.6994\n",
            "Epoch: 003, Step: 241, Loss: 0.6917\n",
            "Epoch: 003, Step: 242, Loss: 0.7292\n",
            "Epoch: 003, Step: 243, Loss: 0.7290\n",
            "Epoch: 003, Step: 244, Loss: 0.6943\n",
            "Epoch: 003, Step: 245, Loss: 0.6908\n",
            "Epoch: 003, Step: 246, Loss: 0.6579\n",
            "Epoch: 003, Step: 247, Loss: 0.7305\n",
            "Epoch: 003, Step: 248, Loss: 0.7262\n",
            "Epoch: 003, Step: 249, Loss: 0.7225\n",
            "Epoch: 003, Step: 250, Loss: 0.6576\n",
            "Epoch: 003, Step: 251, Loss: 0.7241\n",
            "Epoch: 003, Step: 252, Loss: 0.6904\n",
            "Epoch: 003, Step: 253, Loss: 0.6864\n",
            "Epoch: 003, Step: 254, Loss: 0.6884\n",
            "Epoch: 003, Step: 255, Loss: 0.6972\n",
            "Epoch: 003, Step: 256, Loss: 0.7039\n",
            "Epoch: 003, Step: 257, Loss: 0.7341\n",
            "Epoch: 003, Step: 258, Loss: 0.6924\n",
            "Epoch: 003, Step: 259, Loss: 0.7332\n",
            "Epoch: 003, Step: 260, Loss: 0.7289\n",
            "Epoch: 003, Step: 261, Loss: 0.6972\n",
            "Epoch: 003, Step: 262, Loss: 0.6955\n",
            "Epoch: 003, Step: 263, Loss: 0.7327\n",
            "Epoch: 003, Step: 264, Loss: 0.6964\n",
            "Epoch: 003, Step: 265, Loss: 0.7285\n",
            "Epoch: 003, Step: 266, Loss: 0.7218\n",
            "Epoch: 003, Step: 267, Loss: 0.6926\n",
            "Epoch: 003, Step: 268, Loss: 0.6981\n",
            "Epoch: 003, Step: 269, Loss: 0.6583\n",
            "Epoch: 003, Step: 270, Loss: 0.7251\n",
            "Epoch: 003, Step: 271, Loss: 0.6918\n",
            "Epoch: 003, Step: 272, Loss: 0.7313\n",
            "Epoch: 003, Step: 273, Loss: 0.6902\n",
            "Epoch: 003, Step: 274, Loss: 0.6889\n",
            "Epoch: 003, Step: 275, Loss: 0.6608\n",
            "Epoch: 003, Step: 276, Loss: 0.6945\n",
            "Epoch: 003, Step: 277, Loss: 0.6892\n",
            "Epoch: 003, Step: 278, Loss: 0.7269\n",
            "Epoch: 003, Step: 279, Loss: 0.6546\n",
            "Epoch: 003, Step: 280, Loss: 0.7244\n",
            "Epoch: 003, Step: 281, Loss: 0.6621\n",
            "Epoch: 003, Step: 282, Loss: 0.6959\n",
            "Epoch: 003, Step: 283, Loss: 0.7309\n",
            "Epoch: 003, Step: 284, Loss: 0.7227\n",
            "Epoch: 003, Step: 285, Loss: 0.6924\n",
            "Epoch: 003, Step: 286, Loss: 0.7003\n",
            "Epoch: 003, Step: 287, Loss: 0.7252\n",
            "Epoch: 003, Step: 288, Loss: 0.6957\n",
            "Epoch: 003, Step: 289, Loss: 0.6980\n",
            "Epoch: 003, Step: 290, Loss: 0.6965\n",
            "Epoch: 003, Step: 291, Loss: 0.6951\n",
            "Epoch: 003, Step: 292, Loss: 0.7282\n",
            "Epoch: 003, Step: 293, Loss: 0.7232\n",
            "Epoch: 003, Step: 294, Loss: 0.7277\n",
            "Epoch: 003, Step: 295, Loss: 0.7221\n",
            "Epoch: 003, Step: 296, Loss: 0.7315\n",
            "Epoch: 003, Step: 297, Loss: 0.6954\n",
            "Epoch: 003, Step: 298, Loss: 0.6943\n",
            "Epoch: 003, Step: 299, Loss: 0.7003\n",
            "Epoch: 003, Step: 300, Loss: 0.6536\n",
            "Epoch: 003, Step: 301, Loss: 0.7298\n",
            "Epoch: 003, Step: 302, Loss: 0.6904\n",
            "Epoch: 003, Step: 303, Loss: 0.7291\n",
            "Epoch: 003, Step: 304, Loss: 0.6942\n",
            "Epoch: 003, Step: 305, Loss: 0.6962\n",
            "Epoch: 003, Step: 306, Loss: 0.6962\n",
            "Epoch: 003, Step: 307, Loss: 0.6931\n",
            "Epoch: 003, Step: 308, Loss: 0.6944\n",
            "Epoch: 003, Step: 309, Loss: 0.7255\n",
            "Epoch: 003, Step: 310, Loss: 0.6996\n",
            "Epoch: 003, Step: 311, Loss: 0.7229\n",
            "Epoch: 003, Step: 312, Loss: 0.6888\n",
            "Epoch: 003, Step: 313, Loss: 0.6979\n",
            "Epoch: 003, Step: 314, Loss: 0.6929\n",
            "Epoch: 003, Step: 315, Loss: 0.6581\n",
            "Epoch: 003, Step: 316, Loss: 0.7289\n",
            "Epoch: 003, Step: 317, Loss: 0.7343\n",
            "Epoch: 003, Step: 318, Loss: 0.7278\n",
            "Epoch: 003, Step: 319, Loss: 0.6981\n",
            "Epoch: 003, Step: 320, Loss: 0.7273\n",
            "Epoch: 003, Step: 321, Loss: 0.6937\n",
            "Epoch: 003, Step: 322, Loss: 0.6962\n",
            "Epoch: 003, Step: 323, Loss: 0.6899\n",
            "Epoch: 003, Step: 324, Loss: 0.6865\n",
            "Epoch: 003, Step: 325, Loss: 0.6964\n",
            "Epoch: 003, Step: 326, Loss: 0.7238\n",
            "Epoch: 003, Step: 327, Loss: 0.6556\n",
            "Epoch: 003, Step: 328, Loss: 0.6959\n",
            "Epoch: 003, Step: 329, Loss: 0.6890\n",
            "Epoch: 003, Step: 330, Loss: 0.7288\n",
            "Epoch: 003, Step: 331, Loss: 0.6955\n",
            "Epoch: 003, Step: 332, Loss: 0.6971\n",
            "Epoch: 003, Step: 333, Loss: 0.6555\n",
            "Epoch: 003, Step: 334, Loss: 0.6877\n",
            "Epoch: 003, Step: 335, Loss: 0.7251\n",
            "Epoch: 003, Step: 336, Loss: 0.6986\n",
            "Epoch: 003, Step: 337, Loss: 0.6639\n",
            "Epoch: 003, Step: 338, Loss: 0.7143\n",
            "Epoch: 003, Step: 339, Loss: 0.7269\n",
            "Epoch: 003, Step: 340, Loss: 0.6892\n",
            "Epoch: 003, Step: 341, Loss: 0.7291\n",
            "Epoch: 003, Step: 342, Loss: 0.6894\n",
            "Epoch: 003, Step: 343, Loss: 0.6927\n",
            "Epoch: 003, Step: 344, Loss: 0.6994\n",
            "Epoch: 003, Step: 345, Loss: 0.6587\n",
            "Epoch: 003, Step: 346, Loss: 0.7297\n",
            "Epoch: 003, Step: 347, Loss: 0.6550\n",
            "Epoch: 003, Step: 348, Loss: 0.6960\n",
            "Epoch: 003, Step: 349, Loss: 0.7284\n",
            "Epoch: 003, Step: 350, Loss: 0.7274\n",
            "Epoch: 003, Step: 351, Loss: 0.7188\n",
            "Epoch: 003, Step: 352, Loss: 0.6955\n",
            "Epoch: 003, Step: 353, Loss: 0.7242\n",
            "Epoch: 003, Step: 354, Loss: 0.7284\n",
            "Epoch: 003, Step: 355, Loss: 0.6967\n",
            "Epoch: 003, Step: 356, Loss: 0.7269\n",
            "Epoch: 003, Step: 357, Loss: 0.6993\n",
            "Epoch: 003, Step: 358, Loss: 0.6920\n",
            "Epoch: 003, Step: 359, Loss: 0.6880\n",
            "Epoch: 003, Step: 360, Loss: 0.7325\n",
            "Epoch: 003, Step: 361, Loss: 0.6893\n",
            "Epoch: 003, Step: 362, Loss: 0.6512\n",
            "Epoch: 003, Step: 363, Loss: 0.7249\n",
            "Epoch: 003, Step: 364, Loss: 0.6888\n",
            "Epoch: 003, Step: 365, Loss: 0.7271\n",
            "Epoch: 003, Step: 366, Loss: 0.7340\n",
            "Epoch: 003, Step: 367, Loss: 0.6660\n",
            "Epoch: 003, Step: 368, Loss: 0.6919\n",
            "Epoch: 003, Step: 369, Loss: 0.6525\n",
            "Epoch: 003, Step: 370, Loss: 0.6638\n",
            "Epoch: 003, Step: 371, Loss: 0.6934\n",
            "Epoch: 003, Step: 372, Loss: 0.6611\n",
            "Epoch: 003, Step: 373, Loss: 0.6882\n",
            "Epoch: 003, Step: 374, Loss: 0.6940\n",
            "Epoch: 003, Step: 375, Loss: 0.6966\n",
            "Epoch: 003, Step: 376, Loss: 0.6957\n",
            "Epoch: 003, Step: 377, Loss: 0.7127\n",
            "Epoch: 003, Step: 378, Loss: 0.7213\n",
            "Epoch: 003, Step: 379, Loss: 0.6919\n",
            "Epoch: 003, Step: 380, Loss: 0.7267\n",
            "Epoch: 003, Step: 381, Loss: 0.7289\n",
            "Epoch: 003, Step: 382, Loss: 0.6893\n",
            "Epoch: 003, Step: 383, Loss: 0.7235\n",
            "Epoch: 003, Step: 384, Loss: 0.7283\n",
            "Epoch: 003, Step: 385, Loss: 0.7334\n",
            "Epoch: 003, Step: 386, Loss: 0.6678\n",
            "Epoch: 003, Step: 387, Loss: 0.6980\n",
            "Epoch: 003, Step: 388, Loss: 0.6632\n",
            "Epoch: 003, Step: 389, Loss: 0.6871\n",
            "Epoch: 003, Step: 390, Loss: 0.7304\n",
            "Epoch: 003, Step: 391, Loss: 0.7280\n",
            "Epoch: 003, Step: 392, Loss: 0.6921\n",
            "Epoch: 003, Step: 393, Loss: 0.6590\n",
            "Epoch: 003, Step: 394, Loss: 0.6570\n",
            "Epoch: 003, Step: 395, Loss: 0.6984\n",
            "Epoch: 003, Step: 396, Loss: 0.6926\n",
            "Epoch: 003, Step: 397, Loss: 0.7306\n",
            "Epoch: 003, Step: 398, Loss: 0.6648\n",
            "Epoch: 003, Step: 399, Loss: 0.6955\n",
            "Epoch: 003, Step: 400, Loss: 0.6656\n",
            "Epoch: 003, Step: 401, Loss: 0.6893\n",
            "Epoch: 003, Step: 402, Loss: 0.6921\n",
            "Epoch: 003, Step: 403, Loss: 0.6626\n",
            "Epoch: 003, Step: 404, Loss: 0.7305\n",
            "Epoch: 003, Step: 405, Loss: 0.7231\n",
            "Epoch: 003, Step: 406, Loss: 0.6542\n",
            "Epoch: 003, Step: 407, Loss: 0.6924\n",
            "Epoch: 003, Step: 408, Loss: 0.6706\n",
            "Epoch: 003, Step: 409, Loss: 0.6571\n",
            "Epoch: 003, Step: 410, Loss: 0.6929\n",
            "Epoch: 003, Step: 411, Loss: 0.7171\n",
            "Epoch: 003, Step: 412, Loss: 0.7212\n",
            "Epoch: 003, Step: 000, Val Loss: 0.6609\n",
            "Epoch: 003, Step: 001, Val Loss: 0.6951\n",
            "Epoch: 003, Step: 002, Val Loss: 0.7270\n",
            "Epoch: 003, Step: 003, Val Loss: 0.6934\n",
            "Epoch: 003, Step: 004, Val Loss: 0.6941\n",
            "Epoch: 003, Step: 005, Val Loss: 0.7267\n",
            "Epoch: 003, Step: 006, Val Loss: 0.7268\n",
            "Epoch: 003, Step: 007, Val Loss: 0.6939\n",
            "Epoch: 003, Step: 008, Val Loss: 0.7242\n",
            "Epoch: 003, Step: 009, Val Loss: 0.7266\n",
            "Epoch: 003, Step: 010, Val Loss: 0.6936\n",
            "Epoch: 003, Step: 011, Val Loss: 0.7253\n",
            "Epoch: 003, Step: 012, Val Loss: 0.7267\n",
            "Epoch: 003, Step: 013, Val Loss: 0.7258\n",
            "Epoch: 003, Step: 014, Val Loss: 0.7261\n",
            "Epoch: 003, Step: 015, Val Loss: 0.7257\n",
            "Epoch: 003, Step: 016, Val Loss: 0.6619\n",
            "Epoch: 003, Step: 017, Val Loss: 0.6593\n",
            "Epoch: 003, Step: 018, Val Loss: 0.6619\n",
            "Epoch: 003, Step: 019, Val Loss: 0.7258\n",
            "Epoch: 003, Step: 020, Val Loss: 0.7265\n",
            "Epoch: 003, Step: 021, Val Loss: 0.6935\n",
            "Epoch: 003, Step: 022, Val Loss: 0.6930\n",
            "Epoch: 003, Step: 023, Val Loss: 0.6596\n",
            "Epoch: 003, Step: 024, Val Loss: 0.7257\n",
            "Epoch: 003, Step: 025, Val Loss: 0.6936\n",
            "Epoch: 003, Step: 026, Val Loss: 0.7259\n",
            "Epoch: 003, Step: 027, Val Loss: 0.6944\n",
            "Epoch: 003, Step: 028, Val Loss: 0.7254\n",
            "Epoch: 003, Step: 029, Val Loss: 0.6935\n",
            "Epoch: 003, Step: 030, Val Loss: 0.6935\n",
            "Epoch: 003, Step: 031, Val Loss: 0.7252\n",
            "Epoch: 003, Step: 032, Val Loss: 0.6931\n",
            "Epoch: 003, Step: 033, Val Loss: 0.6939\n",
            "Epoch: 003, Step: 034, Val Loss: 0.6953\n",
            "Epoch: 003, Step: 035, Val Loss: 0.6608\n",
            "Epoch: 003, Step: 036, Val Loss: 0.6938\n",
            "Epoch: 003, Step: 037, Val Loss: 0.6939\n",
            "Epoch: 003, Step: 038, Val Loss: 0.7264\n",
            "Epoch: 003, Step: 039, Val Loss: 0.7259\n",
            "Epoch: 003, Step: 040, Val Loss: 0.7257\n",
            "Epoch: 003, Step: 041, Val Loss: 0.6926\n",
            "Epoch: 003, Step: 042, Val Loss: 0.7262\n",
            "Epoch: 003, Step: 043, Val Loss: 0.6957\n",
            "Epoch: 003, Step: 044, Val Loss: 0.6932\n",
            "Epoch: 003, Step: 045, Val Loss: 0.7249\n",
            "Epoch: 003, Step: 046, Val Loss: 0.7271\n",
            "Epoch: 003, Step: 047, Val Loss: 0.6938\n",
            "Epoch: 003, Step: 048, Val Loss: 0.6924\n",
            "Epoch: 003, Step: 049, Val Loss: 0.6607\n",
            "Epoch: 003, Step: 050, Val Loss: 0.7262\n",
            "Epoch: 003, Step: 051, Val Loss: 0.6930\n",
            "Epoch: 003, Step: 052, Val Loss: 0.7270\n",
            "Epoch: 003, Step: 053, Val Loss: 0.6603\n",
            "Epoch: 003, Step: 054, Val Loss: 0.6610\n",
            "Epoch: 003, Step: 055, Val Loss: 0.6936\n",
            "Epoch: 003, Step: 056, Val Loss: 0.7259\n",
            "Epoch: 003, Step: 057, Val Loss: 0.7269\n",
            "Epoch: 003, Step: 058, Val Loss: 0.6606\n",
            "Epoch: 003, Step: 059, Val Loss: 0.6946\n",
            "Epoch: 003, Step: 060, Val Loss: 0.7270\n",
            "Epoch: 003, Step: 061, Val Loss: 0.7266\n",
            "Epoch: 003, Step: 062, Val Loss: 0.6942\n",
            "Epoch: 003, Step: 063, Val Loss: 0.6932\n",
            "Epoch: 003, Step: 064, Val Loss: 0.6607\n",
            "Epoch: 003, Step: 065, Val Loss: 0.7261\n",
            "Epoch: 003, Step: 066, Val Loss: 0.7259\n",
            "Epoch: 003, Step: 067, Val Loss: 0.6941\n",
            "Epoch: 003, Step: 068, Val Loss: 0.6938\n",
            "Epoch: 003, Step: 069, Val Loss: 0.6948\n",
            "Epoch: 003, Step: 070, Val Loss: 0.6929\n",
            "Epoch: 003, Step: 071, Val Loss: 0.6955\n",
            "Epoch: 003, Step: 072, Val Loss: 0.6606\n",
            "Epoch: 003, Step: 073, Val Loss: 0.7267\n",
            "Epoch: 003, Step: 074, Val Loss: 0.6916\n",
            "Epoch: 003, Step: 075, Val Loss: 0.6604\n",
            "Epoch: 003, Step: 076, Val Loss: 0.7272\n",
            "Epoch: 003, Step: 077, Val Loss: 0.7268\n",
            "Epoch: 003, Step: 078, Val Loss: 0.6927\n",
            "Epoch: 003, Step: 079, Val Loss: 0.6610\n",
            "Epoch: 003, Step: 080, Val Loss: 0.6933\n",
            "Epoch: 003, Step: 081, Val Loss: 0.6934\n",
            "Epoch: 003, Step: 082, Val Loss: 0.7261\n",
            "Epoch: 003, Step: 083, Val Loss: 0.6940\n",
            "Epoch: 003, Step: 084, Val Loss: 0.7253\n",
            "Epoch: 003, Step: 085, Val Loss: 0.6599\n",
            "Epoch: 003, Step: 086, Val Loss: 0.7266\n",
            "Epoch: 003, Step: 087, Val Loss: 0.7269\n",
            "Epoch: 003, Step: 088, Val Loss: 0.7261\n",
            "Epoch: 003, Step: 089, Val Loss: 0.7275\n",
            "Epoch: 003, Step: 090, Val Loss: 0.6586\n",
            "Epoch: 003, Step: 091, Val Loss: 0.7260\n",
            "Epoch: 003, Step: 092, Val Loss: 0.6612\n",
            "Epoch: 003, Step: 093, Val Loss: 0.6610\n",
            "Epoch: 003, Step: 094, Val Loss: 0.7262\n",
            "Epoch: 003, Step: 095, Val Loss: 0.6939\n",
            "Epoch: 003, Step: 096, Val Loss: 0.6946\n",
            "Epoch: 003, Step: 097, Val Loss: 0.6604\n",
            "Epoch: 003, Step: 098, Val Loss: 0.7258\n",
            "Epoch: 003, Step: 099, Val Loss: 0.6602\n",
            "Epoch: 003, Step: 100, Val Loss: 0.6930\n",
            "Epoch: 003, Step: 101, Val Loss: 0.6941\n",
            "Epoch: 003, Step: 102, Val Loss: 0.6620\n",
            "Epoch: 003, Step: 103, Val Loss: 0.7256\n",
            "Epoch: 003, Step: 104, Val Loss: 0.6933\n",
            "Epoch: 003, Step: 105, Val Loss: 0.6599\n",
            "Epoch: 003, Step: 106, Val Loss: 0.7272\n",
            "Epoch: 003, Step: 107, Val Loss: 0.6619\n",
            "Epoch: 003, Step: 108, Val Loss: 0.6934\n",
            "Epoch: 003, Step: 109, Val Loss: 0.6937\n",
            "Epoch: 003, Step: 110, Val Loss: 0.7263\n",
            "Epoch: 003, Step: 111, Val Loss: 0.6593\n",
            "Epoch: 003, Step: 112, Val Loss: 0.7261\n",
            "Epoch: 003, Step: 113, Val Loss: 0.6934\n",
            "Epoch: 003, Step: 114, Val Loss: 0.6929\n",
            "Epoch: 003, Step: 115, Val Loss: 0.6942\n",
            "Epoch: 003, Step: 116, Val Loss: 0.6938\n",
            "Epoch: 003, Step: 117, Val Loss: 0.7264\n",
            "Epoch: 004, Step: 000, Loss: 0.6897\n",
            "Epoch: 004, Step: 001, Loss: 0.6969\n",
            "Epoch: 004, Step: 002, Loss: 0.7024\n",
            "Epoch: 004, Step: 003, Loss: 0.6931\n",
            "Epoch: 004, Step: 004, Loss: 0.6605\n",
            "Epoch: 004, Step: 005, Loss: 0.6610\n",
            "Epoch: 004, Step: 006, Loss: 0.6585\n",
            "Epoch: 004, Step: 007, Loss: 0.7028\n",
            "Epoch: 004, Step: 008, Loss: 0.7255\n",
            "Epoch: 004, Step: 009, Loss: 0.6977\n",
            "Epoch: 004, Step: 010, Loss: 0.7327\n",
            "Epoch: 004, Step: 011, Loss: 0.7285\n",
            "Epoch: 004, Step: 012, Loss: 0.7273\n",
            "Epoch: 004, Step: 013, Loss: 0.6959\n",
            "Epoch: 004, Step: 014, Loss: 0.7339\n",
            "Epoch: 004, Step: 015, Loss: 0.6646\n",
            "Epoch: 004, Step: 016, Loss: 0.7313\n",
            "Epoch: 004, Step: 017, Loss: 0.7331\n",
            "Epoch: 004, Step: 018, Loss: 0.7268\n",
            "Epoch: 004, Step: 019, Loss: 0.6930\n",
            "Epoch: 004, Step: 020, Loss: 0.6684\n",
            "Epoch: 004, Step: 021, Loss: 0.6612\n",
            "Epoch: 004, Step: 022, Loss: 0.7238\n",
            "Epoch: 004, Step: 023, Loss: 0.6607\n",
            "Epoch: 004, Step: 024, Loss: 0.7227\n",
            "Epoch: 004, Step: 025, Loss: 0.6946\n",
            "Epoch: 004, Step: 026, Loss: 0.6580\n",
            "Epoch: 004, Step: 027, Loss: 0.7207\n",
            "Epoch: 004, Step: 028, Loss: 0.6936\n",
            "Epoch: 004, Step: 029, Loss: 0.7279\n",
            "Epoch: 004, Step: 030, Loss: 0.6934\n",
            "Epoch: 004, Step: 031, Loss: 0.7190\n",
            "Epoch: 004, Step: 032, Loss: 0.7244\n",
            "Epoch: 004, Step: 033, Loss: 0.7311\n",
            "Epoch: 004, Step: 034, Loss: 0.6646\n",
            "Epoch: 004, Step: 035, Loss: 0.6645\n",
            "Epoch: 004, Step: 036, Loss: 0.7266\n",
            "Epoch: 004, Step: 037, Loss: 0.7311\n",
            "Epoch: 004, Step: 038, Loss: 0.6907\n",
            "Epoch: 004, Step: 039, Loss: 0.6919\n",
            "Epoch: 004, Step: 040, Loss: 0.6881\n",
            "Epoch: 004, Step: 041, Loss: 0.6903\n",
            "Epoch: 004, Step: 042, Loss: 0.6923\n",
            "Epoch: 004, Step: 043, Loss: 0.6988\n",
            "Epoch: 004, Step: 044, Loss: 0.6925\n",
            "Epoch: 004, Step: 045, Loss: 0.6994\n",
            "Epoch: 004, Step: 046, Loss: 0.7308\n",
            "Epoch: 004, Step: 047, Loss: 0.6929\n",
            "Epoch: 004, Step: 048, Loss: 0.6855\n",
            "Epoch: 004, Step: 049, Loss: 0.6572\n",
            "Epoch: 004, Step: 050, Loss: 0.6937\n",
            "Epoch: 004, Step: 051, Loss: 0.7283\n",
            "Epoch: 004, Step: 052, Loss: 0.6913\n",
            "Epoch: 004, Step: 053, Loss: 0.6942\n",
            "Epoch: 004, Step: 054, Loss: 0.7216\n",
            "Epoch: 004, Step: 055, Loss: 0.6917\n",
            "Epoch: 004, Step: 056, Loss: 0.6945\n",
            "Epoch: 004, Step: 057, Loss: 0.6907\n",
            "Epoch: 004, Step: 058, Loss: 0.7261\n",
            "Epoch: 004, Step: 059, Loss: 0.6945\n",
            "Epoch: 004, Step: 060, Loss: 0.6894\n",
            "Epoch: 004, Step: 061, Loss: 0.6586\n",
            "Epoch: 004, Step: 062, Loss: 0.7172\n",
            "Epoch: 004, Step: 063, Loss: 0.6855\n",
            "Epoch: 004, Step: 064, Loss: 0.7276\n",
            "Epoch: 004, Step: 065, Loss: 0.7278\n",
            "Epoch: 004, Step: 066, Loss: 0.6607\n",
            "Epoch: 004, Step: 067, Loss: 0.7277\n",
            "Epoch: 004, Step: 068, Loss: 0.7201\n",
            "Epoch: 004, Step: 069, Loss: 0.7269\n",
            "Epoch: 004, Step: 070, Loss: 0.6572\n",
            "Epoch: 004, Step: 071, Loss: 0.6964\n",
            "Epoch: 004, Step: 072, Loss: 0.6945\n",
            "Epoch: 004, Step: 073, Loss: 0.6661\n",
            "Epoch: 004, Step: 074, Loss: 0.7316\n",
            "Epoch: 004, Step: 075, Loss: 0.7234\n",
            "Epoch: 004, Step: 076, Loss: 0.7332\n",
            "Epoch: 004, Step: 077, Loss: 0.6942\n",
            "Epoch: 004, Step: 078, Loss: 0.6920\n",
            "Epoch: 004, Step: 079, Loss: 0.6940\n",
            "Epoch: 004, Step: 080, Loss: 0.7191\n",
            "Epoch: 004, Step: 081, Loss: 0.7196\n",
            "Epoch: 004, Step: 082, Loss: 0.6963\n",
            "Epoch: 004, Step: 083, Loss: 0.6605\n",
            "Epoch: 004, Step: 084, Loss: 0.6934\n",
            "Epoch: 004, Step: 085, Loss: 0.6962\n",
            "Epoch: 004, Step: 086, Loss: 0.7242\n",
            "Epoch: 004, Step: 087, Loss: 0.7203\n",
            "Epoch: 004, Step: 088, Loss: 0.6874\n",
            "Epoch: 004, Step: 089, Loss: 0.7237\n",
            "Epoch: 004, Step: 090, Loss: 0.6940\n",
            "Epoch: 004, Step: 091, Loss: 0.7213\n",
            "Epoch: 004, Step: 092, Loss: 0.6916\n",
            "Epoch: 004, Step: 093, Loss: 0.7236\n",
            "Epoch: 004, Step: 094, Loss: 0.7231\n",
            "Epoch: 004, Step: 095, Loss: 0.6966\n",
            "Epoch: 004, Step: 096, Loss: 0.6582\n",
            "Epoch: 004, Step: 097, Loss: 0.6963\n",
            "Epoch: 004, Step: 098, Loss: 0.7248\n",
            "Epoch: 004, Step: 099, Loss: 0.6890\n",
            "Epoch: 004, Step: 100, Loss: 0.7277\n",
            "Epoch: 004, Step: 101, Loss: 0.6952\n",
            "Epoch: 004, Step: 102, Loss: 0.7308\n",
            "Epoch: 004, Step: 103, Loss: 0.7241\n",
            "Epoch: 004, Step: 104, Loss: 0.6869\n",
            "Epoch: 004, Step: 105, Loss: 0.7017\n",
            "Epoch: 004, Step: 106, Loss: 0.7289\n",
            "Epoch: 004, Step: 107, Loss: 0.7317\n",
            "Epoch: 004, Step: 108, Loss: 0.6935\n",
            "Epoch: 004, Step: 109, Loss: 0.7229\n",
            "Epoch: 004, Step: 110, Loss: 0.7227\n",
            "Epoch: 004, Step: 111, Loss: 0.7279\n",
            "Epoch: 004, Step: 112, Loss: 0.6562\n",
            "Epoch: 004, Step: 113, Loss: 0.6973\n",
            "Epoch: 004, Step: 114, Loss: 0.6872\n",
            "Epoch: 004, Step: 115, Loss: 0.6978\n",
            "Epoch: 004, Step: 116, Loss: 0.6956\n",
            "Epoch: 004, Step: 117, Loss: 0.7240\n",
            "Epoch: 004, Step: 118, Loss: 0.6606\n",
            "Epoch: 004, Step: 119, Loss: 0.6913\n",
            "Epoch: 004, Step: 120, Loss: 0.7007\n",
            "Epoch: 004, Step: 121, Loss: 0.7268\n",
            "Epoch: 004, Step: 122, Loss: 0.7257\n",
            "Epoch: 004, Step: 123, Loss: 0.6879\n",
            "Epoch: 004, Step: 124, Loss: 0.6656\n",
            "Epoch: 004, Step: 125, Loss: 0.6975\n",
            "Epoch: 004, Step: 126, Loss: 0.7223\n",
            "Epoch: 004, Step: 127, Loss: 0.6931\n",
            "Epoch: 004, Step: 128, Loss: 0.6560\n",
            "Epoch: 004, Step: 129, Loss: 0.6894\n",
            "Epoch: 004, Step: 130, Loss: 0.6588\n",
            "Epoch: 004, Step: 131, Loss: 0.6940\n",
            "Epoch: 004, Step: 132, Loss: 0.6924\n",
            "Epoch: 004, Step: 133, Loss: 0.6612\n",
            "Epoch: 004, Step: 134, Loss: 0.7202\n",
            "Epoch: 004, Step: 135, Loss: 0.6663\n",
            "Epoch: 004, Step: 136, Loss: 0.7224\n",
            "Epoch: 004, Step: 137, Loss: 0.6925\n",
            "Epoch: 004, Step: 138, Loss: 0.6615\n",
            "Epoch: 004, Step: 139, Loss: 0.6980\n",
            "Epoch: 004, Step: 140, Loss: 0.6593\n",
            "Epoch: 004, Step: 141, Loss: 0.7260\n",
            "Epoch: 004, Step: 142, Loss: 0.6930\n",
            "Epoch: 004, Step: 143, Loss: 0.7231\n",
            "Epoch: 004, Step: 144, Loss: 0.6947\n",
            "Epoch: 004, Step: 145, Loss: 0.6635\n",
            "Epoch: 004, Step: 146, Loss: 0.6630\n",
            "Epoch: 004, Step: 147, Loss: 0.6982\n",
            "Epoch: 004, Step: 148, Loss: 0.6650\n",
            "Epoch: 004, Step: 149, Loss: 0.7365\n",
            "Epoch: 004, Step: 150, Loss: 0.6887\n",
            "Epoch: 004, Step: 151, Loss: 0.7220\n",
            "Epoch: 004, Step: 152, Loss: 0.6583\n",
            "Epoch: 004, Step: 153, Loss: 0.6894\n",
            "Epoch: 004, Step: 154, Loss: 0.6671\n",
            "Epoch: 004, Step: 155, Loss: 0.6925\n",
            "Epoch: 004, Step: 156, Loss: 0.6567\n",
            "Epoch: 004, Step: 157, Loss: 0.7343\n",
            "Epoch: 004, Step: 158, Loss: 0.7281\n",
            "Epoch: 004, Step: 159, Loss: 0.7231\n",
            "Epoch: 004, Step: 160, Loss: 0.6994\n",
            "Epoch: 004, Step: 161, Loss: 0.7261\n",
            "Epoch: 004, Step: 162, Loss: 0.6926\n",
            "Epoch: 004, Step: 163, Loss: 0.7242\n",
            "Epoch: 004, Step: 164, Loss: 0.6945\n",
            "Epoch: 004, Step: 165, Loss: 0.6861\n",
            "Epoch: 004, Step: 166, Loss: 0.7249\n",
            "Epoch: 004, Step: 167, Loss: 0.7282\n",
            "Epoch: 004, Step: 168, Loss: 0.7213\n",
            "Epoch: 004, Step: 169, Loss: 0.6558\n",
            "Epoch: 004, Step: 170, Loss: 0.6634\n",
            "Epoch: 004, Step: 171, Loss: 0.7320\n",
            "Epoch: 004, Step: 172, Loss: 0.7201\n",
            "Epoch: 004, Step: 173, Loss: 0.7232\n",
            "Epoch: 004, Step: 174, Loss: 0.7260\n",
            "Epoch: 004, Step: 175, Loss: 0.7224\n",
            "Epoch: 004, Step: 176, Loss: 0.7271\n",
            "Epoch: 004, Step: 177, Loss: 0.7221\n",
            "Epoch: 004, Step: 178, Loss: 0.7240\n",
            "Epoch: 004, Step: 179, Loss: 0.6899\n",
            "Epoch: 004, Step: 180, Loss: 0.7329\n",
            "Epoch: 004, Step: 181, Loss: 0.7267\n",
            "Epoch: 004, Step: 182, Loss: 0.6585\n",
            "Epoch: 004, Step: 183, Loss: 0.7291\n",
            "Epoch: 004, Step: 184, Loss: 0.6577\n",
            "Epoch: 004, Step: 185, Loss: 0.6909\n",
            "Epoch: 004, Step: 186, Loss: 0.6916\n",
            "Epoch: 004, Step: 187, Loss: 0.6655\n",
            "Epoch: 004, Step: 188, Loss: 0.6625\n",
            "Epoch: 004, Step: 189, Loss: 0.7043\n",
            "Epoch: 004, Step: 190, Loss: 0.7263\n",
            "Epoch: 004, Step: 191, Loss: 0.6929\n",
            "Epoch: 004, Step: 192, Loss: 0.7276\n",
            "Epoch: 004, Step: 193, Loss: 0.6990\n",
            "Epoch: 004, Step: 194, Loss: 0.7276\n",
            "Epoch: 004, Step: 195, Loss: 0.7000\n",
            "Epoch: 004, Step: 196, Loss: 0.6872\n",
            "Epoch: 004, Step: 197, Loss: 0.6866\n",
            "Epoch: 004, Step: 198, Loss: 0.6954\n",
            "Epoch: 004, Step: 199, Loss: 0.7330\n",
            "Epoch: 004, Step: 200, Loss: 0.6566\n",
            "Epoch: 004, Step: 201, Loss: 0.6956\n",
            "Epoch: 004, Step: 202, Loss: 0.7298\n",
            "Epoch: 004, Step: 203, Loss: 0.7295\n",
            "Epoch: 004, Step: 204, Loss: 0.7313\n",
            "Epoch: 004, Step: 205, Loss: 0.7268\n",
            "Epoch: 004, Step: 206, Loss: 0.6945\n",
            "Epoch: 004, Step: 207, Loss: 0.6991\n",
            "Epoch: 004, Step: 208, Loss: 0.6968\n",
            "Epoch: 004, Step: 209, Loss: 0.6604\n",
            "Epoch: 004, Step: 210, Loss: 0.6968\n",
            "Epoch: 004, Step: 211, Loss: 0.6615\n",
            "Epoch: 004, Step: 212, Loss: 0.6905\n",
            "Epoch: 004, Step: 213, Loss: 0.6884\n",
            "Epoch: 004, Step: 214, Loss: 0.6625\n",
            "Epoch: 004, Step: 215, Loss: 0.7242\n",
            "Epoch: 004, Step: 216, Loss: 0.7303\n",
            "Epoch: 004, Step: 217, Loss: 0.7284\n",
            "Epoch: 004, Step: 218, Loss: 0.6971\n",
            "Epoch: 004, Step: 219, Loss: 0.6921\n",
            "Epoch: 004, Step: 220, Loss: 0.7017\n",
            "Epoch: 004, Step: 221, Loss: 0.6689\n",
            "Epoch: 004, Step: 222, Loss: 0.6973\n",
            "Epoch: 004, Step: 223, Loss: 0.6614\n",
            "Epoch: 004, Step: 224, Loss: 0.6923\n",
            "Epoch: 004, Step: 225, Loss: 0.6624\n",
            "Epoch: 004, Step: 226, Loss: 0.6655\n",
            "Epoch: 004, Step: 227, Loss: 0.6645\n",
            "Epoch: 004, Step: 228, Loss: 0.6929\n",
            "Epoch: 004, Step: 229, Loss: 0.6919\n",
            "Epoch: 004, Step: 230, Loss: 0.7237\n",
            "Epoch: 004, Step: 231, Loss: 0.6954\n",
            "Epoch: 004, Step: 232, Loss: 0.6964\n",
            "Epoch: 004, Step: 233, Loss: 0.6587\n",
            "Epoch: 004, Step: 234, Loss: 0.6893\n",
            "Epoch: 004, Step: 235, Loss: 0.6637\n",
            "Epoch: 004, Step: 236, Loss: 0.7347\n",
            "Epoch: 004, Step: 237, Loss: 0.6970\n",
            "Epoch: 004, Step: 238, Loss: 0.6959\n",
            "Epoch: 004, Step: 239, Loss: 0.7267\n",
            "Epoch: 004, Step: 240, Loss: 0.6913\n",
            "Epoch: 004, Step: 241, Loss: 0.7269\n",
            "Epoch: 004, Step: 242, Loss: 0.7286\n",
            "Epoch: 004, Step: 243, Loss: 0.7225\n",
            "Epoch: 004, Step: 244, Loss: 0.6966\n",
            "Epoch: 004, Step: 245, Loss: 0.7287\n",
            "Epoch: 004, Step: 246, Loss: 0.6990\n",
            "Epoch: 004, Step: 247, Loss: 0.6987\n",
            "Epoch: 004, Step: 248, Loss: 0.6620\n",
            "Epoch: 004, Step: 249, Loss: 0.6595\n",
            "Epoch: 004, Step: 250, Loss: 0.7261\n",
            "Epoch: 004, Step: 251, Loss: 0.6986\n",
            "Epoch: 004, Step: 252, Loss: 0.6912\n",
            "Epoch: 004, Step: 253, Loss: 0.7341\n",
            "Epoch: 004, Step: 254, Loss: 0.6989\n",
            "Epoch: 004, Step: 255, Loss: 0.6961\n",
            "Epoch: 004, Step: 256, Loss: 0.6891\n",
            "Epoch: 004, Step: 257, Loss: 0.7246\n",
            "Epoch: 004, Step: 258, Loss: 0.7001\n",
            "Epoch: 004, Step: 259, Loss: 0.6875\n",
            "Epoch: 004, Step: 260, Loss: 0.6634\n",
            "Epoch: 004, Step: 261, Loss: 0.7249\n",
            "Epoch: 004, Step: 262, Loss: 0.6980\n",
            "Epoch: 004, Step: 263, Loss: 0.7262\n",
            "Epoch: 004, Step: 264, Loss: 0.6684\n",
            "Epoch: 004, Step: 265, Loss: 0.6954\n",
            "Epoch: 004, Step: 266, Loss: 0.6923\n",
            "Epoch: 004, Step: 267, Loss: 0.7252\n",
            "Epoch: 004, Step: 268, Loss: 0.6683\n",
            "Epoch: 004, Step: 269, Loss: 0.7283\n",
            "Epoch: 004, Step: 270, Loss: 0.7264\n",
            "Epoch: 004, Step: 271, Loss: 0.7258\n",
            "Epoch: 004, Step: 272, Loss: 0.7277\n",
            "Epoch: 004, Step: 273, Loss: 0.6909\n",
            "Epoch: 004, Step: 274, Loss: 0.6913\n",
            "Epoch: 004, Step: 275, Loss: 0.7219\n",
            "Epoch: 004, Step: 276, Loss: 0.6939\n",
            "Epoch: 004, Step: 277, Loss: 0.7276\n",
            "Epoch: 004, Step: 278, Loss: 0.7270\n",
            "Epoch: 004, Step: 279, Loss: 0.6864\n",
            "Epoch: 004, Step: 280, Loss: 0.6619\n",
            "Epoch: 004, Step: 281, Loss: 0.6903\n",
            "Epoch: 004, Step: 282, Loss: 0.6611\n",
            "Epoch: 004, Step: 283, Loss: 0.6650\n",
            "Epoch: 004, Step: 284, Loss: 0.7288\n",
            "Epoch: 004, Step: 285, Loss: 0.6926\n",
            "Epoch: 004, Step: 286, Loss: 0.6946\n",
            "Epoch: 004, Step: 287, Loss: 0.6949\n",
            "Epoch: 004, Step: 288, Loss: 0.7256\n",
            "Epoch: 004, Step: 289, Loss: 0.7246\n",
            "Epoch: 004, Step: 290, Loss: 0.7262\n",
            "Epoch: 004, Step: 291, Loss: 0.6954\n",
            "Epoch: 004, Step: 292, Loss: 0.6916\n",
            "Epoch: 004, Step: 293, Loss: 0.7274\n",
            "Epoch: 004, Step: 294, Loss: 0.6932\n",
            "Epoch: 004, Step: 295, Loss: 0.6643\n",
            "Epoch: 004, Step: 296, Loss: 0.6597\n",
            "Epoch: 004, Step: 297, Loss: 0.7249\n",
            "Epoch: 004, Step: 298, Loss: 0.7293\n",
            "Epoch: 004, Step: 299, Loss: 0.6900\n",
            "Epoch: 004, Step: 300, Loss: 0.7270\n",
            "Epoch: 004, Step: 301, Loss: 0.7220\n",
            "Epoch: 004, Step: 302, Loss: 0.7217\n",
            "Epoch: 004, Step: 303, Loss: 0.7259\n",
            "Epoch: 004, Step: 304, Loss: 0.6588\n",
            "Epoch: 004, Step: 305, Loss: 0.6650\n",
            "Epoch: 004, Step: 306, Loss: 0.6935\n",
            "Epoch: 004, Step: 307, Loss: 0.6588\n",
            "Epoch: 004, Step: 308, Loss: 0.7231\n",
            "Epoch: 004, Step: 309, Loss: 0.6590\n",
            "Epoch: 004, Step: 310, Loss: 0.7007\n",
            "Epoch: 004, Step: 311, Loss: 0.6990\n",
            "Epoch: 004, Step: 312, Loss: 0.6627\n",
            "Epoch: 004, Step: 313, Loss: 0.7278\n",
            "Epoch: 004, Step: 314, Loss: 0.6894\n",
            "Epoch: 004, Step: 315, Loss: 0.6907\n",
            "Epoch: 004, Step: 316, Loss: 0.6923\n",
            "Epoch: 004, Step: 317, Loss: 0.6913\n",
            "Epoch: 004, Step: 318, Loss: 0.6890\n",
            "Epoch: 004, Step: 319, Loss: 0.6922\n",
            "Epoch: 004, Step: 320, Loss: 0.6915\n",
            "Epoch: 004, Step: 321, Loss: 0.6987\n",
            "Epoch: 004, Step: 322, Loss: 0.7301\n",
            "Epoch: 004, Step: 323, Loss: 0.6873\n",
            "Epoch: 004, Step: 324, Loss: 0.7248\n",
            "Epoch: 004, Step: 325, Loss: 0.7290\n",
            "Epoch: 004, Step: 326, Loss: 0.6917\n",
            "Epoch: 004, Step: 327, Loss: 0.6917\n",
            "Epoch: 004, Step: 328, Loss: 0.6980\n",
            "Epoch: 004, Step: 329, Loss: 0.6934\n",
            "Epoch: 004, Step: 330, Loss: 0.6909\n",
            "Epoch: 004, Step: 331, Loss: 0.6621\n",
            "Epoch: 004, Step: 332, Loss: 0.6634\n",
            "Epoch: 004, Step: 333, Loss: 0.6976\n",
            "Epoch: 004, Step: 334, Loss: 0.6975\n",
            "Epoch: 004, Step: 335, Loss: 0.6975\n",
            "Epoch: 004, Step: 336, Loss: 0.7327\n",
            "Epoch: 004, Step: 337, Loss: 0.7302\n",
            "Epoch: 004, Step: 338, Loss: 0.7195\n",
            "Epoch: 004, Step: 339, Loss: 0.6880\n",
            "Epoch: 004, Step: 340, Loss: 0.7157\n",
            "Epoch: 004, Step: 341, Loss: 0.7274\n",
            "Epoch: 004, Step: 342, Loss: 0.7302\n",
            "Epoch: 004, Step: 343, Loss: 0.6686\n",
            "Epoch: 004, Step: 344, Loss: 0.6946\n",
            "Epoch: 004, Step: 345, Loss: 0.6976\n",
            "Epoch: 004, Step: 346, Loss: 0.7020\n",
            "Epoch: 004, Step: 347, Loss: 0.6931\n",
            "Epoch: 004, Step: 348, Loss: 0.6642\n",
            "Epoch: 004, Step: 349, Loss: 0.6940\n",
            "Epoch: 004, Step: 350, Loss: 0.6810\n",
            "Epoch: 004, Step: 351, Loss: 0.7206\n",
            "Epoch: 004, Step: 352, Loss: 0.6855\n",
            "Epoch: 004, Step: 353, Loss: 0.6935\n",
            "Epoch: 004, Step: 354, Loss: 0.7195\n",
            "Epoch: 004, Step: 355, Loss: 0.7290\n",
            "Epoch: 004, Step: 356, Loss: 0.7273\n",
            "Epoch: 004, Step: 357, Loss: 0.6888\n",
            "Epoch: 004, Step: 358, Loss: 0.6979\n",
            "Epoch: 004, Step: 359, Loss: 0.6654\n",
            "Epoch: 004, Step: 360, Loss: 0.6996\n",
            "Epoch: 004, Step: 361, Loss: 0.6969\n",
            "Epoch: 004, Step: 362, Loss: 0.6947\n",
            "Epoch: 004, Step: 363, Loss: 0.6960\n",
            "Epoch: 004, Step: 364, Loss: 0.6674\n",
            "Epoch: 004, Step: 365, Loss: 0.6905\n",
            "Epoch: 004, Step: 366, Loss: 0.7292\n",
            "Epoch: 004, Step: 367, Loss: 0.6839\n",
            "Epoch: 004, Step: 368, Loss: 0.7335\n",
            "Epoch: 004, Step: 369, Loss: 0.6614\n",
            "Epoch: 004, Step: 370, Loss: 0.6946\n",
            "Epoch: 004, Step: 371, Loss: 0.7203\n",
            "Epoch: 004, Step: 372, Loss: 0.7298\n",
            "Epoch: 004, Step: 373, Loss: 0.7301\n",
            "Epoch: 004, Step: 374, Loss: 0.6988\n",
            "Epoch: 004, Step: 375, Loss: 0.7187\n",
            "Epoch: 004, Step: 376, Loss: 0.6959\n",
            "Epoch: 004, Step: 377, Loss: 0.7246\n",
            "Epoch: 004, Step: 378, Loss: 0.6945\n",
            "Epoch: 004, Step: 379, Loss: 0.6624\n",
            "Epoch: 004, Step: 380, Loss: 0.6527\n",
            "Epoch: 004, Step: 381, Loss: 0.6995\n",
            "Epoch: 004, Step: 382, Loss: 0.7216\n",
            "Epoch: 004, Step: 383, Loss: 0.6997\n",
            "Epoch: 004, Step: 384, Loss: 0.7265\n",
            "Epoch: 004, Step: 385, Loss: 0.6632\n",
            "Epoch: 004, Step: 386, Loss: 0.7249\n",
            "Epoch: 004, Step: 387, Loss: 0.6890\n",
            "Epoch: 004, Step: 388, Loss: 0.7283\n",
            "Epoch: 004, Step: 389, Loss: 0.7253\n",
            "Epoch: 004, Step: 390, Loss: 0.6525\n",
            "Epoch: 004, Step: 391, Loss: 0.6879\n",
            "Epoch: 004, Step: 392, Loss: 0.6573\n",
            "Epoch: 004, Step: 393, Loss: 0.7065\n",
            "Epoch: 004, Step: 394, Loss: 0.7262\n",
            "Epoch: 004, Step: 395, Loss: 0.7268\n",
            "Epoch: 004, Step: 396, Loss: 0.6940\n",
            "Epoch: 004, Step: 397, Loss: 0.6532\n",
            "Epoch: 004, Step: 398, Loss: 0.6916\n",
            "Epoch: 004, Step: 399, Loss: 0.7254\n",
            "Epoch: 004, Step: 400, Loss: 0.6937\n",
            "Epoch: 004, Step: 401, Loss: 0.6892\n",
            "Epoch: 004, Step: 402, Loss: 0.6930\n",
            "Epoch: 004, Step: 403, Loss: 0.6863\n",
            "Epoch: 004, Step: 404, Loss: 0.6959\n",
            "Epoch: 004, Step: 405, Loss: 0.6996\n",
            "Epoch: 004, Step: 406, Loss: 0.6859\n",
            "Epoch: 004, Step: 407, Loss: 0.6569\n",
            "Epoch: 004, Step: 408, Loss: 0.7032\n",
            "Epoch: 004, Step: 409, Loss: 0.7287\n",
            "Epoch: 004, Step: 410, Loss: 0.7253\n",
            "Epoch: 004, Step: 411, Loss: 0.6572\n",
            "Epoch: 004, Step: 412, Loss: 0.7299\n",
            "Epoch: 004, Step: 000, Val Loss: 0.6613\n",
            "Epoch: 004, Step: 001, Val Loss: 0.7259\n",
            "Epoch: 004, Step: 002, Val Loss: 0.6938\n",
            "Epoch: 004, Step: 003, Val Loss: 0.6935\n",
            "Epoch: 004, Step: 004, Val Loss: 0.6936\n",
            "Epoch: 004, Step: 005, Val Loss: 0.7259\n",
            "Epoch: 004, Step: 006, Val Loss: 0.7260\n",
            "Epoch: 004, Step: 007, Val Loss: 0.6935\n",
            "Epoch: 004, Step: 008, Val Loss: 0.6935\n",
            "Epoch: 004, Step: 009, Val Loss: 0.7268\n",
            "Epoch: 004, Step: 010, Val Loss: 0.7259\n",
            "Epoch: 004, Step: 011, Val Loss: 0.6622\n",
            "Epoch: 004, Step: 012, Val Loss: 0.6937\n",
            "Epoch: 004, Step: 013, Val Loss: 0.6936\n",
            "Epoch: 004, Step: 014, Val Loss: 0.6935\n",
            "Epoch: 004, Step: 015, Val Loss: 0.6613\n",
            "Epoch: 004, Step: 016, Val Loss: 0.7248\n",
            "Epoch: 004, Step: 017, Val Loss: 0.6937\n",
            "Epoch: 004, Step: 018, Val Loss: 0.6593\n",
            "Epoch: 004, Step: 019, Val Loss: 0.7268\n",
            "Epoch: 004, Step: 020, Val Loss: 0.6938\n",
            "Epoch: 004, Step: 021, Val Loss: 0.7258\n",
            "Epoch: 004, Step: 022, Val Loss: 0.6935\n",
            "Epoch: 004, Step: 023, Val Loss: 0.7264\n",
            "Epoch: 004, Step: 024, Val Loss: 0.7255\n",
            "Epoch: 004, Step: 025, Val Loss: 0.6936\n",
            "Epoch: 004, Step: 026, Val Loss: 0.6610\n",
            "Epoch: 004, Step: 027, Val Loss: 0.7263\n",
            "Epoch: 004, Step: 028, Val Loss: 0.6611\n",
            "Epoch: 004, Step: 029, Val Loss: 0.6938\n",
            "Epoch: 004, Step: 030, Val Loss: 0.6934\n",
            "Epoch: 004, Step: 031, Val Loss: 0.6936\n",
            "Epoch: 004, Step: 032, Val Loss: 0.6936\n",
            "Epoch: 004, Step: 033, Val Loss: 0.7262\n",
            "Epoch: 004, Step: 034, Val Loss: 0.7251\n",
            "Epoch: 004, Step: 035, Val Loss: 0.7247\n",
            "Epoch: 004, Step: 036, Val Loss: 0.6941\n",
            "Epoch: 004, Step: 037, Val Loss: 0.6924\n",
            "Epoch: 004, Step: 038, Val Loss: 0.6934\n",
            "Epoch: 004, Step: 039, Val Loss: 0.6928\n",
            "Epoch: 004, Step: 040, Val Loss: 0.6596\n",
            "Epoch: 004, Step: 041, Val Loss: 0.7256\n",
            "Epoch: 004, Step: 042, Val Loss: 0.6622\n",
            "Epoch: 004, Step: 043, Val Loss: 0.6935\n",
            "Epoch: 004, Step: 044, Val Loss: 0.6614\n",
            "Epoch: 004, Step: 045, Val Loss: 0.7265\n",
            "Epoch: 004, Step: 046, Val Loss: 0.6932\n",
            "Epoch: 004, Step: 047, Val Loss: 0.6930\n",
            "Epoch: 004, Step: 048, Val Loss: 0.7257\n",
            "Epoch: 004, Step: 049, Val Loss: 0.6944\n",
            "Epoch: 004, Step: 050, Val Loss: 0.7263\n",
            "Epoch: 004, Step: 051, Val Loss: 0.6602\n",
            "Epoch: 004, Step: 052, Val Loss: 0.7251\n",
            "Epoch: 004, Step: 053, Val Loss: 0.6941\n",
            "Epoch: 004, Step: 054, Val Loss: 0.6939\n",
            "Epoch: 004, Step: 055, Val Loss: 0.6936\n",
            "Epoch: 004, Step: 056, Val Loss: 0.6936\n",
            "Epoch: 004, Step: 057, Val Loss: 0.6945\n",
            "Epoch: 004, Step: 058, Val Loss: 0.6613\n",
            "Epoch: 004, Step: 059, Val Loss: 0.6931\n",
            "Epoch: 004, Step: 060, Val Loss: 0.7265\n",
            "Epoch: 004, Step: 061, Val Loss: 0.6906\n",
            "Epoch: 004, Step: 062, Val Loss: 0.6939\n",
            "Epoch: 004, Step: 063, Val Loss: 0.6610\n",
            "Epoch: 004, Step: 064, Val Loss: 0.6938\n",
            "Epoch: 004, Step: 065, Val Loss: 0.6926\n",
            "Epoch: 004, Step: 066, Val Loss: 0.6947\n",
            "Epoch: 004, Step: 067, Val Loss: 0.7258\n",
            "Epoch: 004, Step: 068, Val Loss: 0.6940\n",
            "Epoch: 004, Step: 069, Val Loss: 0.6937\n",
            "Epoch: 004, Step: 070, Val Loss: 0.7265\n",
            "Epoch: 004, Step: 071, Val Loss: 0.6920\n",
            "Epoch: 004, Step: 072, Val Loss: 0.7265\n",
            "Epoch: 004, Step: 073, Val Loss: 0.6611\n",
            "Epoch: 004, Step: 074, Val Loss: 0.6946\n",
            "Epoch: 004, Step: 075, Val Loss: 0.6928\n",
            "Epoch: 004, Step: 076, Val Loss: 0.6606\n",
            "Epoch: 004, Step: 077, Val Loss: 0.7269\n",
            "Epoch: 004, Step: 078, Val Loss: 0.7260\n",
            "Epoch: 004, Step: 079, Val Loss: 0.6929\n",
            "Epoch: 004, Step: 080, Val Loss: 0.6609\n",
            "Epoch: 004, Step: 081, Val Loss: 0.6942\n",
            "Epoch: 004, Step: 082, Val Loss: 0.6945\n",
            "Epoch: 004, Step: 083, Val Loss: 0.6930\n",
            "Epoch: 004, Step: 084, Val Loss: 0.6607\n",
            "Epoch: 004, Step: 085, Val Loss: 0.7264\n",
            "Epoch: 004, Step: 086, Val Loss: 0.6936\n",
            "Epoch: 004, Step: 087, Val Loss: 0.7274\n",
            "Epoch: 004, Step: 088, Val Loss: 0.7258\n",
            "Epoch: 004, Step: 089, Val Loss: 0.7261\n",
            "Epoch: 004, Step: 090, Val Loss: 0.7264\n",
            "Epoch: 004, Step: 091, Val Loss: 0.7259\n",
            "Epoch: 004, Step: 092, Val Loss: 0.6952\n",
            "Epoch: 004, Step: 093, Val Loss: 0.7276\n",
            "Epoch: 004, Step: 094, Val Loss: 0.6923\n",
            "Epoch: 004, Step: 095, Val Loss: 0.7249\n",
            "Epoch: 004, Step: 096, Val Loss: 0.7260\n",
            "Epoch: 004, Step: 097, Val Loss: 0.6618\n",
            "Epoch: 004, Step: 098, Val Loss: 0.6944\n",
            "Epoch: 004, Step: 099, Val Loss: 0.6944\n",
            "Epoch: 004, Step: 100, Val Loss: 0.6947\n",
            "Epoch: 004, Step: 101, Val Loss: 0.7261\n",
            "Epoch: 004, Step: 102, Val Loss: 0.6936\n",
            "Epoch: 004, Step: 103, Val Loss: 0.6623\n",
            "Epoch: 004, Step: 104, Val Loss: 0.6945\n",
            "Epoch: 004, Step: 105, Val Loss: 0.7274\n",
            "Epoch: 004, Step: 106, Val Loss: 0.7267\n",
            "Epoch: 004, Step: 107, Val Loss: 0.6934\n",
            "Epoch: 004, Step: 108, Val Loss: 0.7267\n",
            "Epoch: 004, Step: 109, Val Loss: 0.6917\n",
            "Epoch: 004, Step: 110, Val Loss: 0.7273\n",
            "Epoch: 004, Step: 111, Val Loss: 0.7250\n",
            "Epoch: 004, Step: 112, Val Loss: 0.6610\n",
            "Epoch: 004, Step: 113, Val Loss: 0.6614\n",
            "Epoch: 004, Step: 114, Val Loss: 0.7264\n",
            "Epoch: 004, Step: 115, Val Loss: 0.7254\n",
            "Epoch: 004, Step: 116, Val Loss: 0.7265\n",
            "Epoch: 004, Step: 117, Val Loss: 0.6931\n",
            "Epoch: 005, Step: 000, Loss: 0.6608\n",
            "Epoch: 005, Step: 001, Loss: 0.7315\n",
            "Epoch: 005, Step: 002, Loss: 0.7293\n",
            "Epoch: 005, Step: 003, Loss: 0.6506\n",
            "Epoch: 005, Step: 004, Loss: 0.6858\n",
            "Epoch: 005, Step: 005, Loss: 0.6572\n",
            "Epoch: 005, Step: 006, Loss: 0.6603\n",
            "Epoch: 005, Step: 007, Loss: 0.6919\n",
            "Epoch: 005, Step: 008, Loss: 0.6945\n",
            "Epoch: 005, Step: 009, Loss: 0.6937\n",
            "Epoch: 005, Step: 010, Loss: 0.6973\n",
            "Epoch: 005, Step: 011, Loss: 0.6981\n",
            "Epoch: 005, Step: 012, Loss: 0.7291\n",
            "Epoch: 005, Step: 013, Loss: 0.6988\n",
            "Epoch: 005, Step: 014, Loss: 0.6973\n",
            "Epoch: 005, Step: 015, Loss: 0.6979\n",
            "Epoch: 005, Step: 016, Loss: 0.7261\n",
            "Epoch: 005, Step: 017, Loss: 0.6640\n",
            "Epoch: 005, Step: 018, Loss: 0.6863\n",
            "Epoch: 005, Step: 019, Loss: 0.7162\n",
            "Epoch: 005, Step: 020, Loss: 0.7251\n",
            "Epoch: 005, Step: 021, Loss: 0.6670\n",
            "Epoch: 005, Step: 022, Loss: 0.6918\n",
            "Epoch: 005, Step: 023, Loss: 0.7279\n",
            "Epoch: 005, Step: 024, Loss: 0.7301\n",
            "Epoch: 005, Step: 025, Loss: 0.6954\n",
            "Epoch: 005, Step: 026, Loss: 0.7230\n",
            "Epoch: 005, Step: 027, Loss: 0.6915\n",
            "Epoch: 005, Step: 028, Loss: 0.6933\n",
            "Epoch: 005, Step: 029, Loss: 0.6988\n",
            "Epoch: 005, Step: 030, Loss: 0.7287\n",
            "Epoch: 005, Step: 031, Loss: 0.7273\n",
            "Epoch: 005, Step: 032, Loss: 0.6871\n",
            "Epoch: 005, Step: 033, Loss: 0.6869\n",
            "Epoch: 005, Step: 034, Loss: 0.7320\n",
            "Epoch: 005, Step: 035, Loss: 0.6895\n",
            "Epoch: 005, Step: 036, Loss: 0.6896\n",
            "Epoch: 005, Step: 037, Loss: 0.6964\n",
            "Epoch: 005, Step: 038, Loss: 0.6909\n",
            "Epoch: 005, Step: 039, Loss: 0.6938\n",
            "Epoch: 005, Step: 040, Loss: 0.7213\n",
            "Epoch: 005, Step: 041, Loss: 0.6954\n",
            "Epoch: 005, Step: 042, Loss: 0.7003\n",
            "Epoch: 005, Step: 043, Loss: 0.6924\n",
            "Epoch: 005, Step: 044, Loss: 0.7320\n",
            "Epoch: 005, Step: 045, Loss: 0.6919\n",
            "Epoch: 005, Step: 046, Loss: 0.6896\n",
            "Epoch: 005, Step: 047, Loss: 0.6954\n",
            "Epoch: 005, Step: 048, Loss: 0.7275\n",
            "Epoch: 005, Step: 049, Loss: 0.6952\n",
            "Epoch: 005, Step: 050, Loss: 0.6949\n",
            "Epoch: 005, Step: 051, Loss: 0.7267\n",
            "Epoch: 005, Step: 052, Loss: 0.7325\n",
            "Epoch: 005, Step: 053, Loss: 0.7304\n",
            "Epoch: 005, Step: 054, Loss: 0.7225\n",
            "Epoch: 005, Step: 055, Loss: 0.7316\n",
            "Epoch: 005, Step: 056, Loss: 0.6934\n",
            "Epoch: 005, Step: 057, Loss: 0.6993\n",
            "Epoch: 005, Step: 058, Loss: 0.7232\n",
            "Epoch: 005, Step: 059, Loss: 0.7004\n",
            "Epoch: 005, Step: 060, Loss: 0.6933\n",
            "Epoch: 005, Step: 061, Loss: 0.6927\n",
            "Epoch: 005, Step: 062, Loss: 0.7249\n",
            "Epoch: 005, Step: 063, Loss: 0.7061\n",
            "Epoch: 005, Step: 064, Loss: 0.6999\n",
            "Epoch: 005, Step: 065, Loss: 0.7024\n",
            "Epoch: 005, Step: 066, Loss: 0.6937\n",
            "Epoch: 005, Step: 067, Loss: 0.7260\n",
            "Epoch: 005, Step: 068, Loss: 0.6974\n",
            "Epoch: 005, Step: 069, Loss: 0.6931\n",
            "Epoch: 005, Step: 070, Loss: 0.6971\n",
            "Epoch: 005, Step: 071, Loss: 0.6891\n",
            "Epoch: 005, Step: 072, Loss: 0.6615\n",
            "Epoch: 005, Step: 073, Loss: 0.7271\n",
            "Epoch: 005, Step: 074, Loss: 0.6592\n",
            "Epoch: 005, Step: 075, Loss: 0.7004\n",
            "Epoch: 005, Step: 076, Loss: 0.7270\n",
            "Epoch: 005, Step: 077, Loss: 0.6686\n",
            "Epoch: 005, Step: 078, Loss: 0.7185\n",
            "Epoch: 005, Step: 079, Loss: 0.6866\n",
            "Epoch: 005, Step: 080, Loss: 0.7305\n",
            "Epoch: 005, Step: 081, Loss: 0.6935\n",
            "Epoch: 005, Step: 082, Loss: 0.7252\n",
            "Epoch: 005, Step: 083, Loss: 0.6920\n",
            "Epoch: 005, Step: 084, Loss: 0.6983\n",
            "Epoch: 005, Step: 085, Loss: 0.6910\n",
            "Epoch: 005, Step: 086, Loss: 0.6976\n",
            "Epoch: 005, Step: 087, Loss: 0.7297\n",
            "Epoch: 005, Step: 088, Loss: 0.6898\n",
            "Epoch: 005, Step: 089, Loss: 0.6933\n",
            "Epoch: 005, Step: 090, Loss: 0.6553\n",
            "Epoch: 005, Step: 091, Loss: 0.6959\n",
            "Epoch: 005, Step: 092, Loss: 0.7257\n",
            "Epoch: 005, Step: 093, Loss: 0.6621\n",
            "Epoch: 005, Step: 094, Loss: 0.7010\n",
            "Epoch: 005, Step: 095, Loss: 0.7226\n",
            "Epoch: 005, Step: 096, Loss: 0.6888\n",
            "Epoch: 005, Step: 097, Loss: 0.6996\n",
            "Epoch: 005, Step: 098, Loss: 0.6919\n",
            "Epoch: 005, Step: 099, Loss: 0.6922\n",
            "Epoch: 005, Step: 100, Loss: 0.6917\n",
            "Epoch: 005, Step: 101, Loss: 0.7349\n",
            "Epoch: 005, Step: 102, Loss: 0.6887\n",
            "Epoch: 005, Step: 103, Loss: 0.7250\n",
            "Epoch: 005, Step: 104, Loss: 0.6983\n",
            "Epoch: 005, Step: 105, Loss: 0.7312\n",
            "Epoch: 005, Step: 106, Loss: 0.6911\n",
            "Epoch: 005, Step: 107, Loss: 0.6940\n",
            "Epoch: 005, Step: 108, Loss: 0.7213\n",
            "Epoch: 005, Step: 109, Loss: 0.7238\n",
            "Epoch: 005, Step: 110, Loss: 0.6644\n",
            "Epoch: 005, Step: 111, Loss: 0.7237\n",
            "Epoch: 005, Step: 112, Loss: 0.6990\n",
            "Epoch: 005, Step: 113, Loss: 0.7219\n",
            "Epoch: 005, Step: 114, Loss: 0.6613\n",
            "Epoch: 005, Step: 115, Loss: 0.6862\n",
            "Epoch: 005, Step: 116, Loss: 0.6904\n",
            "Epoch: 005, Step: 117, Loss: 0.6639\n",
            "Epoch: 005, Step: 118, Loss: 0.6639\n",
            "Epoch: 005, Step: 119, Loss: 0.7000\n",
            "Epoch: 005, Step: 120, Loss: 0.6580\n",
            "Epoch: 005, Step: 121, Loss: 0.6945\n",
            "Epoch: 005, Step: 122, Loss: 0.6900\n",
            "Epoch: 005, Step: 123, Loss: 0.6627\n",
            "Epoch: 005, Step: 124, Loss: 0.7217\n",
            "Epoch: 005, Step: 125, Loss: 0.6900\n",
            "Epoch: 005, Step: 126, Loss: 0.6939\n",
            "Epoch: 005, Step: 127, Loss: 0.6932\n",
            "Epoch: 005, Step: 128, Loss: 0.6950\n",
            "Epoch: 005, Step: 129, Loss: 0.7240\n",
            "Epoch: 005, Step: 130, Loss: 0.6985\n",
            "Epoch: 005, Step: 131, Loss: 0.6943\n",
            "Epoch: 005, Step: 132, Loss: 0.6591\n",
            "Epoch: 005, Step: 133, Loss: 0.6968\n",
            "Epoch: 005, Step: 134, Loss: 0.7289\n",
            "Epoch: 005, Step: 135, Loss: 0.7243\n",
            "Epoch: 005, Step: 136, Loss: 0.7218\n",
            "Epoch: 005, Step: 137, Loss: 0.6878\n",
            "Epoch: 005, Step: 138, Loss: 0.6864\n",
            "Epoch: 005, Step: 139, Loss: 0.6928\n",
            "Epoch: 005, Step: 140, Loss: 0.6905\n",
            "Epoch: 005, Step: 141, Loss: 0.6888\n",
            "Epoch: 005, Step: 142, Loss: 0.7268\n",
            "Epoch: 005, Step: 143, Loss: 0.6892\n",
            "Epoch: 005, Step: 144, Loss: 0.6908\n",
            "Epoch: 005, Step: 145, Loss: 0.7247\n",
            "Epoch: 005, Step: 146, Loss: 0.6963\n",
            "Epoch: 005, Step: 147, Loss: 0.7003\n",
            "Epoch: 005, Step: 148, Loss: 0.7286\n",
            "Epoch: 005, Step: 149, Loss: 0.6677\n",
            "Epoch: 005, Step: 150, Loss: 0.7261\n",
            "Epoch: 005, Step: 151, Loss: 0.7201\n",
            "Epoch: 005, Step: 152, Loss: 0.6894\n",
            "Epoch: 005, Step: 153, Loss: 0.6940\n",
            "Epoch: 005, Step: 154, Loss: 0.7227\n",
            "Epoch: 005, Step: 155, Loss: 0.7168\n",
            "Epoch: 005, Step: 156, Loss: 0.7344\n",
            "Epoch: 005, Step: 157, Loss: 0.7310\n",
            "Epoch: 005, Step: 158, Loss: 0.7337\n",
            "Epoch: 005, Step: 159, Loss: 0.6637\n",
            "Epoch: 005, Step: 160, Loss: 0.6949\n",
            "Epoch: 005, Step: 161, Loss: 0.7283\n",
            "Epoch: 005, Step: 162, Loss: 0.6955\n",
            "Epoch: 005, Step: 163, Loss: 0.6963\n",
            "Epoch: 005, Step: 164, Loss: 0.7247\n",
            "Epoch: 005, Step: 165, Loss: 0.6963\n",
            "Epoch: 005, Step: 166, Loss: 0.6932\n",
            "Epoch: 005, Step: 167, Loss: 0.7321\n",
            "Epoch: 005, Step: 168, Loss: 0.6924\n",
            "Epoch: 005, Step: 169, Loss: 0.7375\n",
            "Epoch: 005, Step: 170, Loss: 0.7222\n",
            "Epoch: 005, Step: 171, Loss: 0.7255\n",
            "Epoch: 005, Step: 172, Loss: 0.7291\n",
            "Epoch: 005, Step: 173, Loss: 0.6898\n",
            "Epoch: 005, Step: 174, Loss: 0.7229\n",
            "Epoch: 005, Step: 175, Loss: 0.6911\n",
            "Epoch: 005, Step: 176, Loss: 0.6613\n",
            "Epoch: 005, Step: 177, Loss: 0.6935\n",
            "Epoch: 005, Step: 178, Loss: 0.6934\n",
            "Epoch: 005, Step: 179, Loss: 0.6623\n",
            "Epoch: 005, Step: 180, Loss: 0.6562\n",
            "Epoch: 005, Step: 181, Loss: 0.6534\n",
            "Epoch: 005, Step: 182, Loss: 0.6941\n",
            "Epoch: 005, Step: 183, Loss: 0.6899\n",
            "Epoch: 005, Step: 184, Loss: 0.7294\n",
            "Epoch: 005, Step: 185, Loss: 0.6934\n",
            "Epoch: 005, Step: 186, Loss: 0.6917\n",
            "Epoch: 005, Step: 187, Loss: 0.6606\n",
            "Epoch: 005, Step: 188, Loss: 0.6627\n",
            "Epoch: 005, Step: 189, Loss: 0.7271\n",
            "Epoch: 005, Step: 190, Loss: 0.7296\n",
            "Epoch: 005, Step: 191, Loss: 0.6975\n",
            "Epoch: 005, Step: 192, Loss: 0.6934\n",
            "Epoch: 005, Step: 193, Loss: 0.7202\n",
            "Epoch: 005, Step: 194, Loss: 0.7284\n",
            "Epoch: 005, Step: 195, Loss: 0.6978\n",
            "Epoch: 005, Step: 196, Loss: 0.7228\n",
            "Epoch: 005, Step: 197, Loss: 0.7258\n",
            "Epoch: 005, Step: 198, Loss: 0.6960\n",
            "Epoch: 005, Step: 199, Loss: 0.6939\n",
            "Epoch: 005, Step: 200, Loss: 0.6882\n",
            "Epoch: 005, Step: 201, Loss: 0.6942\n",
            "Epoch: 005, Step: 202, Loss: 0.6974\n",
            "Epoch: 005, Step: 203, Loss: 0.7000\n",
            "Epoch: 005, Step: 204, Loss: 0.7259\n",
            "Epoch: 005, Step: 205, Loss: 0.7332\n",
            "Epoch: 005, Step: 206, Loss: 0.6893\n",
            "Epoch: 005, Step: 207, Loss: 0.6959\n",
            "Epoch: 005, Step: 208, Loss: 0.6988\n",
            "Epoch: 005, Step: 209, Loss: 0.7316\n",
            "Epoch: 005, Step: 210, Loss: 0.6931\n",
            "Epoch: 005, Step: 211, Loss: 0.6941\n",
            "Epoch: 005, Step: 212, Loss: 0.6916\n",
            "Epoch: 005, Step: 213, Loss: 0.7026\n",
            "Epoch: 005, Step: 214, Loss: 0.6989\n",
            "Epoch: 005, Step: 215, Loss: 0.7213\n",
            "Epoch: 005, Step: 216, Loss: 0.7245\n",
            "Epoch: 005, Step: 217, Loss: 0.7262\n",
            "Epoch: 005, Step: 218, Loss: 0.6937\n",
            "Epoch: 005, Step: 219, Loss: 0.6970\n",
            "Epoch: 005, Step: 220, Loss: 0.6947\n",
            "Epoch: 005, Step: 221, Loss: 0.6880\n",
            "Epoch: 005, Step: 222, Loss: 0.6966\n",
            "Epoch: 005, Step: 223, Loss: 0.6882\n",
            "Epoch: 005, Step: 224, Loss: 0.6907\n",
            "Epoch: 005, Step: 225, Loss: 0.6917\n",
            "Epoch: 005, Step: 226, Loss: 0.6599\n",
            "Epoch: 005, Step: 227, Loss: 0.6660\n",
            "Epoch: 005, Step: 228, Loss: 0.6964\n",
            "Epoch: 005, Step: 229, Loss: 0.6930\n",
            "Epoch: 005, Step: 230, Loss: 0.6946\n",
            "Epoch: 005, Step: 231, Loss: 0.6905\n",
            "Epoch: 005, Step: 232, Loss: 0.7276\n",
            "Epoch: 005, Step: 233, Loss: 0.7268\n",
            "Epoch: 005, Step: 234, Loss: 0.6934\n",
            "Epoch: 005, Step: 235, Loss: 0.6966\n",
            "Epoch: 005, Step: 236, Loss: 0.6954\n",
            "Epoch: 005, Step: 237, Loss: 0.6954\n",
            "Epoch: 005, Step: 238, Loss: 0.7266\n",
            "Epoch: 005, Step: 239, Loss: 0.7190\n",
            "Epoch: 005, Step: 240, Loss: 0.6602\n",
            "Epoch: 005, Step: 241, Loss: 0.7259\n",
            "Epoch: 005, Step: 242, Loss: 0.7246\n",
            "Epoch: 005, Step: 243, Loss: 0.7214\n",
            "Epoch: 005, Step: 244, Loss: 0.6911\n",
            "Epoch: 005, Step: 245, Loss: 0.7299\n",
            "Epoch: 005, Step: 246, Loss: 0.7216\n",
            "Epoch: 005, Step: 247, Loss: 0.6497\n",
            "Epoch: 005, Step: 248, Loss: 0.6957\n",
            "Epoch: 005, Step: 249, Loss: 0.6934\n",
            "Epoch: 005, Step: 250, Loss: 0.7273\n",
            "Epoch: 005, Step: 251, Loss: 0.6886\n",
            "Epoch: 005, Step: 252, Loss: 0.7259\n",
            "Epoch: 005, Step: 253, Loss: 0.6969\n",
            "Epoch: 005, Step: 254, Loss: 0.7270\n",
            "Epoch: 005, Step: 255, Loss: 0.6928\n",
            "Epoch: 005, Step: 256, Loss: 0.6554\n",
            "Epoch: 005, Step: 257, Loss: 0.7286\n",
            "Epoch: 005, Step: 258, Loss: 0.7304\n",
            "Epoch: 005, Step: 259, Loss: 0.7243\n",
            "Epoch: 005, Step: 260, Loss: 0.6946\n",
            "Epoch: 005, Step: 261, Loss: 0.6960\n",
            "Epoch: 005, Step: 262, Loss: 0.6944\n",
            "Epoch: 005, Step: 263, Loss: 0.6976\n",
            "Epoch: 005, Step: 264, Loss: 0.6975\n",
            "Epoch: 005, Step: 265, Loss: 0.7337\n",
            "Epoch: 005, Step: 266, Loss: 0.6903\n",
            "Epoch: 005, Step: 267, Loss: 0.7278\n",
            "Epoch: 005, Step: 268, Loss: 0.6946\n",
            "Epoch: 005, Step: 269, Loss: 0.6627\n",
            "Epoch: 005, Step: 270, Loss: 0.7333\n",
            "Epoch: 005, Step: 271, Loss: 0.7317\n",
            "Epoch: 005, Step: 272, Loss: 0.6844\n",
            "Epoch: 005, Step: 273, Loss: 0.6577\n",
            "Epoch: 005, Step: 274, Loss: 0.6952\n",
            "Epoch: 005, Step: 275, Loss: 0.6570\n",
            "Epoch: 005, Step: 276, Loss: 0.6936\n",
            "Epoch: 005, Step: 277, Loss: 0.7197\n",
            "Epoch: 005, Step: 278, Loss: 0.6584\n",
            "Epoch: 005, Step: 279, Loss: 0.7219\n",
            "Epoch: 005, Step: 280, Loss: 0.6607\n",
            "Epoch: 005, Step: 281, Loss: 0.6939\n",
            "Epoch: 005, Step: 282, Loss: 0.6586\n",
            "Epoch: 005, Step: 283, Loss: 0.6945\n",
            "Epoch: 005, Step: 284, Loss: 0.7238\n",
            "Epoch: 005, Step: 285, Loss: 0.6908\n",
            "Epoch: 005, Step: 286, Loss: 0.7272\n",
            "Epoch: 005, Step: 287, Loss: 0.7245\n",
            "Epoch: 005, Step: 288, Loss: 0.7297\n",
            "Epoch: 005, Step: 289, Loss: 0.6534\n",
            "Epoch: 005, Step: 290, Loss: 0.7014\n",
            "Epoch: 005, Step: 291, Loss: 0.7271\n",
            "Epoch: 005, Step: 292, Loss: 0.6980\n",
            "Epoch: 005, Step: 293, Loss: 0.6922\n",
            "Epoch: 005, Step: 294, Loss: 0.7021\n",
            "Epoch: 005, Step: 295, Loss: 0.6484\n",
            "Epoch: 005, Step: 296, Loss: 0.6960\n",
            "Epoch: 005, Step: 297, Loss: 0.6514\n",
            "Epoch: 005, Step: 298, Loss: 0.6969\n",
            "Epoch: 005, Step: 299, Loss: 0.6910\n",
            "Epoch: 005, Step: 300, Loss: 0.6896\n",
            "Epoch: 005, Step: 301, Loss: 0.7322\n",
            "Epoch: 005, Step: 302, Loss: 0.6930\n",
            "Epoch: 005, Step: 303, Loss: 0.6574\n",
            "Epoch: 005, Step: 304, Loss: 0.7260\n",
            "Epoch: 005, Step: 305, Loss: 0.6831\n",
            "Epoch: 005, Step: 306, Loss: 0.6561\n",
            "Epoch: 005, Step: 307, Loss: 0.7263\n",
            "Epoch: 005, Step: 308, Loss: 0.7175\n",
            "Epoch: 005, Step: 309, Loss: 0.7256\n",
            "Epoch: 005, Step: 310, Loss: 0.6638\n",
            "Epoch: 005, Step: 311, Loss: 0.6593\n",
            "Epoch: 005, Step: 312, Loss: 0.7313\n",
            "Epoch: 005, Step: 313, Loss: 0.7220\n",
            "Epoch: 005, Step: 314, Loss: 0.7278\n",
            "Epoch: 005, Step: 315, Loss: 0.6961\n",
            "Epoch: 005, Step: 316, Loss: 0.7309\n",
            "Epoch: 005, Step: 317, Loss: 0.6974\n",
            "Epoch: 005, Step: 318, Loss: 0.6679\n",
            "Epoch: 005, Step: 319, Loss: 0.6598\n",
            "Epoch: 005, Step: 320, Loss: 0.7339\n",
            "Epoch: 005, Step: 321, Loss: 0.7274\n",
            "Epoch: 005, Step: 322, Loss: 0.6932\n",
            "Epoch: 005, Step: 323, Loss: 0.6903\n",
            "Epoch: 005, Step: 324, Loss: 0.7045\n",
            "Epoch: 005, Step: 325, Loss: 0.6967\n",
            "Epoch: 005, Step: 326, Loss: 0.6623\n",
            "Epoch: 005, Step: 327, Loss: 0.7306\n",
            "Epoch: 005, Step: 328, Loss: 0.6936\n",
            "Epoch: 005, Step: 329, Loss: 0.6954\n",
            "Epoch: 005, Step: 330, Loss: 0.7286\n",
            "Epoch: 005, Step: 331, Loss: 0.7224\n",
            "Epoch: 005, Step: 332, Loss: 0.7186\n",
            "Epoch: 005, Step: 333, Loss: 0.6931\n",
            "Epoch: 005, Step: 334, Loss: 0.6981\n",
            "Epoch: 005, Step: 335, Loss: 0.6869\n",
            "Epoch: 005, Step: 336, Loss: 0.6664\n",
            "Epoch: 005, Step: 337, Loss: 0.7260\n",
            "Epoch: 005, Step: 338, Loss: 0.6984\n",
            "Epoch: 005, Step: 339, Loss: 0.6605\n",
            "Epoch: 005, Step: 340, Loss: 0.6924\n",
            "Epoch: 005, Step: 341, Loss: 0.6617\n",
            "Epoch: 005, Step: 342, Loss: 0.6610\n",
            "Epoch: 005, Step: 343, Loss: 0.7285\n",
            "Epoch: 005, Step: 344, Loss: 0.7215\n",
            "Epoch: 005, Step: 345, Loss: 0.7258\n",
            "Epoch: 005, Step: 346, Loss: 0.6949\n",
            "Epoch: 005, Step: 347, Loss: 0.6993\n",
            "Epoch: 005, Step: 348, Loss: 0.7253\n",
            "Epoch: 005, Step: 349, Loss: 0.7259\n",
            "Epoch: 005, Step: 350, Loss: 0.7361\n",
            "Epoch: 005, Step: 351, Loss: 0.6929\n",
            "Epoch: 005, Step: 352, Loss: 0.7333\n",
            "Epoch: 005, Step: 353, Loss: 0.7282\n",
            "Epoch: 005, Step: 354, Loss: 0.6882\n",
            "Epoch: 005, Step: 355, Loss: 0.7191\n",
            "Epoch: 005, Step: 356, Loss: 0.6551\n",
            "Epoch: 005, Step: 357, Loss: 0.6894\n",
            "Epoch: 005, Step: 358, Loss: 0.6549\n",
            "Epoch: 005, Step: 359, Loss: 0.6941\n",
            "Epoch: 005, Step: 360, Loss: 0.6954\n",
            "Epoch: 005, Step: 361, Loss: 0.7302\n",
            "Epoch: 005, Step: 362, Loss: 0.7247\n",
            "Epoch: 005, Step: 363, Loss: 0.7194\n",
            "Epoch: 005, Step: 364, Loss: 0.6920\n",
            "Epoch: 005, Step: 365, Loss: 0.6965\n",
            "Epoch: 005, Step: 366, Loss: 0.6582\n",
            "Epoch: 005, Step: 367, Loss: 0.6907\n",
            "Epoch: 005, Step: 368, Loss: 0.6638\n",
            "Epoch: 005, Step: 369, Loss: 0.6895\n",
            "Epoch: 005, Step: 370, Loss: 0.7314\n",
            "Epoch: 005, Step: 371, Loss: 0.6894\n",
            "Epoch: 005, Step: 372, Loss: 0.6944\n",
            "Epoch: 005, Step: 373, Loss: 0.6947\n",
            "Epoch: 005, Step: 374, Loss: 0.6920\n",
            "Epoch: 005, Step: 375, Loss: 0.6942\n",
            "Epoch: 005, Step: 376, Loss: 0.6926\n",
            "Epoch: 005, Step: 377, Loss: 0.7223\n",
            "Epoch: 005, Step: 378, Loss: 0.6967\n",
            "Epoch: 005, Step: 379, Loss: 0.6497\n",
            "Epoch: 005, Step: 380, Loss: 0.7010\n",
            "Epoch: 005, Step: 381, Loss: 0.6613\n",
            "Epoch: 005, Step: 382, Loss: 0.6569\n",
            "Epoch: 005, Step: 383, Loss: 0.6950\n",
            "Epoch: 005, Step: 384, Loss: 0.6927\n",
            "Epoch: 005, Step: 385, Loss: 0.7366\n",
            "Epoch: 005, Step: 386, Loss: 0.6951\n",
            "Epoch: 005, Step: 387, Loss: 0.6654\n",
            "Epoch: 005, Step: 388, Loss: 0.7314\n",
            "Epoch: 005, Step: 389, Loss: 0.6951\n",
            "Epoch: 005, Step: 390, Loss: 0.7236\n",
            "Epoch: 005, Step: 391, Loss: 0.6921\n",
            "Epoch: 005, Step: 392, Loss: 0.6952\n",
            "Epoch: 005, Step: 393, Loss: 0.6978\n",
            "Epoch: 005, Step: 394, Loss: 0.6912\n",
            "Epoch: 005, Step: 395, Loss: 0.6964\n",
            "Epoch: 005, Step: 396, Loss: 0.6937\n",
            "Epoch: 005, Step: 397, Loss: 0.6947\n",
            "Epoch: 005, Step: 398, Loss: 0.6544\n",
            "Epoch: 005, Step: 399, Loss: 0.6612\n",
            "Epoch: 005, Step: 400, Loss: 0.6961\n",
            "Epoch: 005, Step: 401, Loss: 0.6917\n",
            "Epoch: 005, Step: 402, Loss: 0.6923\n",
            "Epoch: 005, Step: 403, Loss: 0.6863\n",
            "Epoch: 005, Step: 404, Loss: 0.6610\n",
            "Epoch: 005, Step: 405, Loss: 0.6886\n",
            "Epoch: 005, Step: 406, Loss: 0.6896\n",
            "Epoch: 005, Step: 407, Loss: 0.7275\n",
            "Epoch: 005, Step: 408, Loss: 0.6989\n",
            "Epoch: 005, Step: 409, Loss: 0.6559\n",
            "Epoch: 005, Step: 410, Loss: 0.6935\n",
            "Epoch: 005, Step: 411, Loss: 0.6922\n",
            "Epoch: 005, Step: 412, Loss: 0.7222\n",
            "Epoch: 005, Step: 000, Val Loss: 0.6942\n",
            "Epoch: 005, Step: 001, Val Loss: 0.6609\n",
            "Epoch: 005, Step: 002, Val Loss: 0.6943\n",
            "Epoch: 005, Step: 003, Val Loss: 0.6935\n",
            "Epoch: 005, Step: 004, Val Loss: 0.7272\n",
            "Epoch: 005, Step: 005, Val Loss: 0.6946\n",
            "Epoch: 005, Step: 006, Val Loss: 0.6620\n",
            "Epoch: 005, Step: 007, Val Loss: 0.6616\n",
            "Epoch: 005, Step: 008, Val Loss: 0.7257\n",
            "Epoch: 005, Step: 009, Val Loss: 0.7266\n",
            "Epoch: 005, Step: 010, Val Loss: 0.7254\n",
            "Epoch: 005, Step: 011, Val Loss: 0.6936\n",
            "Epoch: 005, Step: 012, Val Loss: 0.6938\n",
            "Epoch: 005, Step: 013, Val Loss: 0.6623\n",
            "Epoch: 005, Step: 014, Val Loss: 0.6944\n",
            "Epoch: 005, Step: 015, Val Loss: 0.6922\n",
            "Epoch: 005, Step: 016, Val Loss: 0.7269\n",
            "Epoch: 005, Step: 017, Val Loss: 0.6946\n",
            "Epoch: 005, Step: 018, Val Loss: 0.6935\n",
            "Epoch: 005, Step: 019, Val Loss: 0.7266\n",
            "Epoch: 005, Step: 020, Val Loss: 0.6942\n",
            "Epoch: 005, Step: 021, Val Loss: 0.7261\n",
            "Epoch: 005, Step: 022, Val Loss: 0.6918\n",
            "Epoch: 005, Step: 023, Val Loss: 0.7270\n",
            "Epoch: 005, Step: 024, Val Loss: 0.7249\n",
            "Epoch: 005, Step: 025, Val Loss: 0.7266\n",
            "Epoch: 005, Step: 026, Val Loss: 0.6937\n",
            "Epoch: 005, Step: 027, Val Loss: 0.7268\n",
            "Epoch: 005, Step: 028, Val Loss: 0.6946\n",
            "Epoch: 005, Step: 029, Val Loss: 0.7258\n",
            "Epoch: 005, Step: 030, Val Loss: 0.7263\n",
            "Epoch: 005, Step: 031, Val Loss: 0.6607\n",
            "Epoch: 005, Step: 032, Val Loss: 0.6939\n",
            "Epoch: 005, Step: 033, Val Loss: 0.6939\n",
            "Epoch: 005, Step: 034, Val Loss: 0.6911\n",
            "Epoch: 005, Step: 035, Val Loss: 0.6933\n",
            "Epoch: 005, Step: 036, Val Loss: 0.6935\n",
            "Epoch: 005, Step: 037, Val Loss: 0.6932\n",
            "Epoch: 005, Step: 038, Val Loss: 0.7257\n",
            "Epoch: 005, Step: 039, Val Loss: 0.6615\n",
            "Epoch: 005, Step: 040, Val Loss: 0.7253\n",
            "Epoch: 005, Step: 041, Val Loss: 0.7269\n",
            "Epoch: 005, Step: 042, Val Loss: 0.6939\n",
            "Epoch: 005, Step: 043, Val Loss: 0.6923\n",
            "Epoch: 005, Step: 044, Val Loss: 0.7252\n",
            "Epoch: 005, Step: 045, Val Loss: 0.6606\n",
            "Epoch: 005, Step: 046, Val Loss: 0.6941\n",
            "Epoch: 005, Step: 047, Val Loss: 0.7258\n",
            "Epoch: 005, Step: 048, Val Loss: 0.7276\n",
            "Epoch: 005, Step: 049, Val Loss: 0.7269\n",
            "Epoch: 005, Step: 050, Val Loss: 0.6610\n",
            "Epoch: 005, Step: 051, Val Loss: 0.7266\n",
            "Epoch: 005, Step: 052, Val Loss: 0.6916\n",
            "Epoch: 005, Step: 053, Val Loss: 0.6609\n",
            "Epoch: 005, Step: 054, Val Loss: 0.7267\n",
            "Epoch: 005, Step: 055, Val Loss: 0.6948\n",
            "Epoch: 005, Step: 056, Val Loss: 0.6924\n",
            "Epoch: 005, Step: 057, Val Loss: 0.6609\n",
            "Epoch: 005, Step: 058, Val Loss: 0.6930\n",
            "Epoch: 005, Step: 059, Val Loss: 0.6935\n",
            "Epoch: 005, Step: 060, Val Loss: 0.6926\n",
            "Epoch: 005, Step: 061, Val Loss: 0.6930\n",
            "Epoch: 005, Step: 062, Val Loss: 0.6937\n",
            "Epoch: 005, Step: 063, Val Loss: 0.7265\n",
            "Epoch: 005, Step: 064, Val Loss: 0.6933\n",
            "Epoch: 005, Step: 065, Val Loss: 0.6611\n",
            "Epoch: 005, Step: 066, Val Loss: 0.6589\n",
            "Epoch: 005, Step: 067, Val Loss: 0.7259\n",
            "Epoch: 005, Step: 068, Val Loss: 0.6943\n",
            "Epoch: 005, Step: 069, Val Loss: 0.7255\n",
            "Epoch: 005, Step: 070, Val Loss: 0.6936\n",
            "Epoch: 005, Step: 071, Val Loss: 0.6945\n",
            "Epoch: 005, Step: 072, Val Loss: 0.7244\n",
            "Epoch: 005, Step: 073, Val Loss: 0.6939\n",
            "Epoch: 005, Step: 074, Val Loss: 0.7260\n",
            "Epoch: 005, Step: 075, Val Loss: 0.6610\n",
            "Epoch: 005, Step: 076, Val Loss: 0.6932\n",
            "Epoch: 005, Step: 077, Val Loss: 0.6932\n",
            "Epoch: 005, Step: 078, Val Loss: 0.6937\n",
            "Epoch: 005, Step: 079, Val Loss: 0.6948\n",
            "Epoch: 005, Step: 080, Val Loss: 0.6924\n",
            "Epoch: 005, Step: 081, Val Loss: 0.6938\n",
            "Epoch: 005, Step: 082, Val Loss: 0.6926\n",
            "Epoch: 005, Step: 083, Val Loss: 0.6933\n",
            "Epoch: 005, Step: 084, Val Loss: 0.6932\n",
            "Epoch: 005, Step: 085, Val Loss: 0.7253\n",
            "Epoch: 005, Step: 086, Val Loss: 0.6937\n",
            "Epoch: 005, Step: 087, Val Loss: 0.6932\n",
            "Epoch: 005, Step: 088, Val Loss: 0.7257\n",
            "Epoch: 005, Step: 089, Val Loss: 0.6941\n",
            "Epoch: 005, Step: 090, Val Loss: 0.7263\n",
            "Epoch: 005, Step: 091, Val Loss: 0.6928\n",
            "Epoch: 005, Step: 092, Val Loss: 0.6939\n",
            "Epoch: 005, Step: 093, Val Loss: 0.7262\n",
            "Epoch: 005, Step: 094, Val Loss: 0.7251\n",
            "Epoch: 005, Step: 095, Val Loss: 0.6613\n",
            "Epoch: 005, Step: 096, Val Loss: 0.6947\n",
            "Epoch: 005, Step: 097, Val Loss: 0.6618\n",
            "Epoch: 005, Step: 098, Val Loss: 0.7260\n",
            "Epoch: 005, Step: 099, Val Loss: 0.7263\n",
            "Epoch: 005, Step: 100, Val Loss: 0.7265\n",
            "Epoch: 005, Step: 101, Val Loss: 0.6623\n",
            "Epoch: 005, Step: 102, Val Loss: 0.6931\n",
            "Epoch: 005, Step: 103, Val Loss: 0.7259\n",
            "Epoch: 005, Step: 104, Val Loss: 0.7266\n",
            "Epoch: 005, Step: 105, Val Loss: 0.6952\n",
            "Epoch: 005, Step: 106, Val Loss: 0.6941\n",
            "Epoch: 005, Step: 107, Val Loss: 0.6928\n",
            "Epoch: 005, Step: 108, Val Loss: 0.6609\n",
            "Epoch: 005, Step: 109, Val Loss: 0.6951\n",
            "Epoch: 005, Step: 110, Val Loss: 0.7257\n",
            "Epoch: 005, Step: 111, Val Loss: 0.6930\n",
            "Epoch: 005, Step: 112, Val Loss: 0.7258\n",
            "Epoch: 005, Step: 113, Val Loss: 0.7263\n",
            "Epoch: 005, Step: 114, Val Loss: 0.6930\n",
            "Epoch: 005, Step: 115, Val Loss: 0.7269\n",
            "Epoch: 005, Step: 116, Val Loss: 0.6606\n",
            "Epoch: 005, Step: 117, Val Loss: 0.6931\n",
            "Epoch: 006, Step: 000, Loss: 0.6922\n",
            "Epoch: 006, Step: 001, Loss: 0.7153\n",
            "Epoch: 006, Step: 002, Loss: 0.6588\n",
            "Epoch: 006, Step: 003, Loss: 0.6933\n",
            "Epoch: 006, Step: 004, Loss: 0.6964\n",
            "Epoch: 006, Step: 005, Loss: 0.7229\n",
            "Epoch: 006, Step: 006, Loss: 0.6859\n",
            "Epoch: 006, Step: 007, Loss: 0.7022\n",
            "Epoch: 006, Step: 008, Loss: 0.6927\n",
            "Epoch: 006, Step: 009, Loss: 0.7267\n",
            "Epoch: 006, Step: 010, Loss: 0.6965\n",
            "Epoch: 006, Step: 011, Loss: 0.6934\n",
            "Epoch: 006, Step: 012, Loss: 0.6584\n",
            "Epoch: 006, Step: 013, Loss: 0.6646\n",
            "Epoch: 006, Step: 014, Loss: 0.6571\n",
            "Epoch: 006, Step: 015, Loss: 0.6954\n",
            "Epoch: 006, Step: 016, Loss: 0.7289\n",
            "Epoch: 006, Step: 017, Loss: 0.7241\n",
            "Epoch: 006, Step: 018, Loss: 0.7251\n",
            "Epoch: 006, Step: 019, Loss: 0.7247\n",
            "Epoch: 006, Step: 020, Loss: 0.6952\n",
            "Epoch: 006, Step: 021, Loss: 0.6926\n",
            "Epoch: 006, Step: 022, Loss: 0.7219\n",
            "Epoch: 006, Step: 023, Loss: 0.7272\n",
            "Epoch: 006, Step: 024, Loss: 0.6950\n",
            "Epoch: 006, Step: 025, Loss: 0.6868\n",
            "Epoch: 006, Step: 026, Loss: 0.6501\n",
            "Epoch: 006, Step: 027, Loss: 0.6626\n",
            "Epoch: 006, Step: 028, Loss: 0.6954\n",
            "Epoch: 006, Step: 029, Loss: 0.6886\n",
            "Epoch: 006, Step: 030, Loss: 0.7305\n",
            "Epoch: 006, Step: 031, Loss: 0.7277\n",
            "Epoch: 006, Step: 032, Loss: 0.6934\n",
            "Epoch: 006, Step: 033, Loss: 0.7140\n",
            "Epoch: 006, Step: 034, Loss: 0.7255\n",
            "Epoch: 006, Step: 035, Loss: 0.7230\n",
            "Epoch: 006, Step: 036, Loss: 0.7273\n",
            "Epoch: 006, Step: 037, Loss: 0.6989\n",
            "Epoch: 006, Step: 038, Loss: 0.7236\n",
            "Epoch: 006, Step: 039, Loss: 0.7230\n",
            "Epoch: 006, Step: 040, Loss: 0.6969\n",
            "Epoch: 006, Step: 041, Loss: 0.7019\n",
            "Epoch: 006, Step: 042, Loss: 0.6612\n",
            "Epoch: 006, Step: 043, Loss: 0.7228\n",
            "Epoch: 006, Step: 044, Loss: 0.7321\n",
            "Epoch: 006, Step: 045, Loss: 0.6921\n",
            "Epoch: 006, Step: 046, Loss: 0.6670\n",
            "Epoch: 006, Step: 047, Loss: 0.6664\n",
            "Epoch: 006, Step: 048, Loss: 0.6924\n",
            "Epoch: 006, Step: 049, Loss: 0.7217\n",
            "Epoch: 006, Step: 050, Loss: 0.7249\n",
            "Epoch: 006, Step: 051, Loss: 0.6959\n",
            "Epoch: 006, Step: 052, Loss: 0.6902\n",
            "Epoch: 006, Step: 053, Loss: 0.6997\n",
            "Epoch: 006, Step: 054, Loss: 0.7279\n",
            "Epoch: 006, Step: 055, Loss: 0.6936\n",
            "Epoch: 006, Step: 056, Loss: 0.6590\n",
            "Epoch: 006, Step: 057, Loss: 0.6947\n",
            "Epoch: 006, Step: 058, Loss: 0.7272\n",
            "Epoch: 006, Step: 059, Loss: 0.6583\n",
            "Epoch: 006, Step: 060, Loss: 0.6894\n",
            "Epoch: 006, Step: 061, Loss: 0.7223\n",
            "Epoch: 006, Step: 062, Loss: 0.6939\n",
            "Epoch: 006, Step: 063, Loss: 0.7187\n",
            "Epoch: 006, Step: 064, Loss: 0.7335\n",
            "Epoch: 006, Step: 065, Loss: 0.6693\n",
            "Epoch: 006, Step: 066, Loss: 0.6895\n",
            "Epoch: 006, Step: 067, Loss: 0.7317\n",
            "Epoch: 006, Step: 068, Loss: 0.6963\n",
            "Epoch: 006, Step: 069, Loss: 0.6635\n",
            "Epoch: 006, Step: 070, Loss: 0.7280\n",
            "Epoch: 006, Step: 071, Loss: 0.6884\n",
            "Epoch: 006, Step: 072, Loss: 0.7198\n",
            "Epoch: 006, Step: 073, Loss: 0.6948\n",
            "Epoch: 006, Step: 074, Loss: 0.7002\n",
            "Epoch: 006, Step: 075, Loss: 0.6928\n",
            "Epoch: 006, Step: 076, Loss: 0.7210\n",
            "Epoch: 006, Step: 077, Loss: 0.6672\n",
            "Epoch: 006, Step: 078, Loss: 0.6609\n",
            "Epoch: 006, Step: 079, Loss: 0.6981\n",
            "Epoch: 006, Step: 080, Loss: 0.7267\n",
            "Epoch: 006, Step: 081, Loss: 0.6983\n",
            "Epoch: 006, Step: 082, Loss: 0.7236\n",
            "Epoch: 006, Step: 083, Loss: 0.6869\n",
            "Epoch: 006, Step: 084, Loss: 0.6991\n",
            "Epoch: 006, Step: 085, Loss: 0.6985\n",
            "Epoch: 006, Step: 086, Loss: 0.6969\n",
            "Epoch: 006, Step: 087, Loss: 0.7310\n",
            "Epoch: 006, Step: 088, Loss: 0.7313\n",
            "Epoch: 006, Step: 089, Loss: 0.7254\n",
            "Epoch: 006, Step: 090, Loss: 0.6977\n",
            "Epoch: 006, Step: 091, Loss: 0.6900\n",
            "Epoch: 006, Step: 092, Loss: 0.7262\n",
            "Epoch: 006, Step: 093, Loss: 0.6925\n",
            "Epoch: 006, Step: 094, Loss: 0.6653\n",
            "Epoch: 006, Step: 095, Loss: 0.7287\n",
            "Epoch: 006, Step: 096, Loss: 0.6960\n",
            "Epoch: 006, Step: 097, Loss: 0.6713\n",
            "Epoch: 006, Step: 098, Loss: 0.6580\n",
            "Epoch: 006, Step: 099, Loss: 0.6923\n",
            "Epoch: 006, Step: 100, Loss: 0.6880\n",
            "Epoch: 006, Step: 101, Loss: 0.6595\n",
            "Epoch: 006, Step: 102, Loss: 0.6942\n",
            "Epoch: 006, Step: 103, Loss: 0.6927\n",
            "Epoch: 006, Step: 104, Loss: 0.6964\n",
            "Epoch: 006, Step: 105, Loss: 0.7293\n",
            "Epoch: 006, Step: 106, Loss: 0.7258\n",
            "Epoch: 006, Step: 107, Loss: 0.7374\n",
            "Epoch: 006, Step: 108, Loss: 0.7245\n",
            "Epoch: 006, Step: 109, Loss: 0.6880\n",
            "Epoch: 006, Step: 110, Loss: 0.6983\n",
            "Epoch: 006, Step: 111, Loss: 0.7201\n",
            "Epoch: 006, Step: 112, Loss: 0.6950\n",
            "Epoch: 006, Step: 113, Loss: 0.6951\n",
            "Epoch: 006, Step: 114, Loss: 0.6919\n",
            "Epoch: 006, Step: 115, Loss: 0.7324\n",
            "Epoch: 006, Step: 116, Loss: 0.6550\n",
            "Epoch: 006, Step: 117, Loss: 0.6900\n",
            "Epoch: 006, Step: 118, Loss: 0.6500\n",
            "Epoch: 006, Step: 119, Loss: 0.6683\n",
            "Epoch: 006, Step: 120, Loss: 0.6623\n",
            "Epoch: 006, Step: 121, Loss: 0.6972\n",
            "Epoch: 006, Step: 122, Loss: 0.7238\n",
            "Epoch: 006, Step: 123, Loss: 0.7186\n",
            "Epoch: 006, Step: 124, Loss: 0.7294\n",
            "Epoch: 006, Step: 125, Loss: 0.7249\n",
            "Epoch: 006, Step: 126, Loss: 0.6934\n",
            "Epoch: 006, Step: 127, Loss: 0.6999\n",
            "Epoch: 006, Step: 128, Loss: 0.7255\n",
            "Epoch: 006, Step: 129, Loss: 0.6975\n",
            "Epoch: 006, Step: 130, Loss: 0.6914\n",
            "Epoch: 006, Step: 131, Loss: 0.6547\n",
            "Epoch: 006, Step: 132, Loss: 0.6639\n",
            "Epoch: 006, Step: 133, Loss: 0.7335\n",
            "Epoch: 006, Step: 134, Loss: 0.6595\n",
            "Epoch: 006, Step: 135, Loss: 0.6640\n",
            "Epoch: 006, Step: 136, Loss: 0.7249\n",
            "Epoch: 006, Step: 137, Loss: 0.6972\n",
            "Epoch: 006, Step: 138, Loss: 0.7260\n",
            "Epoch: 006, Step: 139, Loss: 0.6605\n",
            "Epoch: 006, Step: 140, Loss: 0.6940\n",
            "Epoch: 006, Step: 141, Loss: 0.7232\n",
            "Epoch: 006, Step: 142, Loss: 0.6881\n",
            "Epoch: 006, Step: 143, Loss: 0.6966\n",
            "Epoch: 006, Step: 144, Loss: 0.7260\n",
            "Epoch: 006, Step: 145, Loss: 0.7180\n",
            "Epoch: 006, Step: 146, Loss: 0.6953\n",
            "Epoch: 006, Step: 147, Loss: 0.6879\n",
            "Epoch: 006, Step: 148, Loss: 0.6989\n",
            "Epoch: 006, Step: 149, Loss: 0.6888\n",
            "Epoch: 006, Step: 150, Loss: 0.6626\n",
            "Epoch: 006, Step: 151, Loss: 0.7247\n",
            "Epoch: 006, Step: 152, Loss: 0.7210\n",
            "Epoch: 006, Step: 153, Loss: 0.7265\n",
            "Epoch: 006, Step: 154, Loss: 0.7257\n",
            "Epoch: 006, Step: 155, Loss: 0.6989\n",
            "Epoch: 006, Step: 156, Loss: 0.6614\n",
            "Epoch: 006, Step: 157, Loss: 0.7278\n",
            "Epoch: 006, Step: 158, Loss: 0.6965\n",
            "Epoch: 006, Step: 159, Loss: 0.6977\n",
            "Epoch: 006, Step: 160, Loss: 0.6607\n",
            "Epoch: 006, Step: 161, Loss: 0.6932\n",
            "Epoch: 006, Step: 162, Loss: 0.6660\n",
            "Epoch: 006, Step: 163, Loss: 0.6598\n",
            "Epoch: 006, Step: 164, Loss: 0.7280\n",
            "Epoch: 006, Step: 165, Loss: 0.6972\n",
            "Epoch: 006, Step: 166, Loss: 0.6932\n",
            "Epoch: 006, Step: 167, Loss: 0.6970\n",
            "Epoch: 006, Step: 168, Loss: 0.7220\n",
            "Epoch: 006, Step: 169, Loss: 0.6933\n",
            "Epoch: 006, Step: 170, Loss: 0.6978\n",
            "Epoch: 006, Step: 171, Loss: 0.6934\n",
            "Epoch: 006, Step: 172, Loss: 0.7246\n",
            "Epoch: 006, Step: 173, Loss: 0.6904\n",
            "Epoch: 006, Step: 174, Loss: 0.6924\n",
            "Epoch: 006, Step: 175, Loss: 0.7043\n",
            "Epoch: 006, Step: 176, Loss: 0.6951\n",
            "Epoch: 006, Step: 177, Loss: 0.7329\n",
            "Epoch: 006, Step: 178, Loss: 0.7193\n",
            "Epoch: 006, Step: 179, Loss: 0.6913\n",
            "Epoch: 006, Step: 180, Loss: 0.6924\n",
            "Epoch: 006, Step: 181, Loss: 0.6935\n",
            "Epoch: 006, Step: 182, Loss: 0.7238\n",
            "Epoch: 006, Step: 183, Loss: 0.6925\n",
            "Epoch: 006, Step: 184, Loss: 0.6974\n",
            "Epoch: 006, Step: 185, Loss: 0.7312\n",
            "Epoch: 006, Step: 186, Loss: 0.6958\n",
            "Epoch: 006, Step: 187, Loss: 0.6966\n",
            "Epoch: 006, Step: 188, Loss: 0.7249\n",
            "Epoch: 006, Step: 189, Loss: 0.7237\n",
            "Epoch: 006, Step: 190, Loss: 0.6893\n",
            "Epoch: 006, Step: 191, Loss: 0.7281\n",
            "Epoch: 006, Step: 192, Loss: 0.7229\n",
            "Epoch: 006, Step: 193, Loss: 0.6908\n",
            "Epoch: 006, Step: 194, Loss: 0.6938\n",
            "Epoch: 006, Step: 195, Loss: 0.6542\n",
            "Epoch: 006, Step: 196, Loss: 0.7294\n",
            "Epoch: 006, Step: 197, Loss: 0.6630\n",
            "Epoch: 006, Step: 198, Loss: 0.6621\n",
            "Epoch: 006, Step: 199, Loss: 0.7180\n",
            "Epoch: 006, Step: 200, Loss: 0.7313\n",
            "Epoch: 006, Step: 201, Loss: 0.6908\n",
            "Epoch: 006, Step: 202, Loss: 0.6908\n",
            "Epoch: 006, Step: 203, Loss: 0.7205\n",
            "Epoch: 006, Step: 204, Loss: 0.7211\n",
            "Epoch: 006, Step: 205, Loss: 0.6929\n",
            "Epoch: 006, Step: 206, Loss: 0.6904\n",
            "Epoch: 006, Step: 207, Loss: 0.6595\n",
            "Epoch: 006, Step: 208, Loss: 0.6891\n",
            "Epoch: 006, Step: 209, Loss: 0.6971\n",
            "Epoch: 006, Step: 210, Loss: 0.6933\n",
            "Epoch: 006, Step: 211, Loss: 0.7327\n",
            "Epoch: 006, Step: 212, Loss: 0.6591\n",
            "Epoch: 006, Step: 213, Loss: 0.6992\n",
            "Epoch: 006, Step: 214, Loss: 0.6918\n",
            "Epoch: 006, Step: 215, Loss: 0.7052\n",
            "Epoch: 006, Step: 216, Loss: 0.6952\n",
            "Epoch: 006, Step: 217, Loss: 0.7003\n",
            "Epoch: 006, Step: 218, Loss: 0.7018\n",
            "Epoch: 006, Step: 219, Loss: 0.7278\n",
            "Epoch: 006, Step: 220, Loss: 0.6950\n",
            "Epoch: 006, Step: 221, Loss: 0.7230\n",
            "Epoch: 006, Step: 222, Loss: 0.6903\n",
            "Epoch: 006, Step: 223, Loss: 0.7214\n",
            "Epoch: 006, Step: 224, Loss: 0.7003\n",
            "Epoch: 006, Step: 225, Loss: 0.6890\n",
            "Epoch: 006, Step: 226, Loss: 0.6904\n",
            "Epoch: 006, Step: 227, Loss: 0.6969\n",
            "Epoch: 006, Step: 228, Loss: 0.6666\n",
            "Epoch: 006, Step: 229, Loss: 0.7347\n",
            "Epoch: 006, Step: 230, Loss: 0.7001\n",
            "Epoch: 006, Step: 231, Loss: 0.6956\n",
            "Epoch: 006, Step: 232, Loss: 0.7223\n",
            "Epoch: 006, Step: 233, Loss: 0.6863\n",
            "Epoch: 006, Step: 234, Loss: 0.7021\n",
            "Epoch: 006, Step: 235, Loss: 0.6584\n",
            "Epoch: 006, Step: 236, Loss: 0.7251\n",
            "Epoch: 006, Step: 237, Loss: 0.6908\n",
            "Epoch: 006, Step: 238, Loss: 0.6642\n",
            "Epoch: 006, Step: 239, Loss: 0.6920\n",
            "Epoch: 006, Step: 240, Loss: 0.6571\n",
            "Epoch: 006, Step: 241, Loss: 0.6569\n",
            "Epoch: 006, Step: 242, Loss: 0.7343\n",
            "Epoch: 006, Step: 243, Loss: 0.6641\n",
            "Epoch: 006, Step: 244, Loss: 0.6980\n",
            "Epoch: 006, Step: 245, Loss: 0.6663\n",
            "Epoch: 006, Step: 246, Loss: 0.7212\n",
            "Epoch: 006, Step: 247, Loss: 0.7245\n",
            "Epoch: 006, Step: 248, Loss: 0.7250\n",
            "Epoch: 006, Step: 249, Loss: 0.6946\n",
            "Epoch: 006, Step: 250, Loss: 0.6929\n",
            "Epoch: 006, Step: 251, Loss: 0.7342\n",
            "Epoch: 006, Step: 252, Loss: 0.7243\n",
            "Epoch: 006, Step: 253, Loss: 0.6964\n",
            "Epoch: 006, Step: 254, Loss: 0.6589\n",
            "Epoch: 006, Step: 255, Loss: 0.7332\n",
            "Epoch: 006, Step: 256, Loss: 0.6928\n",
            "Epoch: 006, Step: 257, Loss: 0.6887\n",
            "Epoch: 006, Step: 258, Loss: 0.6576\n",
            "Epoch: 006, Step: 259, Loss: 0.6858\n",
            "Epoch: 006, Step: 260, Loss: 0.7264\n",
            "Epoch: 006, Step: 261, Loss: 0.6944\n",
            "Epoch: 006, Step: 262, Loss: 0.7272\n",
            "Epoch: 006, Step: 263, Loss: 0.6910\n",
            "Epoch: 006, Step: 264, Loss: 0.6596\n",
            "Epoch: 006, Step: 265, Loss: 0.7012\n",
            "Epoch: 006, Step: 266, Loss: 0.6921\n",
            "Epoch: 006, Step: 267, Loss: 0.6619\n",
            "Epoch: 006, Step: 268, Loss: 0.6898\n",
            "Epoch: 006, Step: 269, Loss: 0.6933\n",
            "Epoch: 006, Step: 270, Loss: 0.7001\n",
            "Epoch: 006, Step: 271, Loss: 0.7255\n",
            "Epoch: 006, Step: 272, Loss: 0.7017\n",
            "Epoch: 006, Step: 273, Loss: 0.6994\n",
            "Epoch: 006, Step: 274, Loss: 0.6884\n",
            "Epoch: 006, Step: 275, Loss: 0.7239\n",
            "Epoch: 006, Step: 276, Loss: 0.7265\n",
            "Epoch: 006, Step: 277, Loss: 0.7309\n",
            "Epoch: 006, Step: 278, Loss: 0.6947\n",
            "Epoch: 006, Step: 279, Loss: 0.6550\n",
            "Epoch: 006, Step: 280, Loss: 0.7001\n",
            "Epoch: 006, Step: 281, Loss: 0.6656\n",
            "Epoch: 006, Step: 282, Loss: 0.6979\n",
            "Epoch: 006, Step: 283, Loss: 0.7228\n",
            "Epoch: 006, Step: 284, Loss: 0.6974\n",
            "Epoch: 006, Step: 285, Loss: 0.6975\n",
            "Epoch: 006, Step: 286, Loss: 0.6954\n",
            "Epoch: 006, Step: 287, Loss: 0.6955\n",
            "Epoch: 006, Step: 288, Loss: 0.6657\n",
            "Epoch: 006, Step: 289, Loss: 0.6893\n",
            "Epoch: 006, Step: 290, Loss: 0.7277\n",
            "Epoch: 006, Step: 291, Loss: 0.6988\n",
            "Epoch: 006, Step: 292, Loss: 0.6983\n",
            "Epoch: 006, Step: 293, Loss: 0.6954\n",
            "Epoch: 006, Step: 294, Loss: 0.7325\n",
            "Epoch: 006, Step: 295, Loss: 0.7002\n",
            "Epoch: 006, Step: 296, Loss: 0.6970\n",
            "Epoch: 006, Step: 297, Loss: 0.6837\n",
            "Epoch: 006, Step: 298, Loss: 0.7283\n",
            "Epoch: 006, Step: 299, Loss: 0.7298\n",
            "Epoch: 006, Step: 300, Loss: 0.6890\n",
            "Epoch: 006, Step: 301, Loss: 0.6607\n",
            "Epoch: 006, Step: 302, Loss: 0.7005\n",
            "Epoch: 006, Step: 303, Loss: 0.6948\n",
            "Epoch: 006, Step: 304, Loss: 0.6942\n",
            "Epoch: 006, Step: 305, Loss: 0.7272\n",
            "Epoch: 006, Step: 306, Loss: 0.6568\n",
            "Epoch: 006, Step: 307, Loss: 0.7383\n",
            "Epoch: 006, Step: 308, Loss: 0.7261\n",
            "Epoch: 006, Step: 309, Loss: 0.6924\n",
            "Epoch: 006, Step: 310, Loss: 0.7268\n",
            "Epoch: 006, Step: 311, Loss: 0.7196\n",
            "Epoch: 006, Step: 312, Loss: 0.6967\n",
            "Epoch: 006, Step: 313, Loss: 0.6967\n",
            "Epoch: 006, Step: 314, Loss: 0.6609\n",
            "Epoch: 006, Step: 315, Loss: 0.6946\n",
            "Epoch: 006, Step: 316, Loss: 0.7238\n",
            "Epoch: 006, Step: 317, Loss: 0.7342\n",
            "Epoch: 006, Step: 318, Loss: 0.7304\n",
            "Epoch: 006, Step: 319, Loss: 0.6921\n",
            "Epoch: 006, Step: 320, Loss: 0.6984\n",
            "Epoch: 006, Step: 321, Loss: 0.6894\n",
            "Epoch: 006, Step: 322, Loss: 0.6928\n",
            "Epoch: 006, Step: 323, Loss: 0.6616\n",
            "Epoch: 006, Step: 324, Loss: 0.6936\n",
            "Epoch: 006, Step: 325, Loss: 0.7238\n",
            "Epoch: 006, Step: 326, Loss: 0.6518\n",
            "Epoch: 006, Step: 327, Loss: 0.6936\n",
            "Epoch: 006, Step: 328, Loss: 0.7000\n",
            "Epoch: 006, Step: 329, Loss: 0.7265\n",
            "Epoch: 006, Step: 330, Loss: 0.6582\n",
            "Epoch: 006, Step: 331, Loss: 0.7202\n",
            "Epoch: 006, Step: 332, Loss: 0.6905\n",
            "Epoch: 006, Step: 333, Loss: 0.7273\n",
            "Epoch: 006, Step: 334, Loss: 0.6971\n",
            "Epoch: 006, Step: 335, Loss: 0.6623\n",
            "Epoch: 006, Step: 336, Loss: 0.7252\n",
            "Epoch: 006, Step: 337, Loss: 0.6921\n",
            "Epoch: 006, Step: 338, Loss: 0.7257\n",
            "Epoch: 006, Step: 339, Loss: 0.6914\n",
            "Epoch: 006, Step: 340, Loss: 0.6607\n",
            "Epoch: 006, Step: 341, Loss: 0.6994\n",
            "Epoch: 006, Step: 342, Loss: 0.6548\n",
            "Epoch: 006, Step: 343, Loss: 0.6638\n",
            "Epoch: 006, Step: 344, Loss: 0.6892\n",
            "Epoch: 006, Step: 345, Loss: 0.6553\n",
            "Epoch: 006, Step: 346, Loss: 0.6973\n",
            "Epoch: 006, Step: 347, Loss: 0.7269\n",
            "Epoch: 006, Step: 348, Loss: 0.7319\n",
            "Epoch: 006, Step: 349, Loss: 0.7265\n",
            "Epoch: 006, Step: 350, Loss: 0.6916\n",
            "Epoch: 006, Step: 351, Loss: 0.7290\n",
            "Epoch: 006, Step: 352, Loss: 0.6938\n",
            "Epoch: 006, Step: 353, Loss: 0.6907\n",
            "Epoch: 006, Step: 354, Loss: 0.7324\n",
            "Epoch: 006, Step: 355, Loss: 0.6907\n",
            "Epoch: 006, Step: 356, Loss: 0.6902\n",
            "Epoch: 006, Step: 357, Loss: 0.7226\n",
            "Epoch: 006, Step: 358, Loss: 0.6571\n",
            "Epoch: 006, Step: 359, Loss: 0.7264\n",
            "Epoch: 006, Step: 360, Loss: 0.7244\n",
            "Epoch: 006, Step: 361, Loss: 0.7258\n",
            "Epoch: 006, Step: 362, Loss: 0.6966\n",
            "Epoch: 006, Step: 363, Loss: 0.6959\n",
            "Epoch: 006, Step: 364, Loss: 0.7269\n",
            "Epoch: 006, Step: 365, Loss: 0.7267\n",
            "Epoch: 006, Step: 366, Loss: 0.7257\n",
            "Epoch: 006, Step: 367, Loss: 0.6661\n",
            "Epoch: 006, Step: 368, Loss: 0.6625\n",
            "Epoch: 006, Step: 369, Loss: 0.7185\n",
            "Epoch: 006, Step: 370, Loss: 0.7257\n",
            "Epoch: 006, Step: 371, Loss: 0.6963\n",
            "Epoch: 006, Step: 372, Loss: 0.6907\n",
            "Epoch: 006, Step: 373, Loss: 0.6970\n",
            "Epoch: 006, Step: 374, Loss: 0.6960\n",
            "Epoch: 006, Step: 375, Loss: 0.7350\n",
            "Epoch: 006, Step: 376, Loss: 0.7311\n",
            "Epoch: 006, Step: 377, Loss: 0.7198\n",
            "Epoch: 006, Step: 378, Loss: 0.7274\n",
            "Epoch: 006, Step: 379, Loss: 0.6620\n",
            "Epoch: 006, Step: 380, Loss: 0.7247\n",
            "Epoch: 006, Step: 381, Loss: 0.6660\n",
            "Epoch: 006, Step: 382, Loss: 0.7229\n",
            "Epoch: 006, Step: 383, Loss: 0.6920\n",
            "Epoch: 006, Step: 384, Loss: 0.7025\n",
            "Epoch: 006, Step: 385, Loss: 0.6930\n",
            "Epoch: 006, Step: 386, Loss: 0.6608\n",
            "Epoch: 006, Step: 387, Loss: 0.7285\n",
            "Epoch: 006, Step: 388, Loss: 0.6935\n",
            "Epoch: 006, Step: 389, Loss: 0.7303\n",
            "Epoch: 006, Step: 390, Loss: 0.6591\n",
            "Epoch: 006, Step: 391, Loss: 0.6963\n",
            "Epoch: 006, Step: 392, Loss: 0.6940\n",
            "Epoch: 006, Step: 393, Loss: 0.6954\n",
            "Epoch: 006, Step: 394, Loss: 0.6934\n",
            "Epoch: 006, Step: 395, Loss: 0.6961\n",
            "Epoch: 006, Step: 396, Loss: 0.6974\n",
            "Epoch: 006, Step: 397, Loss: 0.7299\n",
            "Epoch: 006, Step: 398, Loss: 0.7244\n",
            "Epoch: 006, Step: 399, Loss: 0.6897\n",
            "Epoch: 006, Step: 400, Loss: 0.6629\n",
            "Epoch: 006, Step: 401, Loss: 0.7321\n",
            "Epoch: 006, Step: 402, Loss: 0.6985\n",
            "Epoch: 006, Step: 403, Loss: 0.6680\n",
            "Epoch: 006, Step: 404, Loss: 0.6950\n",
            "Epoch: 006, Step: 405, Loss: 0.7288\n",
            "Epoch: 006, Step: 406, Loss: 0.6949\n",
            "Epoch: 006, Step: 407, Loss: 0.6933\n",
            "Epoch: 006, Step: 408, Loss: 0.7281\n",
            "Epoch: 006, Step: 409, Loss: 0.7231\n",
            "Epoch: 006, Step: 410, Loss: 0.6919\n",
            "Epoch: 006, Step: 411, Loss: 0.7260\n",
            "Epoch: 006, Step: 412, Loss: 0.7324\n",
            "Epoch: 006, Step: 000, Val Loss: 0.6933\n",
            "Epoch: 006, Step: 001, Val Loss: 0.7267\n",
            "Epoch: 006, Step: 002, Val Loss: 0.6948\n",
            "Epoch: 006, Step: 003, Val Loss: 0.7265\n",
            "Epoch: 006, Step: 004, Val Loss: 0.6945\n",
            "Epoch: 006, Step: 005, Val Loss: 0.6939\n",
            "Epoch: 006, Step: 006, Val Loss: 0.7266\n",
            "Epoch: 006, Step: 007, Val Loss: 0.6944\n",
            "Epoch: 006, Step: 008, Val Loss: 0.7276\n",
            "Epoch: 006, Step: 009, Val Loss: 0.6950\n",
            "Epoch: 006, Step: 010, Val Loss: 0.6945\n",
            "Epoch: 006, Step: 011, Val Loss: 0.7267\n",
            "Epoch: 006, Step: 012, Val Loss: 0.6926\n",
            "Epoch: 006, Step: 013, Val Loss: 0.6611\n",
            "Epoch: 006, Step: 014, Val Loss: 0.6919\n",
            "Epoch: 006, Step: 015, Val Loss: 0.7261\n",
            "Epoch: 006, Step: 016, Val Loss: 0.6941\n",
            "Epoch: 006, Step: 017, Val Loss: 0.7266\n",
            "Epoch: 006, Step: 018, Val Loss: 0.6934\n",
            "Epoch: 006, Step: 019, Val Loss: 0.6937\n",
            "Epoch: 006, Step: 020, Val Loss: 0.7263\n",
            "Epoch: 006, Step: 021, Val Loss: 0.6611\n",
            "Epoch: 006, Step: 022, Val Loss: 0.7263\n",
            "Epoch: 006, Step: 023, Val Loss: 0.6948\n",
            "Epoch: 006, Step: 024, Val Loss: 0.7263\n",
            "Epoch: 006, Step: 025, Val Loss: 0.7261\n",
            "Epoch: 006, Step: 026, Val Loss: 0.6592\n",
            "Epoch: 006, Step: 027, Val Loss: 0.6930\n",
            "Epoch: 006, Step: 028, Val Loss: 0.6939\n",
            "Epoch: 006, Step: 029, Val Loss: 0.6934\n",
            "Epoch: 006, Step: 030, Val Loss: 0.6935\n",
            "Epoch: 006, Step: 031, Val Loss: 0.6925\n",
            "Epoch: 006, Step: 032, Val Loss: 0.6926\n",
            "Epoch: 006, Step: 033, Val Loss: 0.6939\n",
            "Epoch: 006, Step: 034, Val Loss: 0.6944\n",
            "Epoch: 006, Step: 035, Val Loss: 0.6623\n",
            "Epoch: 006, Step: 036, Val Loss: 0.6946\n",
            "Epoch: 006, Step: 037, Val Loss: 0.7261\n",
            "Epoch: 006, Step: 038, Val Loss: 0.6935\n",
            "Epoch: 006, Step: 039, Val Loss: 0.7263\n",
            "Epoch: 006, Step: 040, Val Loss: 0.6925\n",
            "Epoch: 006, Step: 041, Val Loss: 0.6948\n",
            "Epoch: 006, Step: 042, Val Loss: 0.6937\n",
            "Epoch: 006, Step: 043, Val Loss: 0.6940\n",
            "Epoch: 006, Step: 044, Val Loss: 0.7258\n",
            "Epoch: 006, Step: 045, Val Loss: 0.7257\n",
            "Epoch: 006, Step: 046, Val Loss: 0.6944\n",
            "Epoch: 006, Step: 047, Val Loss: 0.6612\n",
            "Epoch: 006, Step: 048, Val Loss: 0.7272\n",
            "Epoch: 006, Step: 049, Val Loss: 0.6940\n",
            "Epoch: 006, Step: 050, Val Loss: 0.6916\n",
            "Epoch: 006, Step: 051, Val Loss: 0.6934\n",
            "Epoch: 006, Step: 052, Val Loss: 0.7251\n",
            "Epoch: 006, Step: 053, Val Loss: 0.6605\n",
            "Epoch: 006, Step: 054, Val Loss: 0.6618\n",
            "Epoch: 006, Step: 055, Val Loss: 0.6611\n",
            "Epoch: 006, Step: 056, Val Loss: 0.6923\n",
            "Epoch: 006, Step: 057, Val Loss: 0.6594\n",
            "Epoch: 006, Step: 058, Val Loss: 0.6934\n",
            "Epoch: 006, Step: 059, Val Loss: 0.6941\n",
            "Epoch: 006, Step: 060, Val Loss: 0.6928\n",
            "Epoch: 006, Step: 061, Val Loss: 0.7266\n",
            "Epoch: 006, Step: 062, Val Loss: 0.6941\n",
            "Epoch: 006, Step: 063, Val Loss: 0.6935\n",
            "Epoch: 006, Step: 064, Val Loss: 0.6935\n",
            "Epoch: 006, Step: 065, Val Loss: 0.7249\n",
            "Epoch: 006, Step: 066, Val Loss: 0.7271\n",
            "Epoch: 006, Step: 067, Val Loss: 0.7258\n",
            "Epoch: 006, Step: 068, Val Loss: 0.6617\n",
            "Epoch: 006, Step: 069, Val Loss: 0.6937\n",
            "Epoch: 006, Step: 070, Val Loss: 0.7253\n",
            "Epoch: 006, Step: 071, Val Loss: 0.7263\n",
            "Epoch: 006, Step: 072, Val Loss: 0.6609\n",
            "Epoch: 006, Step: 073, Val Loss: 0.6928\n",
            "Epoch: 006, Step: 074, Val Loss: 0.6933\n",
            "Epoch: 006, Step: 075, Val Loss: 0.7262\n",
            "Epoch: 006, Step: 076, Val Loss: 0.7263\n",
            "Epoch: 006, Step: 077, Val Loss: 0.6925\n",
            "Epoch: 006, Step: 078, Val Loss: 0.6928\n",
            "Epoch: 006, Step: 079, Val Loss: 0.6604\n",
            "Epoch: 006, Step: 080, Val Loss: 0.7267\n",
            "Epoch: 006, Step: 081, Val Loss: 0.6940\n",
            "Epoch: 006, Step: 082, Val Loss: 0.6937\n",
            "Epoch: 006, Step: 083, Val Loss: 0.6942\n",
            "Epoch: 006, Step: 084, Val Loss: 0.6938\n",
            "Epoch: 006, Step: 085, Val Loss: 0.7252\n",
            "Epoch: 006, Step: 086, Val Loss: 0.6949\n",
            "Epoch: 006, Step: 087, Val Loss: 0.6933\n",
            "Epoch: 006, Step: 088, Val Loss: 0.6638\n",
            "Epoch: 006, Step: 089, Val Loss: 0.6939\n",
            "Epoch: 006, Step: 090, Val Loss: 0.6928\n",
            "Epoch: 006, Step: 091, Val Loss: 0.7261\n",
            "Epoch: 006, Step: 092, Val Loss: 0.7265\n",
            "Epoch: 006, Step: 093, Val Loss: 0.6931\n",
            "Epoch: 006, Step: 094, Val Loss: 0.7266\n",
            "Epoch: 006, Step: 095, Val Loss: 0.6929\n",
            "Epoch: 006, Step: 096, Val Loss: 0.7269\n",
            "Epoch: 006, Step: 097, Val Loss: 0.6594\n",
            "Epoch: 006, Step: 098, Val Loss: 0.7245\n",
            "Epoch: 006, Step: 099, Val Loss: 0.6583\n",
            "Epoch: 006, Step: 100, Val Loss: 0.6932\n",
            "Epoch: 006, Step: 101, Val Loss: 0.7261\n",
            "Epoch: 006, Step: 102, Val Loss: 0.7245\n",
            "Epoch: 006, Step: 103, Val Loss: 0.6939\n",
            "Epoch: 006, Step: 104, Val Loss: 0.7266\n",
            "Epoch: 006, Step: 105, Val Loss: 0.7269\n",
            "Epoch: 006, Step: 106, Val Loss: 0.7260\n",
            "Epoch: 006, Step: 107, Val Loss: 0.6934\n",
            "Epoch: 006, Step: 108, Val Loss: 0.7259\n",
            "Epoch: 006, Step: 109, Val Loss: 0.6614\n",
            "Epoch: 006, Step: 110, Val Loss: 0.6939\n",
            "Epoch: 006, Step: 111, Val Loss: 0.6936\n",
            "Epoch: 006, Step: 112, Val Loss: 0.6923\n",
            "Epoch: 006, Step: 113, Val Loss: 0.6931\n",
            "Epoch: 006, Step: 114, Val Loss: 0.7266\n",
            "Epoch: 006, Step: 115, Val Loss: 0.7264\n",
            "Epoch: 006, Step: 116, Val Loss: 0.6945\n",
            "Epoch: 006, Step: 117, Val Loss: 0.6605\n",
            "Epoch: 007, Step: 000, Loss: 0.7291\n",
            "Epoch: 007, Step: 001, Loss: 0.6857\n",
            "Epoch: 007, Step: 002, Loss: 0.6900\n",
            "Epoch: 007, Step: 003, Loss: 0.6872\n",
            "Epoch: 007, Step: 004, Loss: 0.6937\n",
            "Epoch: 007, Step: 005, Loss: 0.7199\n",
            "Epoch: 007, Step: 006, Loss: 0.6965\n",
            "Epoch: 007, Step: 007, Loss: 0.6935\n",
            "Epoch: 007, Step: 008, Loss: 0.7006\n",
            "Epoch: 007, Step: 009, Loss: 0.6910\n",
            "Epoch: 007, Step: 010, Loss: 0.6900\n",
            "Epoch: 007, Step: 011, Loss: 0.6611\n",
            "Epoch: 007, Step: 012, Loss: 0.6900\n",
            "Epoch: 007, Step: 013, Loss: 0.7332\n",
            "Epoch: 007, Step: 014, Loss: 0.7256\n",
            "Epoch: 007, Step: 015, Loss: 0.6560\n",
            "Epoch: 007, Step: 016, Loss: 0.7262\n",
            "Epoch: 007, Step: 017, Loss: 0.6951\n",
            "Epoch: 007, Step: 018, Loss: 0.6930\n",
            "Epoch: 007, Step: 019, Loss: 0.6957\n",
            "Epoch: 007, Step: 020, Loss: 0.7256\n",
            "Epoch: 007, Step: 021, Loss: 0.6932\n",
            "Epoch: 007, Step: 022, Loss: 0.6886\n",
            "Epoch: 007, Step: 023, Loss: 0.6895\n",
            "Epoch: 007, Step: 024, Loss: 0.7298\n",
            "Epoch: 007, Step: 025, Loss: 0.6618\n",
            "Epoch: 007, Step: 026, Loss: 0.6579\n",
            "Epoch: 007, Step: 027, Loss: 0.6967\n",
            "Epoch: 007, Step: 028, Loss: 0.6903\n",
            "Epoch: 007, Step: 029, Loss: 0.6903\n",
            "Epoch: 007, Step: 030, Loss: 0.6618\n",
            "Epoch: 007, Step: 031, Loss: 0.7236\n",
            "Epoch: 007, Step: 032, Loss: 0.6887\n",
            "Epoch: 007, Step: 033, Loss: 0.6920\n",
            "Epoch: 007, Step: 034, Loss: 0.6919\n",
            "Epoch: 007, Step: 035, Loss: 0.6580\n",
            "Epoch: 007, Step: 036, Loss: 0.6898\n",
            "Epoch: 007, Step: 037, Loss: 0.6641\n",
            "Epoch: 007, Step: 038, Loss: 0.7266\n",
            "Epoch: 007, Step: 039, Loss: 0.6941\n",
            "Epoch: 007, Step: 040, Loss: 0.6980\n",
            "Epoch: 007, Step: 041, Loss: 0.7284\n",
            "Epoch: 007, Step: 042, Loss: 0.6932\n",
            "Epoch: 007, Step: 043, Loss: 0.6964\n",
            "Epoch: 007, Step: 044, Loss: 0.7278\n",
            "Epoch: 007, Step: 045, Loss: 0.7323\n",
            "Epoch: 007, Step: 046, Loss: 0.6950\n",
            "Epoch: 007, Step: 047, Loss: 0.6903\n",
            "Epoch: 007, Step: 048, Loss: 0.6908\n",
            "Epoch: 007, Step: 049, Loss: 0.6610\n",
            "Epoch: 007, Step: 050, Loss: 0.7220\n",
            "Epoch: 007, Step: 051, Loss: 0.6927\n",
            "Epoch: 007, Step: 052, Loss: 0.6925\n",
            "Epoch: 007, Step: 053, Loss: 0.7251\n",
            "Epoch: 007, Step: 054, Loss: 0.6569\n",
            "Epoch: 007, Step: 055, Loss: 0.7201\n",
            "Epoch: 007, Step: 056, Loss: 0.7295\n",
            "Epoch: 007, Step: 057, Loss: 0.6885\n",
            "Epoch: 007, Step: 058, Loss: 0.6627\n",
            "Epoch: 007, Step: 059, Loss: 0.6908\n",
            "Epoch: 007, Step: 060, Loss: 0.6952\n",
            "Epoch: 007, Step: 061, Loss: 0.6568\n",
            "Epoch: 007, Step: 062, Loss: 0.6946\n",
            "Epoch: 007, Step: 063, Loss: 0.7227\n",
            "Epoch: 007, Step: 064, Loss: 0.6625\n",
            "Epoch: 007, Step: 065, Loss: 0.7246\n",
            "Epoch: 007, Step: 066, Loss: 0.7275\n",
            "Epoch: 007, Step: 067, Loss: 0.7230\n",
            "Epoch: 007, Step: 068, Loss: 0.7195\n",
            "Epoch: 007, Step: 069, Loss: 0.7308\n",
            "Epoch: 007, Step: 070, Loss: 0.7048\n",
            "Epoch: 007, Step: 071, Loss: 0.6964\n",
            "Epoch: 007, Step: 072, Loss: 0.6964\n",
            "Epoch: 007, Step: 073, Loss: 0.7284\n",
            "Epoch: 007, Step: 074, Loss: 0.7220\n",
            "Epoch: 007, Step: 075, Loss: 0.7264\n",
            "Epoch: 007, Step: 076, Loss: 0.7303\n",
            "Epoch: 007, Step: 077, Loss: 0.7281\n",
            "Epoch: 007, Step: 078, Loss: 0.6980\n",
            "Epoch: 007, Step: 079, Loss: 0.6977\n",
            "Epoch: 007, Step: 080, Loss: 0.6904\n",
            "Epoch: 007, Step: 081, Loss: 0.6601\n",
            "Epoch: 007, Step: 082, Loss: 0.6967\n",
            "Epoch: 007, Step: 083, Loss: 0.7309\n",
            "Epoch: 007, Step: 084, Loss: 0.6926\n",
            "Epoch: 007, Step: 085, Loss: 0.7256\n",
            "Epoch: 007, Step: 086, Loss: 0.7257\n",
            "Epoch: 007, Step: 087, Loss: 0.6571\n",
            "Epoch: 007, Step: 088, Loss: 0.7323\n",
            "Epoch: 007, Step: 089, Loss: 0.6875\n",
            "Epoch: 007, Step: 090, Loss: 0.6964\n",
            "Epoch: 007, Step: 091, Loss: 0.7234\n",
            "Epoch: 007, Step: 092, Loss: 0.7265\n",
            "Epoch: 007, Step: 093, Loss: 0.7266\n",
            "Epoch: 007, Step: 094, Loss: 0.6997\n",
            "Epoch: 007, Step: 095, Loss: 0.7299\n",
            "Epoch: 007, Step: 096, Loss: 0.7248\n",
            "Epoch: 007, Step: 097, Loss: 0.6880\n",
            "Epoch: 007, Step: 098, Loss: 0.7215\n",
            "Epoch: 007, Step: 099, Loss: 0.7274\n",
            "Epoch: 007, Step: 100, Loss: 0.7219\n",
            "Epoch: 007, Step: 101, Loss: 0.7301\n",
            "Epoch: 007, Step: 102, Loss: 0.7217\n",
            "Epoch: 007, Step: 103, Loss: 0.6934\n",
            "Epoch: 007, Step: 104, Loss: 0.6866\n",
            "Epoch: 007, Step: 105, Loss: 0.6811\n",
            "Epoch: 007, Step: 106, Loss: 0.6923\n",
            "Epoch: 007, Step: 107, Loss: 0.6889\n",
            "Epoch: 007, Step: 108, Loss: 0.6595\n",
            "Epoch: 007, Step: 109, Loss: 0.7220\n",
            "Epoch: 007, Step: 110, Loss: 0.7240\n",
            "Epoch: 007, Step: 111, Loss: 0.6909\n",
            "Epoch: 007, Step: 112, Loss: 0.6905\n",
            "Epoch: 007, Step: 113, Loss: 0.6658\n",
            "Epoch: 007, Step: 114, Loss: 0.6880\n",
            "Epoch: 007, Step: 115, Loss: 0.7260\n",
            "Epoch: 007, Step: 116, Loss: 0.7224\n",
            "Epoch: 007, Step: 117, Loss: 0.7257\n",
            "Epoch: 007, Step: 118, Loss: 0.6590\n",
            "Epoch: 007, Step: 119, Loss: 0.7247\n",
            "Epoch: 007, Step: 120, Loss: 0.6942\n",
            "Epoch: 007, Step: 121, Loss: 0.7375\n",
            "Epoch: 007, Step: 122, Loss: 0.7274\n",
            "Epoch: 007, Step: 123, Loss: 0.6656\n",
            "Epoch: 007, Step: 124, Loss: 0.6570\n",
            "Epoch: 007, Step: 125, Loss: 0.6870\n",
            "Epoch: 007, Step: 126, Loss: 0.7277\n",
            "Epoch: 007, Step: 127, Loss: 0.6941\n",
            "Epoch: 007, Step: 128, Loss: 0.6914\n",
            "Epoch: 007, Step: 129, Loss: 0.6927\n",
            "Epoch: 007, Step: 130, Loss: 0.6977\n",
            "Epoch: 007, Step: 131, Loss: 0.7304\n",
            "Epoch: 007, Step: 132, Loss: 0.7200\n",
            "Epoch: 007, Step: 133, Loss: 0.7278\n",
            "Epoch: 007, Step: 134, Loss: 0.6590\n",
            "Epoch: 007, Step: 135, Loss: 0.6911\n",
            "Epoch: 007, Step: 136, Loss: 0.7311\n",
            "Epoch: 007, Step: 137, Loss: 0.6856\n",
            "Epoch: 007, Step: 138, Loss: 0.7286\n",
            "Epoch: 007, Step: 139, Loss: 0.7033\n",
            "Epoch: 007, Step: 140, Loss: 0.6988\n",
            "Epoch: 007, Step: 141, Loss: 0.6982\n",
            "Epoch: 007, Step: 142, Loss: 0.6624\n",
            "Epoch: 007, Step: 143, Loss: 0.6981\n",
            "Epoch: 007, Step: 144, Loss: 0.7280\n",
            "Epoch: 007, Step: 145, Loss: 0.6919\n",
            "Epoch: 007, Step: 146, Loss: 0.6852\n",
            "Epoch: 007, Step: 147, Loss: 0.6986\n",
            "Epoch: 007, Step: 148, Loss: 0.6900\n",
            "Epoch: 007, Step: 149, Loss: 0.7324\n",
            "Epoch: 007, Step: 150, Loss: 0.7310\n",
            "Epoch: 007, Step: 151, Loss: 0.6946\n",
            "Epoch: 007, Step: 152, Loss: 0.7293\n",
            "Epoch: 007, Step: 153, Loss: 0.7020\n",
            "Epoch: 007, Step: 154, Loss: 0.7226\n",
            "Epoch: 007, Step: 155, Loss: 0.6942\n",
            "Epoch: 007, Step: 156, Loss: 0.6941\n",
            "Epoch: 007, Step: 157, Loss: 0.6613\n",
            "Epoch: 007, Step: 158, Loss: 0.7306\n",
            "Epoch: 007, Step: 159, Loss: 0.6882\n",
            "Epoch: 007, Step: 160, Loss: 0.6974\n",
            "Epoch: 007, Step: 161, Loss: 0.6968\n",
            "Epoch: 007, Step: 162, Loss: 0.6927\n",
            "Epoch: 007, Step: 163, Loss: 0.6587\n",
            "Epoch: 007, Step: 164, Loss: 0.6658\n",
            "Epoch: 007, Step: 165, Loss: 0.6972\n",
            "Epoch: 007, Step: 166, Loss: 0.6938\n",
            "Epoch: 007, Step: 167, Loss: 0.6927\n",
            "Epoch: 007, Step: 168, Loss: 0.7248\n",
            "Epoch: 007, Step: 169, Loss: 0.6686\n",
            "Epoch: 007, Step: 170, Loss: 0.7331\n",
            "Epoch: 007, Step: 171, Loss: 0.6655\n",
            "Epoch: 007, Step: 172, Loss: 0.6914\n",
            "Epoch: 007, Step: 173, Loss: 0.6953\n",
            "Epoch: 007, Step: 174, Loss: 0.7289\n",
            "Epoch: 007, Step: 175, Loss: 0.7220\n",
            "Epoch: 007, Step: 176, Loss: 0.6956\n",
            "Epoch: 007, Step: 177, Loss: 0.6894\n",
            "Epoch: 007, Step: 178, Loss: 0.6929\n",
            "Epoch: 007, Step: 179, Loss: 0.6949\n",
            "Epoch: 007, Step: 180, Loss: 0.7241\n",
            "Epoch: 007, Step: 181, Loss: 0.6953\n",
            "Epoch: 007, Step: 182, Loss: 0.6848\n",
            "Epoch: 007, Step: 183, Loss: 0.7245\n",
            "Epoch: 007, Step: 184, Loss: 0.6941\n",
            "Epoch: 007, Step: 185, Loss: 0.6951\n",
            "Epoch: 007, Step: 186, Loss: 0.7296\n",
            "Epoch: 007, Step: 187, Loss: 0.6938\n",
            "Epoch: 007, Step: 188, Loss: 0.7217\n",
            "Epoch: 007, Step: 189, Loss: 0.7207\n",
            "Epoch: 007, Step: 190, Loss: 0.6894\n",
            "Epoch: 007, Step: 191, Loss: 0.6868\n",
            "Epoch: 007, Step: 192, Loss: 0.6961\n",
            "Epoch: 007, Step: 193, Loss: 0.6589\n",
            "Epoch: 007, Step: 194, Loss: 0.7246\n",
            "Epoch: 007, Step: 195, Loss: 0.7336\n",
            "Epoch: 007, Step: 196, Loss: 0.6947\n",
            "Epoch: 007, Step: 197, Loss: 0.6963\n",
            "Epoch: 007, Step: 198, Loss: 0.7190\n",
            "Epoch: 007, Step: 199, Loss: 0.7230\n",
            "Epoch: 007, Step: 200, Loss: 0.6889\n",
            "Epoch: 007, Step: 201, Loss: 0.6886\n",
            "Epoch: 007, Step: 202, Loss: 0.6667\n",
            "Epoch: 007, Step: 203, Loss: 0.6952\n",
            "Epoch: 007, Step: 204, Loss: 0.6979\n",
            "Epoch: 007, Step: 205, Loss: 0.6965\n",
            "Epoch: 007, Step: 206, Loss: 0.6543\n",
            "Epoch: 007, Step: 207, Loss: 0.7247\n",
            "Epoch: 007, Step: 208, Loss: 0.6961\n",
            "Epoch: 007, Step: 209, Loss: 0.6828\n",
            "Epoch: 007, Step: 210, Loss: 0.6602\n",
            "Epoch: 007, Step: 211, Loss: 0.7239\n",
            "Epoch: 007, Step: 212, Loss: 0.7178\n",
            "Epoch: 007, Step: 213, Loss: 0.6895\n",
            "Epoch: 007, Step: 214, Loss: 0.6994\n",
            "Epoch: 007, Step: 215, Loss: 0.6935\n",
            "Epoch: 007, Step: 216, Loss: 0.6969\n",
            "Epoch: 007, Step: 217, Loss: 0.6960\n",
            "Epoch: 007, Step: 218, Loss: 0.6540\n",
            "Epoch: 007, Step: 219, Loss: 0.6938\n",
            "Epoch: 007, Step: 220, Loss: 0.7276\n",
            "Epoch: 007, Step: 221, Loss: 0.7210\n",
            "Epoch: 007, Step: 222, Loss: 0.7271\n",
            "Epoch: 007, Step: 223, Loss: 0.6663\n",
            "Epoch: 007, Step: 224, Loss: 0.6940\n",
            "Epoch: 007, Step: 225, Loss: 0.6621\n",
            "Epoch: 007, Step: 226, Loss: 0.7291\n",
            "Epoch: 007, Step: 227, Loss: 0.6943\n",
            "Epoch: 007, Step: 228, Loss: 0.6948\n",
            "Epoch: 007, Step: 229, Loss: 0.6582\n",
            "Epoch: 007, Step: 230, Loss: 0.6551\n",
            "Epoch: 007, Step: 231, Loss: 0.6882\n",
            "Epoch: 007, Step: 232, Loss: 0.6618\n",
            "Epoch: 007, Step: 233, Loss: 0.7240\n",
            "Epoch: 007, Step: 234, Loss: 0.6911\n",
            "Epoch: 007, Step: 235, Loss: 0.6527\n",
            "Epoch: 007, Step: 236, Loss: 0.6838\n",
            "Epoch: 007, Step: 237, Loss: 0.6928\n",
            "Epoch: 007, Step: 238, Loss: 0.6890\n",
            "Epoch: 007, Step: 239, Loss: 0.7325\n",
            "Epoch: 007, Step: 240, Loss: 0.6952\n",
            "Epoch: 007, Step: 241, Loss: 0.6933\n",
            "Epoch: 007, Step: 242, Loss: 0.6697\n",
            "Epoch: 007, Step: 243, Loss: 0.6911\n",
            "Epoch: 007, Step: 244, Loss: 0.6946\n",
            "Epoch: 007, Step: 245, Loss: 0.6969\n",
            "Epoch: 007, Step: 246, Loss: 0.7266\n",
            "Epoch: 007, Step: 247, Loss: 0.6649\n",
            "Epoch: 007, Step: 248, Loss: 0.6916\n",
            "Epoch: 007, Step: 249, Loss: 0.6595\n",
            "Epoch: 007, Step: 250, Loss: 0.6933\n",
            "Epoch: 007, Step: 251, Loss: 0.6603\n",
            "Epoch: 007, Step: 252, Loss: 0.7280\n",
            "Epoch: 007, Step: 253, Loss: 0.7265\n",
            "Epoch: 007, Step: 254, Loss: 0.6969\n",
            "Epoch: 007, Step: 255, Loss: 0.6904\n",
            "Epoch: 007, Step: 256, Loss: 0.6880\n",
            "Epoch: 007, Step: 257, Loss: 0.7245\n",
            "Epoch: 007, Step: 258, Loss: 0.6933\n",
            "Epoch: 007, Step: 259, Loss: 0.6671\n",
            "Epoch: 007, Step: 260, Loss: 0.6928\n",
            "Epoch: 007, Step: 261, Loss: 0.7271\n",
            "Epoch: 007, Step: 262, Loss: 0.6605\n",
            "Epoch: 007, Step: 263, Loss: 0.6648\n",
            "Epoch: 007, Step: 264, Loss: 0.6922\n",
            "Epoch: 007, Step: 265, Loss: 0.7200\n",
            "Epoch: 007, Step: 266, Loss: 0.7014\n",
            "Epoch: 007, Step: 267, Loss: 0.6628\n",
            "Epoch: 007, Step: 268, Loss: 0.7279\n",
            "Epoch: 007, Step: 269, Loss: 0.6878\n",
            "Epoch: 007, Step: 270, Loss: 0.6893\n",
            "Epoch: 007, Step: 271, Loss: 0.7037\n",
            "Epoch: 007, Step: 272, Loss: 0.7178\n",
            "Epoch: 007, Step: 273, Loss: 0.7186\n",
            "Epoch: 007, Step: 274, Loss: 0.6641\n",
            "Epoch: 007, Step: 275, Loss: 0.6933\n",
            "Epoch: 007, Step: 276, Loss: 0.6952\n",
            "Epoch: 007, Step: 277, Loss: 0.6628\n",
            "Epoch: 007, Step: 278, Loss: 0.6933\n",
            "Epoch: 007, Step: 279, Loss: 0.6916\n",
            "Epoch: 007, Step: 280, Loss: 0.7239\n",
            "Epoch: 007, Step: 281, Loss: 0.7274\n",
            "Epoch: 007, Step: 282, Loss: 0.7016\n",
            "Epoch: 007, Step: 283, Loss: 0.7225\n",
            "Epoch: 007, Step: 284, Loss: 0.7237\n",
            "Epoch: 007, Step: 285, Loss: 0.6963\n",
            "Epoch: 007, Step: 286, Loss: 0.7231\n",
            "Epoch: 007, Step: 287, Loss: 0.6663\n",
            "Epoch: 007, Step: 288, Loss: 0.7226\n",
            "Epoch: 007, Step: 289, Loss: 0.6977\n",
            "Epoch: 007, Step: 290, Loss: 0.6945\n",
            "Epoch: 007, Step: 291, Loss: 0.7233\n",
            "Epoch: 007, Step: 292, Loss: 0.6968\n",
            "Epoch: 007, Step: 293, Loss: 0.6890\n",
            "Epoch: 007, Step: 294, Loss: 0.7253\n",
            "Epoch: 007, Step: 295, Loss: 0.7222\n",
            "Epoch: 007, Step: 296, Loss: 0.7260\n",
            "Epoch: 007, Step: 297, Loss: 0.7223\n",
            "Epoch: 007, Step: 298, Loss: 0.7018\n",
            "Epoch: 007, Step: 299, Loss: 0.7267\n",
            "Epoch: 007, Step: 300, Loss: 0.6955\n",
            "Epoch: 007, Step: 301, Loss: 0.6904\n",
            "Epoch: 007, Step: 302, Loss: 0.6965\n",
            "Epoch: 007, Step: 303, Loss: 0.6923\n",
            "Epoch: 007, Step: 304, Loss: 0.6955\n",
            "Epoch: 007, Step: 305, Loss: 0.6953\n",
            "Epoch: 007, Step: 306, Loss: 0.7358\n",
            "Epoch: 007, Step: 307, Loss: 0.7275\n",
            "Epoch: 007, Step: 308, Loss: 0.6569\n",
            "Epoch: 007, Step: 309, Loss: 0.6618\n",
            "Epoch: 007, Step: 310, Loss: 0.6981\n",
            "Epoch: 007, Step: 311, Loss: 0.7273\n",
            "Epoch: 007, Step: 312, Loss: 0.6926\n",
            "Epoch: 007, Step: 313, Loss: 0.6639\n",
            "Epoch: 007, Step: 314, Loss: 0.6893\n",
            "Epoch: 007, Step: 315, Loss: 0.7294\n",
            "Epoch: 007, Step: 316, Loss: 0.6893\n",
            "Epoch: 007, Step: 317, Loss: 0.6919\n",
            "Epoch: 007, Step: 318, Loss: 0.6624\n",
            "Epoch: 007, Step: 319, Loss: 0.6613\n",
            "Epoch: 007, Step: 320, Loss: 0.7213\n",
            "Epoch: 007, Step: 321, Loss: 0.6971\n",
            "Epoch: 007, Step: 322, Loss: 0.6962\n",
            "Epoch: 007, Step: 323, Loss: 0.6600\n",
            "Epoch: 007, Step: 324, Loss: 0.7283\n",
            "Epoch: 007, Step: 325, Loss: 0.6961\n",
            "Epoch: 007, Step: 326, Loss: 0.6674\n",
            "Epoch: 007, Step: 327, Loss: 0.6928\n",
            "Epoch: 007, Step: 328, Loss: 0.6915\n",
            "Epoch: 007, Step: 329, Loss: 0.6985\n",
            "Epoch: 007, Step: 330, Loss: 0.6913\n",
            "Epoch: 007, Step: 331, Loss: 0.6906\n",
            "Epoch: 007, Step: 332, Loss: 0.7331\n",
            "Epoch: 007, Step: 333, Loss: 0.7265\n",
            "Epoch: 007, Step: 334, Loss: 0.6590\n",
            "Epoch: 007, Step: 335, Loss: 0.6966\n",
            "Epoch: 007, Step: 336, Loss: 0.6582\n",
            "Epoch: 007, Step: 337, Loss: 0.6908\n",
            "Epoch: 007, Step: 338, Loss: 0.6936\n",
            "Epoch: 007, Step: 339, Loss: 0.6892\n",
            "Epoch: 007, Step: 340, Loss: 0.7269\n",
            "Epoch: 007, Step: 341, Loss: 0.6963\n",
            "Epoch: 007, Step: 342, Loss: 0.7221\n",
            "Epoch: 007, Step: 343, Loss: 0.6940\n",
            "Epoch: 007, Step: 344, Loss: 0.6979\n",
            "Epoch: 007, Step: 345, Loss: 0.6584\n",
            "Epoch: 007, Step: 346, Loss: 0.7239\n",
            "Epoch: 007, Step: 347, Loss: 0.7004\n",
            "Epoch: 007, Step: 348, Loss: 0.7247\n",
            "Epoch: 007, Step: 349, Loss: 0.7221\n",
            "Epoch: 007, Step: 350, Loss: 0.7202\n",
            "Epoch: 007, Step: 351, Loss: 0.6947\n",
            "Epoch: 007, Step: 352, Loss: 0.6981\n",
            "Epoch: 007, Step: 353, Loss: 0.6992\n",
            "Epoch: 007, Step: 354, Loss: 0.6897\n",
            "Epoch: 007, Step: 355, Loss: 0.6919\n",
            "Epoch: 007, Step: 356, Loss: 0.7264\n",
            "Epoch: 007, Step: 357, Loss: 0.6644\n",
            "Epoch: 007, Step: 358, Loss: 0.6999\n",
            "Epoch: 007, Step: 359, Loss: 0.6935\n",
            "Epoch: 007, Step: 360, Loss: 0.7312\n",
            "Epoch: 007, Step: 361, Loss: 0.6957\n",
            "Epoch: 007, Step: 362, Loss: 0.7293\n",
            "Epoch: 007, Step: 363, Loss: 0.6940\n",
            "Epoch: 007, Step: 364, Loss: 0.6960\n",
            "Epoch: 007, Step: 365, Loss: 0.7252\n",
            "Epoch: 007, Step: 366, Loss: 0.7229\n",
            "Epoch: 007, Step: 367, Loss: 0.6647\n",
            "Epoch: 007, Step: 368, Loss: 0.6893\n",
            "Epoch: 007, Step: 369, Loss: 0.6994\n",
            "Epoch: 007, Step: 370, Loss: 0.7282\n",
            "Epoch: 007, Step: 371, Loss: 0.6864\n",
            "Epoch: 007, Step: 372, Loss: 0.7282\n",
            "Epoch: 007, Step: 373, Loss: 0.7237\n",
            "Epoch: 007, Step: 374, Loss: 0.6915\n",
            "Epoch: 007, Step: 375, Loss: 0.6987\n",
            "Epoch: 007, Step: 376, Loss: 0.7021\n",
            "Epoch: 007, Step: 377, Loss: 0.6989\n",
            "Epoch: 007, Step: 378, Loss: 0.7301\n",
            "Epoch: 007, Step: 379, Loss: 0.6955\n",
            "Epoch: 007, Step: 380, Loss: 0.7300\n",
            "Epoch: 007, Step: 381, Loss: 0.6910\n",
            "Epoch: 007, Step: 382, Loss: 0.6676\n",
            "Epoch: 007, Step: 383, Loss: 0.7287\n",
            "Epoch: 007, Step: 384, Loss: 0.6592\n",
            "Epoch: 007, Step: 385, Loss: 0.7259\n",
            "Epoch: 007, Step: 386, Loss: 0.6598\n",
            "Epoch: 007, Step: 387, Loss: 0.6692\n",
            "Epoch: 007, Step: 388, Loss: 0.7322\n",
            "Epoch: 007, Step: 389, Loss: 0.6914\n",
            "Epoch: 007, Step: 390, Loss: 0.6908\n",
            "Epoch: 007, Step: 391, Loss: 0.7253\n",
            "Epoch: 007, Step: 392, Loss: 0.7274\n",
            "Epoch: 007, Step: 393, Loss: 0.7280\n",
            "Epoch: 007, Step: 394, Loss: 0.6942\n",
            "Epoch: 007, Step: 395, Loss: 0.7178\n",
            "Epoch: 007, Step: 396, Loss: 0.6971\n",
            "Epoch: 007, Step: 397, Loss: 0.6956\n",
            "Epoch: 007, Step: 398, Loss: 0.6931\n",
            "Epoch: 007, Step: 399, Loss: 0.6550\n",
            "Epoch: 007, Step: 400, Loss: 0.6574\n",
            "Epoch: 007, Step: 401, Loss: 0.6625\n",
            "Epoch: 007, Step: 402, Loss: 0.7195\n",
            "Epoch: 007, Step: 403, Loss: 0.6846\n",
            "Epoch: 007, Step: 404, Loss: 0.7338\n",
            "Epoch: 007, Step: 405, Loss: 0.6907\n",
            "Epoch: 007, Step: 406, Loss: 0.7178\n",
            "Epoch: 007, Step: 407, Loss: 0.6822\n",
            "Epoch: 007, Step: 408, Loss: 0.6923\n",
            "Epoch: 007, Step: 409, Loss: 0.7325\n",
            "Epoch: 007, Step: 410, Loss: 0.6595\n",
            "Epoch: 007, Step: 411, Loss: 0.6921\n",
            "Epoch: 007, Step: 412, Loss: 0.7275\n",
            "Epoch: 007, Step: 000, Val Loss: 0.6949\n",
            "Epoch: 007, Step: 001, Val Loss: 0.6915\n",
            "Epoch: 007, Step: 002, Val Loss: 0.6936\n",
            "Epoch: 007, Step: 003, Val Loss: 0.7254\n",
            "Epoch: 007, Step: 004, Val Loss: 0.6939\n",
            "Epoch: 007, Step: 005, Val Loss: 0.6929\n",
            "Epoch: 007, Step: 006, Val Loss: 0.7266\n",
            "Epoch: 007, Step: 007, Val Loss: 0.6930\n",
            "Epoch: 007, Step: 008, Val Loss: 0.7258\n",
            "Epoch: 007, Step: 009, Val Loss: 0.6931\n",
            "Epoch: 007, Step: 010, Val Loss: 0.6927\n",
            "Epoch: 007, Step: 011, Val Loss: 0.6954\n",
            "Epoch: 007, Step: 012, Val Loss: 0.7242\n",
            "Epoch: 007, Step: 013, Val Loss: 0.7261\n",
            "Epoch: 007, Step: 014, Val Loss: 0.6934\n",
            "Epoch: 007, Step: 015, Val Loss: 0.6931\n",
            "Epoch: 007, Step: 016, Val Loss: 0.6604\n",
            "Epoch: 007, Step: 017, Val Loss: 0.6938\n",
            "Epoch: 007, Step: 018, Val Loss: 0.6926\n",
            "Epoch: 007, Step: 019, Val Loss: 0.7265\n",
            "Epoch: 007, Step: 020, Val Loss: 0.6931\n",
            "Epoch: 007, Step: 021, Val Loss: 0.6942\n",
            "Epoch: 007, Step: 022, Val Loss: 0.7272\n",
            "Epoch: 007, Step: 023, Val Loss: 0.6938\n",
            "Epoch: 007, Step: 024, Val Loss: 0.6941\n",
            "Epoch: 007, Step: 025, Val Loss: 0.6611\n",
            "Epoch: 007, Step: 026, Val Loss: 0.6949\n",
            "Epoch: 007, Step: 027, Val Loss: 0.7252\n",
            "Epoch: 007, Step: 028, Val Loss: 0.6943\n",
            "Epoch: 007, Step: 029, Val Loss: 0.6940\n",
            "Epoch: 007, Step: 030, Val Loss: 0.7277\n",
            "Epoch: 007, Step: 031, Val Loss: 0.6946\n",
            "Epoch: 007, Step: 032, Val Loss: 0.6921\n",
            "Epoch: 007, Step: 033, Val Loss: 0.7276\n",
            "Epoch: 007, Step: 034, Val Loss: 0.6622\n",
            "Epoch: 007, Step: 035, Val Loss: 0.6954\n",
            "Epoch: 007, Step: 036, Val Loss: 0.6937\n",
            "Epoch: 007, Step: 037, Val Loss: 0.7273\n",
            "Epoch: 007, Step: 038, Val Loss: 0.6937\n",
            "Epoch: 007, Step: 039, Val Loss: 0.6605\n",
            "Epoch: 007, Step: 040, Val Loss: 0.6942\n",
            "Epoch: 007, Step: 041, Val Loss: 0.6928\n",
            "Epoch: 007, Step: 042, Val Loss: 0.7257\n",
            "Epoch: 007, Step: 043, Val Loss: 0.7260\n",
            "Epoch: 007, Step: 044, Val Loss: 0.6932\n",
            "Epoch: 007, Step: 045, Val Loss: 0.7259\n",
            "Epoch: 007, Step: 046, Val Loss: 0.6932\n",
            "Epoch: 007, Step: 047, Val Loss: 0.6927\n",
            "Epoch: 007, Step: 048, Val Loss: 0.7262\n",
            "Epoch: 007, Step: 049, Val Loss: 0.6939\n",
            "Epoch: 007, Step: 050, Val Loss: 0.6611\n",
            "Epoch: 007, Step: 051, Val Loss: 0.7260\n",
            "Epoch: 007, Step: 052, Val Loss: 0.6935\n",
            "Epoch: 007, Step: 053, Val Loss: 0.7263\n",
            "Epoch: 007, Step: 054, Val Loss: 0.6944\n",
            "Epoch: 007, Step: 055, Val Loss: 0.6927\n",
            "Epoch: 007, Step: 056, Val Loss: 0.6941\n",
            "Epoch: 007, Step: 057, Val Loss: 0.6929\n",
            "Epoch: 007, Step: 058, Val Loss: 0.6595\n",
            "Epoch: 007, Step: 059, Val Loss: 0.7260\n",
            "Epoch: 007, Step: 060, Val Loss: 0.7269\n",
            "Epoch: 007, Step: 061, Val Loss: 0.7264\n",
            "Epoch: 007, Step: 062, Val Loss: 0.7267\n",
            "Epoch: 007, Step: 063, Val Loss: 0.6947\n",
            "Epoch: 007, Step: 064, Val Loss: 0.7266\n",
            "Epoch: 007, Step: 065, Val Loss: 0.6622\n",
            "Epoch: 007, Step: 066, Val Loss: 0.6605\n",
            "Epoch: 007, Step: 067, Val Loss: 0.6612\n",
            "Epoch: 007, Step: 068, Val Loss: 0.6616\n",
            "Epoch: 007, Step: 069, Val Loss: 0.7250\n",
            "Epoch: 007, Step: 070, Val Loss: 0.6947\n",
            "Epoch: 007, Step: 071, Val Loss: 0.6611\n",
            "Epoch: 007, Step: 072, Val Loss: 0.7269\n",
            "Epoch: 007, Step: 073, Val Loss: 0.7261\n",
            "Epoch: 007, Step: 074, Val Loss: 0.6605\n",
            "Epoch: 007, Step: 075, Val Loss: 0.7261\n",
            "Epoch: 007, Step: 076, Val Loss: 0.6928\n",
            "Epoch: 007, Step: 077, Val Loss: 0.7260\n",
            "Epoch: 007, Step: 078, Val Loss: 0.7263\n",
            "Epoch: 007, Step: 079, Val Loss: 0.7258\n",
            "Epoch: 007, Step: 080, Val Loss: 0.6926\n",
            "Epoch: 007, Step: 081, Val Loss: 0.6923\n",
            "Epoch: 007, Step: 082, Val Loss: 0.7256\n",
            "Epoch: 007, Step: 083, Val Loss: 0.6938\n",
            "Epoch: 007, Step: 084, Val Loss: 0.7264\n",
            "Epoch: 007, Step: 085, Val Loss: 0.6935\n",
            "Epoch: 007, Step: 086, Val Loss: 0.7264\n",
            "Epoch: 007, Step: 087, Val Loss: 0.6605\n",
            "Epoch: 007, Step: 088, Val Loss: 0.7250\n",
            "Epoch: 007, Step: 089, Val Loss: 0.6595\n",
            "Epoch: 007, Step: 090, Val Loss: 0.6930\n",
            "Epoch: 007, Step: 091, Val Loss: 0.6934\n",
            "Epoch: 007, Step: 092, Val Loss: 0.6940\n",
            "Epoch: 007, Step: 093, Val Loss: 0.6937\n",
            "Epoch: 007, Step: 094, Val Loss: 0.6920\n",
            "Epoch: 007, Step: 095, Val Loss: 0.6933\n",
            "Epoch: 007, Step: 096, Val Loss: 0.6930\n",
            "Epoch: 007, Step: 097, Val Loss: 0.7266\n",
            "Epoch: 007, Step: 098, Val Loss: 0.6944\n",
            "Epoch: 007, Step: 099, Val Loss: 0.7249\n",
            "Epoch: 007, Step: 100, Val Loss: 0.6598\n",
            "Epoch: 007, Step: 101, Val Loss: 0.7260\n",
            "Epoch: 007, Step: 102, Val Loss: 0.7270\n",
            "Epoch: 007, Step: 103, Val Loss: 0.6610\n",
            "Epoch: 007, Step: 104, Val Loss: 0.7265\n",
            "Epoch: 007, Step: 105, Val Loss: 0.6625\n",
            "Epoch: 007, Step: 106, Val Loss: 0.6947\n",
            "Epoch: 007, Step: 107, Val Loss: 0.6949\n",
            "Epoch: 007, Step: 108, Val Loss: 0.6940\n",
            "Epoch: 007, Step: 109, Val Loss: 0.6934\n",
            "Epoch: 007, Step: 110, Val Loss: 0.7261\n",
            "Epoch: 007, Step: 111, Val Loss: 0.6929\n",
            "Epoch: 007, Step: 112, Val Loss: 0.6930\n",
            "Epoch: 007, Step: 113, Val Loss: 0.6942\n",
            "Epoch: 007, Step: 114, Val Loss: 0.6614\n",
            "Epoch: 007, Step: 115, Val Loss: 0.6933\n",
            "Epoch: 007, Step: 116, Val Loss: 0.7253\n",
            "Epoch: 007, Step: 117, Val Loss: 0.7268\n",
            "Epoch: 008, Step: 000, Loss: 0.6660\n",
            "Epoch: 008, Step: 001, Loss: 0.7316\n",
            "Epoch: 008, Step: 002, Loss: 0.6946\n",
            "Epoch: 008, Step: 003, Loss: 0.6963\n",
            "Epoch: 008, Step: 004, Loss: 0.6958\n",
            "Epoch: 008, Step: 005, Loss: 0.6642\n",
            "Epoch: 008, Step: 006, Loss: 0.7310\n",
            "Epoch: 008, Step: 007, Loss: 0.7238\n",
            "Epoch: 008, Step: 008, Loss: 0.6893\n",
            "Epoch: 008, Step: 009, Loss: 0.7271\n",
            "Epoch: 008, Step: 010, Loss: 0.7221\n",
            "Epoch: 008, Step: 011, Loss: 0.6906\n",
            "Epoch: 008, Step: 012, Loss: 0.7308\n",
            "Epoch: 008, Step: 013, Loss: 0.6903\n",
            "Epoch: 008, Step: 014, Loss: 0.6979\n",
            "Epoch: 008, Step: 015, Loss: 0.6923\n",
            "Epoch: 008, Step: 016, Loss: 0.6872\n",
            "Epoch: 008, Step: 017, Loss: 0.6883\n",
            "Epoch: 008, Step: 018, Loss: 0.7292\n",
            "Epoch: 008, Step: 019, Loss: 0.6960\n",
            "Epoch: 008, Step: 020, Loss: 0.7301\n",
            "Epoch: 008, Step: 021, Loss: 0.7300\n",
            "Epoch: 008, Step: 022, Loss: 0.7171\n",
            "Epoch: 008, Step: 023, Loss: 0.7179\n",
            "Epoch: 008, Step: 024, Loss: 0.6936\n",
            "Epoch: 008, Step: 025, Loss: 0.6906\n",
            "Epoch: 008, Step: 026, Loss: 0.6982\n",
            "Epoch: 008, Step: 027, Loss: 0.6959\n",
            "Epoch: 008, Step: 028, Loss: 0.6905\n",
            "Epoch: 008, Step: 029, Loss: 0.7219\n",
            "Epoch: 008, Step: 030, Loss: 0.7290\n",
            "Epoch: 008, Step: 031, Loss: 0.7304\n",
            "Epoch: 008, Step: 032, Loss: 0.7241\n",
            "Epoch: 008, Step: 033, Loss: 0.7279\n",
            "Epoch: 008, Step: 034, Loss: 0.7014\n",
            "Epoch: 008, Step: 035, Loss: 0.6527\n",
            "Epoch: 008, Step: 036, Loss: 0.7227\n",
            "Epoch: 008, Step: 037, Loss: 0.6873\n",
            "Epoch: 008, Step: 038, Loss: 0.6907\n",
            "Epoch: 008, Step: 039, Loss: 0.7327\n",
            "Epoch: 008, Step: 040, Loss: 0.7225\n",
            "Epoch: 008, Step: 041, Loss: 0.6551\n",
            "Epoch: 008, Step: 042, Loss: 0.7028\n",
            "Epoch: 008, Step: 043, Loss: 0.6941\n",
            "Epoch: 008, Step: 044, Loss: 0.7296\n",
            "Epoch: 008, Step: 045, Loss: 0.7232\n",
            "Epoch: 008, Step: 046, Loss: 0.6926\n",
            "Epoch: 008, Step: 047, Loss: 0.6917\n",
            "Epoch: 008, Step: 048, Loss: 0.6964\n",
            "Epoch: 008, Step: 049, Loss: 0.7263\n",
            "Epoch: 008, Step: 050, Loss: 0.7336\n",
            "Epoch: 008, Step: 051, Loss: 0.6918\n",
            "Epoch: 008, Step: 052, Loss: 0.7284\n",
            "Epoch: 008, Step: 053, Loss: 0.6951\n",
            "Epoch: 008, Step: 054, Loss: 0.7222\n",
            "Epoch: 008, Step: 055, Loss: 0.7295\n",
            "Epoch: 008, Step: 056, Loss: 0.7300\n",
            "Epoch: 008, Step: 057, Loss: 0.7243\n",
            "Epoch: 008, Step: 058, Loss: 0.6914\n",
            "Epoch: 008, Step: 059, Loss: 0.6607\n",
            "Epoch: 008, Step: 060, Loss: 0.6931\n",
            "Epoch: 008, Step: 061, Loss: 0.6630\n",
            "Epoch: 008, Step: 062, Loss: 0.6920\n",
            "Epoch: 008, Step: 063, Loss: 0.6958\n",
            "Epoch: 008, Step: 064, Loss: 0.7288\n",
            "Epoch: 008, Step: 065, Loss: 0.6950\n",
            "Epoch: 008, Step: 066, Loss: 0.6957\n",
            "Epoch: 008, Step: 067, Loss: 0.6920\n",
            "Epoch: 008, Step: 068, Loss: 0.6540\n",
            "Epoch: 008, Step: 069, Loss: 0.6919\n",
            "Epoch: 008, Step: 070, Loss: 0.7244\n",
            "Epoch: 008, Step: 071, Loss: 0.6891\n",
            "Epoch: 008, Step: 072, Loss: 0.6634\n",
            "Epoch: 008, Step: 073, Loss: 0.7203\n",
            "Epoch: 008, Step: 074, Loss: 0.7217\n",
            "Epoch: 008, Step: 075, Loss: 0.7313\n",
            "Epoch: 008, Step: 076, Loss: 0.6916\n",
            "Epoch: 008, Step: 077, Loss: 0.6962\n",
            "Epoch: 008, Step: 078, Loss: 0.6644\n",
            "Epoch: 008, Step: 079, Loss: 0.6631\n",
            "Epoch: 008, Step: 080, Loss: 0.6599\n",
            "Epoch: 008, Step: 081, Loss: 0.6917\n",
            "Epoch: 008, Step: 082, Loss: 0.6604\n",
            "Epoch: 008, Step: 083, Loss: 0.6952\n",
            "Epoch: 008, Step: 084, Loss: 0.6950\n",
            "Epoch: 008, Step: 085, Loss: 0.6960\n",
            "Epoch: 008, Step: 086, Loss: 0.7227\n",
            "Epoch: 008, Step: 087, Loss: 0.6876\n",
            "Epoch: 008, Step: 088, Loss: 0.7310\n",
            "Epoch: 008, Step: 089, Loss: 0.7278\n",
            "Epoch: 008, Step: 090, Loss: 0.6967\n",
            "Epoch: 008, Step: 091, Loss: 0.6910\n",
            "Epoch: 008, Step: 092, Loss: 0.7281\n",
            "Epoch: 008, Step: 093, Loss: 0.6920\n",
            "Epoch: 008, Step: 094, Loss: 0.7259\n",
            "Epoch: 008, Step: 095, Loss: 0.7315\n",
            "Epoch: 008, Step: 096, Loss: 0.6573\n",
            "Epoch: 008, Step: 097, Loss: 0.7266\n",
            "Epoch: 008, Step: 098, Loss: 0.6909\n",
            "Epoch: 008, Step: 099, Loss: 0.7010\n",
            "Epoch: 008, Step: 100, Loss: 0.6949\n",
            "Epoch: 008, Step: 101, Loss: 0.7226\n",
            "Epoch: 008, Step: 102, Loss: 0.6953\n",
            "Epoch: 008, Step: 103, Loss: 0.6943\n",
            "Epoch: 008, Step: 104, Loss: 0.6848\n",
            "Epoch: 008, Step: 105, Loss: 0.6924\n",
            "Epoch: 008, Step: 106, Loss: 0.7254\n",
            "Epoch: 008, Step: 107, Loss: 0.6873\n",
            "Epoch: 008, Step: 108, Loss: 0.6872\n",
            "Epoch: 008, Step: 109, Loss: 0.7316\n",
            "Epoch: 008, Step: 110, Loss: 0.7297\n",
            "Epoch: 008, Step: 111, Loss: 0.6979\n",
            "Epoch: 008, Step: 112, Loss: 0.6632\n",
            "Epoch: 008, Step: 113, Loss: 0.6667\n",
            "Epoch: 008, Step: 114, Loss: 0.6904\n",
            "Epoch: 008, Step: 115, Loss: 0.6969\n",
            "Epoch: 008, Step: 116, Loss: 0.6968\n",
            "Epoch: 008, Step: 117, Loss: 0.7210\n",
            "Epoch: 008, Step: 118, Loss: 0.6965\n",
            "Epoch: 008, Step: 119, Loss: 0.6910\n",
            "Epoch: 008, Step: 120, Loss: 0.7192\n",
            "Epoch: 008, Step: 121, Loss: 0.6966\n",
            "Epoch: 008, Step: 122, Loss: 0.6922\n",
            "Epoch: 008, Step: 123, Loss: 0.7012\n",
            "Epoch: 008, Step: 124, Loss: 0.6925\n",
            "Epoch: 008, Step: 125, Loss: 0.7006\n",
            "Epoch: 008, Step: 126, Loss: 0.6908\n",
            "Epoch: 008, Step: 127, Loss: 0.7332\n",
            "Epoch: 008, Step: 128, Loss: 0.7289\n",
            "Epoch: 008, Step: 129, Loss: 0.6642\n",
            "Epoch: 008, Step: 130, Loss: 0.7325\n",
            "Epoch: 008, Step: 131, Loss: 0.7218\n",
            "Epoch: 008, Step: 132, Loss: 0.6627\n",
            "Epoch: 008, Step: 133, Loss: 0.6980\n",
            "Epoch: 008, Step: 134, Loss: 0.6958\n",
            "Epoch: 008, Step: 135, Loss: 0.7242\n",
            "Epoch: 008, Step: 136, Loss: 0.7344\n",
            "Epoch: 008, Step: 137, Loss: 0.7252\n",
            "Epoch: 008, Step: 138, Loss: 0.6671\n",
            "Epoch: 008, Step: 139, Loss: 0.6976\n",
            "Epoch: 008, Step: 140, Loss: 0.7261\n",
            "Epoch: 008, Step: 141, Loss: 0.6977\n",
            "Epoch: 008, Step: 142, Loss: 0.6916\n",
            "Epoch: 008, Step: 143, Loss: 0.7247\n",
            "Epoch: 008, Step: 144, Loss: 0.7213\n",
            "Epoch: 008, Step: 145, Loss: 0.6947\n",
            "Epoch: 008, Step: 146, Loss: 0.7256\n",
            "Epoch: 008, Step: 147, Loss: 0.6628\n",
            "Epoch: 008, Step: 148, Loss: 0.7240\n",
            "Epoch: 008, Step: 149, Loss: 0.7189\n",
            "Epoch: 008, Step: 150, Loss: 0.7264\n",
            "Epoch: 008, Step: 151, Loss: 0.6587\n",
            "Epoch: 008, Step: 152, Loss: 0.7191\n",
            "Epoch: 008, Step: 153, Loss: 0.6971\n",
            "Epoch: 008, Step: 154, Loss: 0.6602\n",
            "Epoch: 008, Step: 155, Loss: 0.7287\n",
            "Epoch: 008, Step: 156, Loss: 0.6965\n",
            "Epoch: 008, Step: 157, Loss: 0.7288\n",
            "Epoch: 008, Step: 158, Loss: 0.6871\n",
            "Epoch: 008, Step: 159, Loss: 0.6954\n",
            "Epoch: 008, Step: 160, Loss: 0.7277\n",
            "Epoch: 008, Step: 161, Loss: 0.6943\n",
            "Epoch: 008, Step: 162, Loss: 0.7267\n",
            "Epoch: 008, Step: 163, Loss: 0.6920\n",
            "Epoch: 008, Step: 164, Loss: 0.7365\n",
            "Epoch: 008, Step: 165, Loss: 0.6664\n",
            "Epoch: 008, Step: 166, Loss: 0.6950\n",
            "Epoch: 008, Step: 167, Loss: 0.7267\n",
            "Epoch: 008, Step: 168, Loss: 0.6587\n",
            "Epoch: 008, Step: 169, Loss: 0.6992\n",
            "Epoch: 008, Step: 170, Loss: 0.6633\n",
            "Epoch: 008, Step: 171, Loss: 0.6885\n",
            "Epoch: 008, Step: 172, Loss: 0.6933\n",
            "Epoch: 008, Step: 173, Loss: 0.6965\n",
            "Epoch: 008, Step: 174, Loss: 0.6950\n",
            "Epoch: 008, Step: 175, Loss: 0.6941\n",
            "Epoch: 008, Step: 176, Loss: 0.6617\n",
            "Epoch: 008, Step: 177, Loss: 0.6917\n",
            "Epoch: 008, Step: 178, Loss: 0.6537\n",
            "Epoch: 008, Step: 179, Loss: 0.6856\n",
            "Epoch: 008, Step: 180, Loss: 0.6921\n",
            "Epoch: 008, Step: 181, Loss: 0.6946\n",
            "Epoch: 008, Step: 182, Loss: 0.6592\n",
            "Epoch: 008, Step: 183, Loss: 0.6947\n",
            "Epoch: 008, Step: 184, Loss: 0.6947\n",
            "Epoch: 008, Step: 185, Loss: 0.7279\n",
            "Epoch: 008, Step: 186, Loss: 0.6972\n",
            "Epoch: 008, Step: 187, Loss: 0.6967\n",
            "Epoch: 008, Step: 188, Loss: 0.7252\n",
            "Epoch: 008, Step: 189, Loss: 0.7252\n",
            "Epoch: 008, Step: 190, Loss: 0.6664\n",
            "Epoch: 008, Step: 191, Loss: 0.6630\n",
            "Epoch: 008, Step: 192, Loss: 0.6958\n",
            "Epoch: 008, Step: 193, Loss: 0.6944\n",
            "Epoch: 008, Step: 194, Loss: 0.7319\n",
            "Epoch: 008, Step: 195, Loss: 0.7320\n",
            "Epoch: 008, Step: 196, Loss: 0.6668\n",
            "Epoch: 008, Step: 197, Loss: 0.6951\n",
            "Epoch: 008, Step: 198, Loss: 0.7271\n",
            "Epoch: 008, Step: 199, Loss: 0.6666\n",
            "Epoch: 008, Step: 200, Loss: 0.7229\n",
            "Epoch: 008, Step: 201, Loss: 0.7217\n",
            "Epoch: 008, Step: 202, Loss: 0.6601\n",
            "Epoch: 008, Step: 203, Loss: 0.7260\n",
            "Epoch: 008, Step: 204, Loss: 0.6561\n",
            "Epoch: 008, Step: 205, Loss: 0.6988\n",
            "Epoch: 008, Step: 206, Loss: 0.6928\n",
            "Epoch: 008, Step: 207, Loss: 0.6640\n",
            "Epoch: 008, Step: 208, Loss: 0.6969\n",
            "Epoch: 008, Step: 209, Loss: 0.6951\n",
            "Epoch: 008, Step: 210, Loss: 0.6596\n",
            "Epoch: 008, Step: 211, Loss: 0.6959\n",
            "Epoch: 008, Step: 212, Loss: 0.7312\n",
            "Epoch: 008, Step: 213, Loss: 0.6582\n",
            "Epoch: 008, Step: 214, Loss: 0.6558\n",
            "Epoch: 008, Step: 215, Loss: 0.6866\n",
            "Epoch: 008, Step: 216, Loss: 0.7240\n",
            "Epoch: 008, Step: 217, Loss: 0.6591\n",
            "Epoch: 008, Step: 218, Loss: 0.7310\n",
            "Epoch: 008, Step: 219, Loss: 0.7029\n",
            "Epoch: 008, Step: 220, Loss: 0.7280\n",
            "Epoch: 008, Step: 221, Loss: 0.6958\n",
            "Epoch: 008, Step: 222, Loss: 0.7206\n",
            "Epoch: 008, Step: 223, Loss: 0.6600\n",
            "Epoch: 008, Step: 224, Loss: 0.7300\n",
            "Epoch: 008, Step: 225, Loss: 0.6972\n",
            "Epoch: 008, Step: 226, Loss: 0.7237\n",
            "Epoch: 008, Step: 227, Loss: 0.7263\n",
            "Epoch: 008, Step: 228, Loss: 0.6873\n",
            "Epoch: 008, Step: 229, Loss: 0.6889\n",
            "Epoch: 008, Step: 230, Loss: 0.6676\n",
            "Epoch: 008, Step: 231, Loss: 0.6990\n",
            "Epoch: 008, Step: 232, Loss: 0.6909\n",
            "Epoch: 008, Step: 233, Loss: 0.7260\n",
            "Epoch: 008, Step: 234, Loss: 0.6996\n",
            "Epoch: 008, Step: 235, Loss: 0.6949\n",
            "Epoch: 008, Step: 236, Loss: 0.7296\n",
            "Epoch: 008, Step: 237, Loss: 0.7217\n",
            "Epoch: 008, Step: 238, Loss: 0.6984\n",
            "Epoch: 008, Step: 239, Loss: 0.6652\n",
            "Epoch: 008, Step: 240, Loss: 0.6972\n",
            "Epoch: 008, Step: 241, Loss: 0.7258\n",
            "Epoch: 008, Step: 242, Loss: 0.7228\n",
            "Epoch: 008, Step: 243, Loss: 0.6999\n",
            "Epoch: 008, Step: 244, Loss: 0.7280\n",
            "Epoch: 008, Step: 245, Loss: 0.7278\n",
            "Epoch: 008, Step: 246, Loss: 0.6939\n",
            "Epoch: 008, Step: 247, Loss: 0.7276\n",
            "Epoch: 008, Step: 248, Loss: 0.6636\n",
            "Epoch: 008, Step: 249, Loss: 0.7254\n",
            "Epoch: 008, Step: 250, Loss: 0.6925\n",
            "Epoch: 008, Step: 251, Loss: 0.6884\n",
            "Epoch: 008, Step: 252, Loss: 0.6960\n",
            "Epoch: 008, Step: 253, Loss: 0.6905\n",
            "Epoch: 008, Step: 254, Loss: 0.6900\n",
            "Epoch: 008, Step: 255, Loss: 0.7218\n",
            "Epoch: 008, Step: 256, Loss: 0.6927\n",
            "Epoch: 008, Step: 257, Loss: 0.6996\n",
            "Epoch: 008, Step: 258, Loss: 0.6927\n",
            "Epoch: 008, Step: 259, Loss: 0.7310\n",
            "Epoch: 008, Step: 260, Loss: 0.6881\n",
            "Epoch: 008, Step: 261, Loss: 0.6929\n",
            "Epoch: 008, Step: 262, Loss: 0.6868\n",
            "Epoch: 008, Step: 263, Loss: 0.7309\n",
            "Epoch: 008, Step: 264, Loss: 0.7236\n",
            "Epoch: 008, Step: 265, Loss: 0.6845\n",
            "Epoch: 008, Step: 266, Loss: 0.7033\n",
            "Epoch: 008, Step: 267, Loss: 0.7226\n",
            "Epoch: 008, Step: 268, Loss: 0.7285\n",
            "Epoch: 008, Step: 269, Loss: 0.6993\n",
            "Epoch: 008, Step: 270, Loss: 0.6884\n",
            "Epoch: 008, Step: 271, Loss: 0.6938\n",
            "Epoch: 008, Step: 272, Loss: 0.7278\n",
            "Epoch: 008, Step: 273, Loss: 0.7270\n",
            "Epoch: 008, Step: 274, Loss: 0.6968\n",
            "Epoch: 008, Step: 275, Loss: 0.6931\n",
            "Epoch: 008, Step: 276, Loss: 0.6972\n",
            "Epoch: 008, Step: 277, Loss: 0.6520\n",
            "Epoch: 008, Step: 278, Loss: 0.6955\n",
            "Epoch: 008, Step: 279, Loss: 0.6952\n",
            "Epoch: 008, Step: 280, Loss: 0.6939\n",
            "Epoch: 008, Step: 281, Loss: 0.6852\n",
            "Epoch: 008, Step: 282, Loss: 0.6912\n",
            "Epoch: 008, Step: 283, Loss: 0.6930\n",
            "Epoch: 008, Step: 284, Loss: 0.6928\n",
            "Epoch: 008, Step: 285, Loss: 0.6911\n",
            "Epoch: 008, Step: 286, Loss: 0.6914\n",
            "Epoch: 008, Step: 287, Loss: 0.6942\n",
            "Epoch: 008, Step: 288, Loss: 0.7300\n",
            "Epoch: 008, Step: 289, Loss: 0.6660\n",
            "Epoch: 008, Step: 290, Loss: 0.6981\n",
            "Epoch: 008, Step: 291, Loss: 0.6917\n",
            "Epoch: 008, Step: 292, Loss: 0.6636\n",
            "Epoch: 008, Step: 293, Loss: 0.6944\n",
            "Epoch: 008, Step: 294, Loss: 0.7223\n",
            "Epoch: 008, Step: 295, Loss: 0.6904\n",
            "Epoch: 008, Step: 296, Loss: 0.7296\n",
            "Epoch: 008, Step: 297, Loss: 0.6945\n",
            "Epoch: 008, Step: 298, Loss: 0.7272\n",
            "Epoch: 008, Step: 299, Loss: 0.6914\n",
            "Epoch: 008, Step: 300, Loss: 0.6906\n",
            "Epoch: 008, Step: 301, Loss: 0.7243\n",
            "Epoch: 008, Step: 302, Loss: 0.7281\n",
            "Epoch: 008, Step: 303, Loss: 0.7290\n",
            "Epoch: 008, Step: 304, Loss: 0.6926\n",
            "Epoch: 008, Step: 305, Loss: 0.7287\n",
            "Epoch: 008, Step: 306, Loss: 0.6920\n",
            "Epoch: 008, Step: 307, Loss: 0.6925\n",
            "Epoch: 008, Step: 308, Loss: 0.6908\n",
            "Epoch: 008, Step: 309, Loss: 0.7244\n",
            "Epoch: 008, Step: 310, Loss: 0.6921\n",
            "Epoch: 008, Step: 311, Loss: 0.6642\n",
            "Epoch: 008, Step: 312, Loss: 0.6937\n",
            "Epoch: 008, Step: 313, Loss: 0.6886\n",
            "Epoch: 008, Step: 314, Loss: 0.6968\n",
            "Epoch: 008, Step: 315, Loss: 0.6612\n",
            "Epoch: 008, Step: 316, Loss: 0.6920\n",
            "Epoch: 008, Step: 317, Loss: 0.7201\n",
            "Epoch: 008, Step: 318, Loss: 0.6874\n",
            "Epoch: 008, Step: 319, Loss: 0.6626\n",
            "Epoch: 008, Step: 320, Loss: 0.6953\n",
            "Epoch: 008, Step: 321, Loss: 0.6604\n",
            "Epoch: 008, Step: 322, Loss: 0.6817\n",
            "Epoch: 008, Step: 323, Loss: 0.6653\n",
            "Epoch: 008, Step: 324, Loss: 0.6582\n",
            "Epoch: 008, Step: 325, Loss: 0.6641\n",
            "Epoch: 008, Step: 326, Loss: 0.6981\n",
            "Epoch: 008, Step: 327, Loss: 0.7269\n",
            "Epoch: 008, Step: 328, Loss: 0.6954\n",
            "Epoch: 008, Step: 329, Loss: 0.6900\n",
            "Epoch: 008, Step: 330, Loss: 0.7000\n",
            "Epoch: 008, Step: 331, Loss: 0.6936\n",
            "Epoch: 008, Step: 332, Loss: 0.7236\n",
            "Epoch: 008, Step: 333, Loss: 0.7294\n",
            "Epoch: 008, Step: 334, Loss: 0.6948\n",
            "Epoch: 008, Step: 335, Loss: 0.6952\n",
            "Epoch: 008, Step: 336, Loss: 0.6892\n",
            "Epoch: 008, Step: 337, Loss: 0.6972\n",
            "Epoch: 008, Step: 338, Loss: 0.6939\n",
            "Epoch: 008, Step: 339, Loss: 0.6951\n",
            "Epoch: 008, Step: 340, Loss: 0.6999\n",
            "Epoch: 008, Step: 341, Loss: 0.6566\n",
            "Epoch: 008, Step: 342, Loss: 0.7225\n",
            "Epoch: 008, Step: 343, Loss: 0.6585\n",
            "Epoch: 008, Step: 344, Loss: 0.7338\n",
            "Epoch: 008, Step: 345, Loss: 0.6963\n",
            "Epoch: 008, Step: 346, Loss: 0.6933\n",
            "Epoch: 008, Step: 347, Loss: 0.6565\n",
            "Epoch: 008, Step: 348, Loss: 0.6923\n",
            "Epoch: 008, Step: 349, Loss: 0.7259\n",
            "Epoch: 008, Step: 350, Loss: 0.7273\n",
            "Epoch: 008, Step: 351, Loss: 0.7267\n",
            "Epoch: 008, Step: 352, Loss: 0.6637\n",
            "Epoch: 008, Step: 353, Loss: 0.6949\n",
            "Epoch: 008, Step: 354, Loss: 0.6861\n",
            "Epoch: 008, Step: 355, Loss: 0.6914\n",
            "Epoch: 008, Step: 356, Loss: 0.7271\n",
            "Epoch: 008, Step: 357, Loss: 0.6973\n",
            "Epoch: 008, Step: 358, Loss: 0.7250\n",
            "Epoch: 008, Step: 359, Loss: 0.6598\n",
            "Epoch: 008, Step: 360, Loss: 0.6640\n",
            "Epoch: 008, Step: 361, Loss: 0.7265\n",
            "Epoch: 008, Step: 362, Loss: 0.6993\n",
            "Epoch: 008, Step: 363, Loss: 0.6637\n",
            "Epoch: 008, Step: 364, Loss: 0.7253\n",
            "Epoch: 008, Step: 365, Loss: 0.6628\n",
            "Epoch: 008, Step: 366, Loss: 0.6626\n",
            "Epoch: 008, Step: 367, Loss: 0.7310\n",
            "Epoch: 008, Step: 368, Loss: 0.6709\n",
            "Epoch: 008, Step: 369, Loss: 0.6946\n",
            "Epoch: 008, Step: 370, Loss: 0.7241\n",
            "Epoch: 008, Step: 371, Loss: 0.6963\n",
            "Epoch: 008, Step: 372, Loss: 0.6928\n",
            "Epoch: 008, Step: 373, Loss: 0.7266\n",
            "Epoch: 008, Step: 374, Loss: 0.6659\n",
            "Epoch: 008, Step: 375, Loss: 0.6859\n",
            "Epoch: 008, Step: 376, Loss: 0.6627\n",
            "Epoch: 008, Step: 377, Loss: 0.7246\n",
            "Epoch: 008, Step: 378, Loss: 0.6620\n",
            "Epoch: 008, Step: 379, Loss: 0.6916\n",
            "Epoch: 008, Step: 380, Loss: 0.6947\n",
            "Epoch: 008, Step: 381, Loss: 0.7226\n",
            "Epoch: 008, Step: 382, Loss: 0.6603\n",
            "Epoch: 008, Step: 383, Loss: 0.6935\n",
            "Epoch: 008, Step: 384, Loss: 0.6627\n",
            "Epoch: 008, Step: 385, Loss: 0.6987\n",
            "Epoch: 008, Step: 386, Loss: 0.6880\n",
            "Epoch: 008, Step: 387, Loss: 0.7239\n",
            "Epoch: 008, Step: 388, Loss: 0.6847\n",
            "Epoch: 008, Step: 389, Loss: 0.6582\n",
            "Epoch: 008, Step: 390, Loss: 0.7272\n",
            "Epoch: 008, Step: 391, Loss: 0.6649\n",
            "Epoch: 008, Step: 392, Loss: 0.6970\n",
            "Epoch: 008, Step: 393, Loss: 0.6981\n",
            "Epoch: 008, Step: 394, Loss: 0.7229\n",
            "Epoch: 008, Step: 395, Loss: 0.6931\n",
            "Epoch: 008, Step: 396, Loss: 0.7271\n",
            "Epoch: 008, Step: 397, Loss: 0.6940\n",
            "Epoch: 008, Step: 398, Loss: 0.6951\n",
            "Epoch: 008, Step: 399, Loss: 0.7278\n",
            "Epoch: 008, Step: 400, Loss: 0.6948\n",
            "Epoch: 008, Step: 401, Loss: 0.6950\n",
            "Epoch: 008, Step: 402, Loss: 0.7311\n",
            "Epoch: 008, Step: 403, Loss: 0.6647\n",
            "Epoch: 008, Step: 404, Loss: 0.7227\n",
            "Epoch: 008, Step: 405, Loss: 0.7304\n",
            "Epoch: 008, Step: 406, Loss: 0.7274\n",
            "Epoch: 008, Step: 407, Loss: 0.7245\n",
            "Epoch: 008, Step: 408, Loss: 0.7264\n",
            "Epoch: 008, Step: 409, Loss: 0.7247\n",
            "Epoch: 008, Step: 410, Loss: 0.7235\n",
            "Epoch: 008, Step: 411, Loss: 0.6938\n",
            "Epoch: 008, Step: 412, Loss: 0.7276\n",
            "Epoch: 008, Step: 000, Val Loss: 0.6607\n",
            "Epoch: 008, Step: 001, Val Loss: 0.7269\n",
            "Epoch: 008, Step: 002, Val Loss: 0.6930\n",
            "Epoch: 008, Step: 003, Val Loss: 0.6627\n",
            "Epoch: 008, Step: 004, Val Loss: 0.6945\n",
            "Epoch: 008, Step: 005, Val Loss: 0.6606\n",
            "Epoch: 008, Step: 006, Val Loss: 0.6940\n",
            "Epoch: 008, Step: 007, Val Loss: 0.7258\n",
            "Epoch: 008, Step: 008, Val Loss: 0.6941\n",
            "Epoch: 008, Step: 009, Val Loss: 0.7269\n",
            "Epoch: 008, Step: 010, Val Loss: 0.6938\n",
            "Epoch: 008, Step: 011, Val Loss: 0.7256\n",
            "Epoch: 008, Step: 012, Val Loss: 0.6609\n",
            "Epoch: 008, Step: 013, Val Loss: 0.7267\n",
            "Epoch: 008, Step: 014, Val Loss: 0.6931\n",
            "Epoch: 008, Step: 015, Val Loss: 0.7258\n",
            "Epoch: 008, Step: 016, Val Loss: 0.6944\n",
            "Epoch: 008, Step: 017, Val Loss: 0.6929\n",
            "Epoch: 008, Step: 018, Val Loss: 0.7256\n",
            "Epoch: 008, Step: 019, Val Loss: 0.6917\n",
            "Epoch: 008, Step: 020, Val Loss: 0.6938\n",
            "Epoch: 008, Step: 021, Val Loss: 0.7265\n",
            "Epoch: 008, Step: 022, Val Loss: 0.6943\n",
            "Epoch: 008, Step: 023, Val Loss: 0.6933\n",
            "Epoch: 008, Step: 024, Val Loss: 0.6600\n",
            "Epoch: 008, Step: 025, Val Loss: 0.6603\n",
            "Epoch: 008, Step: 026, Val Loss: 0.6934\n",
            "Epoch: 008, Step: 027, Val Loss: 0.6935\n",
            "Epoch: 008, Step: 028, Val Loss: 0.7257\n",
            "Epoch: 008, Step: 029, Val Loss: 0.6926\n",
            "Epoch: 008, Step: 030, Val Loss: 0.7261\n",
            "Epoch: 008, Step: 031, Val Loss: 0.7255\n",
            "Epoch: 008, Step: 032, Val Loss: 0.6952\n",
            "Epoch: 008, Step: 033, Val Loss: 0.6924\n",
            "Epoch: 008, Step: 034, Val Loss: 0.7247\n",
            "Epoch: 008, Step: 035, Val Loss: 0.7249\n",
            "Epoch: 008, Step: 036, Val Loss: 0.6600\n",
            "Epoch: 008, Step: 037, Val Loss: 0.6924\n",
            "Epoch: 008, Step: 038, Val Loss: 0.6592\n",
            "Epoch: 008, Step: 039, Val Loss: 0.6934\n",
            "Epoch: 008, Step: 040, Val Loss: 0.6934\n",
            "Epoch: 008, Step: 041, Val Loss: 0.6937\n",
            "Epoch: 008, Step: 042, Val Loss: 0.6611\n",
            "Epoch: 008, Step: 043, Val Loss: 0.7268\n",
            "Epoch: 008, Step: 044, Val Loss: 0.6941\n",
            "Epoch: 008, Step: 045, Val Loss: 0.6624\n",
            "Epoch: 008, Step: 046, Val Loss: 0.7246\n",
            "Epoch: 008, Step: 047, Val Loss: 0.6937\n",
            "Epoch: 008, Step: 048, Val Loss: 0.7255\n",
            "Epoch: 008, Step: 049, Val Loss: 0.6948\n",
            "Epoch: 008, Step: 050, Val Loss: 0.7270\n",
            "Epoch: 008, Step: 051, Val Loss: 0.6932\n",
            "Epoch: 008, Step: 052, Val Loss: 0.6600\n",
            "Epoch: 008, Step: 053, Val Loss: 0.6621\n",
            "Epoch: 008, Step: 054, Val Loss: 0.6930\n",
            "Epoch: 008, Step: 055, Val Loss: 0.6938\n",
            "Epoch: 008, Step: 056, Val Loss: 0.7271\n",
            "Epoch: 008, Step: 057, Val Loss: 0.6938\n",
            "Epoch: 008, Step: 058, Val Loss: 0.6941\n",
            "Epoch: 008, Step: 059, Val Loss: 0.7255\n",
            "Epoch: 008, Step: 060, Val Loss: 0.6941\n",
            "Epoch: 008, Step: 061, Val Loss: 0.6947\n",
            "Epoch: 008, Step: 062, Val Loss: 0.6941\n",
            "Epoch: 008, Step: 063, Val Loss: 0.6614\n",
            "Epoch: 008, Step: 064, Val Loss: 0.6898\n",
            "Epoch: 008, Step: 065, Val Loss: 0.6610\n",
            "Epoch: 008, Step: 066, Val Loss: 0.6939\n",
            "Epoch: 008, Step: 067, Val Loss: 0.7263\n",
            "Epoch: 008, Step: 068, Val Loss: 0.7272\n",
            "Epoch: 008, Step: 069, Val Loss: 0.6951\n",
            "Epoch: 008, Step: 070, Val Loss: 0.7254\n",
            "Epoch: 008, Step: 071, Val Loss: 0.6940\n",
            "Epoch: 008, Step: 072, Val Loss: 0.6930\n",
            "Epoch: 008, Step: 073, Val Loss: 0.6938\n",
            "Epoch: 008, Step: 074, Val Loss: 0.6927\n",
            "Epoch: 008, Step: 075, Val Loss: 0.6938\n",
            "Epoch: 008, Step: 076, Val Loss: 0.6943\n",
            "Epoch: 008, Step: 077, Val Loss: 0.7267\n",
            "Epoch: 008, Step: 078, Val Loss: 0.6927\n",
            "Epoch: 008, Step: 079, Val Loss: 0.6942\n",
            "Epoch: 008, Step: 080, Val Loss: 0.6943\n",
            "Epoch: 008, Step: 081, Val Loss: 0.6936\n",
            "Epoch: 008, Step: 082, Val Loss: 0.6930\n",
            "Epoch: 008, Step: 083, Val Loss: 0.7253\n",
            "Epoch: 008, Step: 084, Val Loss: 0.6943\n",
            "Epoch: 008, Step: 085, Val Loss: 0.6941\n",
            "Epoch: 008, Step: 086, Val Loss: 0.7266\n",
            "Epoch: 008, Step: 087, Val Loss: 0.7278\n",
            "Epoch: 008, Step: 088, Val Loss: 0.6606\n",
            "Epoch: 008, Step: 089, Val Loss: 0.6943\n",
            "Epoch: 008, Step: 090, Val Loss: 0.7269\n",
            "Epoch: 008, Step: 091, Val Loss: 0.6939\n",
            "Epoch: 008, Step: 092, Val Loss: 0.6941\n",
            "Epoch: 008, Step: 093, Val Loss: 0.7263\n",
            "Epoch: 008, Step: 094, Val Loss: 0.7264\n",
            "Epoch: 008, Step: 095, Val Loss: 0.6932\n",
            "Epoch: 008, Step: 096, Val Loss: 0.7262\n",
            "Epoch: 008, Step: 097, Val Loss: 0.7264\n",
            "Epoch: 008, Step: 098, Val Loss: 0.6934\n",
            "Epoch: 008, Step: 099, Val Loss: 0.6592\n",
            "Epoch: 008, Step: 100, Val Loss: 0.7259\n",
            "Epoch: 008, Step: 101, Val Loss: 0.7255\n",
            "Epoch: 008, Step: 102, Val Loss: 0.6932\n",
            "Epoch: 008, Step: 103, Val Loss: 0.6613\n",
            "Epoch: 008, Step: 104, Val Loss: 0.6599\n",
            "Epoch: 008, Step: 105, Val Loss: 0.7262\n",
            "Epoch: 008, Step: 106, Val Loss: 0.7267\n",
            "Epoch: 008, Step: 107, Val Loss: 0.7264\n",
            "Epoch: 008, Step: 108, Val Loss: 0.7271\n",
            "Epoch: 008, Step: 109, Val Loss: 0.6936\n",
            "Epoch: 008, Step: 110, Val Loss: 0.7245\n",
            "Epoch: 008, Step: 111, Val Loss: 0.6930\n",
            "Epoch: 008, Step: 112, Val Loss: 0.7266\n",
            "Epoch: 008, Step: 113, Val Loss: 0.7262\n",
            "Epoch: 008, Step: 114, Val Loss: 0.7260\n",
            "Epoch: 008, Step: 115, Val Loss: 0.6619\n",
            "Epoch: 008, Step: 116, Val Loss: 0.7275\n",
            "Epoch: 008, Step: 117, Val Loss: 0.6940\n",
            "Epoch: 009, Step: 000, Loss: 0.7227\n",
            "Epoch: 009, Step: 001, Loss: 0.6975\n",
            "Epoch: 009, Step: 002, Loss: 0.6648\n",
            "Epoch: 009, Step: 003, Loss: 0.7281\n",
            "Epoch: 009, Step: 004, Loss: 0.6666\n",
            "Epoch: 009, Step: 005, Loss: 0.6881\n",
            "Epoch: 009, Step: 006, Loss: 0.6609\n",
            "Epoch: 009, Step: 007, Loss: 0.7241\n",
            "Epoch: 009, Step: 008, Loss: 0.7314\n",
            "Epoch: 009, Step: 009, Loss: 0.6864\n",
            "Epoch: 009, Step: 010, Loss: 0.6965\n",
            "Epoch: 009, Step: 011, Loss: 0.6896\n",
            "Epoch: 009, Step: 012, Loss: 0.6876\n",
            "Epoch: 009, Step: 013, Loss: 0.7267\n",
            "Epoch: 009, Step: 014, Loss: 0.6882\n",
            "Epoch: 009, Step: 015, Loss: 0.7349\n",
            "Epoch: 009, Step: 016, Loss: 0.7262\n",
            "Epoch: 009, Step: 017, Loss: 0.6627\n",
            "Epoch: 009, Step: 018, Loss: 0.6894\n",
            "Epoch: 009, Step: 019, Loss: 0.6960\n",
            "Epoch: 009, Step: 020, Loss: 0.6957\n",
            "Epoch: 009, Step: 021, Loss: 0.6936\n",
            "Epoch: 009, Step: 022, Loss: 0.6663\n",
            "Epoch: 009, Step: 023, Loss: 0.7238\n",
            "Epoch: 009, Step: 024, Loss: 0.7236\n",
            "Epoch: 009, Step: 025, Loss: 0.7277\n",
            "Epoch: 009, Step: 026, Loss: 0.7306\n",
            "Epoch: 009, Step: 027, Loss: 0.7193\n",
            "Epoch: 009, Step: 028, Loss: 0.6903\n",
            "Epoch: 009, Step: 029, Loss: 0.6928\n",
            "Epoch: 009, Step: 030, Loss: 0.6964\n",
            "Epoch: 009, Step: 031, Loss: 0.6602\n",
            "Epoch: 009, Step: 032, Loss: 0.6898\n",
            "Epoch: 009, Step: 033, Loss: 0.6851\n",
            "Epoch: 009, Step: 034, Loss: 0.6895\n",
            "Epoch: 009, Step: 035, Loss: 0.6955\n",
            "Epoch: 009, Step: 036, Loss: 0.7331\n",
            "Epoch: 009, Step: 037, Loss: 0.7285\n",
            "Epoch: 009, Step: 038, Loss: 0.6648\n",
            "Epoch: 009, Step: 039, Loss: 0.6891\n",
            "Epoch: 009, Step: 040, Loss: 0.7233\n",
            "Epoch: 009, Step: 041, Loss: 0.7122\n",
            "Epoch: 009, Step: 042, Loss: 0.7238\n",
            "Epoch: 009, Step: 043, Loss: 0.6911\n",
            "Epoch: 009, Step: 044, Loss: 0.7268\n",
            "Epoch: 009, Step: 045, Loss: 0.7314\n",
            "Epoch: 009, Step: 046, Loss: 0.6940\n",
            "Epoch: 009, Step: 047, Loss: 0.6990\n",
            "Epoch: 009, Step: 048, Loss: 0.6534\n",
            "Epoch: 009, Step: 049, Loss: 0.6926\n",
            "Epoch: 009, Step: 050, Loss: 0.6628\n",
            "Epoch: 009, Step: 051, Loss: 0.6661\n",
            "Epoch: 009, Step: 052, Loss: 0.6627\n",
            "Epoch: 009, Step: 053, Loss: 0.7283\n",
            "Epoch: 009, Step: 054, Loss: 0.6897\n",
            "Epoch: 009, Step: 055, Loss: 0.6918\n",
            "Epoch: 009, Step: 056, Loss: 0.7196\n",
            "Epoch: 009, Step: 057, Loss: 0.7292\n",
            "Epoch: 009, Step: 058, Loss: 0.6632\n",
            "Epoch: 009, Step: 059, Loss: 0.7225\n",
            "Epoch: 009, Step: 060, Loss: 0.6980\n",
            "Epoch: 009, Step: 061, Loss: 0.6963\n",
            "Epoch: 009, Step: 062, Loss: 0.6585\n",
            "Epoch: 009, Step: 063, Loss: 0.6910\n",
            "Epoch: 009, Step: 064, Loss: 0.7311\n",
            "Epoch: 009, Step: 065, Loss: 0.6658\n",
            "Epoch: 009, Step: 066, Loss: 0.6952\n",
            "Epoch: 009, Step: 067, Loss: 0.6959\n",
            "Epoch: 009, Step: 068, Loss: 0.6991\n",
            "Epoch: 009, Step: 069, Loss: 0.7238\n",
            "Epoch: 009, Step: 070, Loss: 0.6950\n",
            "Epoch: 009, Step: 071, Loss: 0.6988\n",
            "Epoch: 009, Step: 072, Loss: 0.6910\n",
            "Epoch: 009, Step: 073, Loss: 0.7252\n",
            "Epoch: 009, Step: 074, Loss: 0.6931\n",
            "Epoch: 009, Step: 075, Loss: 0.6946\n",
            "Epoch: 009, Step: 076, Loss: 0.7341\n",
            "Epoch: 009, Step: 077, Loss: 0.7277\n",
            "Epoch: 009, Step: 078, Loss: 0.7221\n",
            "Epoch: 009, Step: 079, Loss: 0.6915\n",
            "Epoch: 009, Step: 080, Loss: 0.6640\n",
            "Epoch: 009, Step: 081, Loss: 0.6949\n",
            "Epoch: 009, Step: 082, Loss: 0.6931\n",
            "Epoch: 009, Step: 083, Loss: 0.6918\n",
            "Epoch: 009, Step: 084, Loss: 0.7284\n",
            "Epoch: 009, Step: 085, Loss: 0.6960\n",
            "Epoch: 009, Step: 086, Loss: 0.7290\n",
            "Epoch: 009, Step: 087, Loss: 0.6947\n",
            "Epoch: 009, Step: 088, Loss: 0.6939\n",
            "Epoch: 009, Step: 089, Loss: 0.6607\n",
            "Epoch: 009, Step: 090, Loss: 0.7268\n",
            "Epoch: 009, Step: 091, Loss: 0.7299\n",
            "Epoch: 009, Step: 092, Loss: 0.7249\n",
            "Epoch: 009, Step: 093, Loss: 0.7259\n",
            "Epoch: 009, Step: 094, Loss: 0.6934\n",
            "Epoch: 009, Step: 095, Loss: 0.6946\n",
            "Epoch: 009, Step: 096, Loss: 0.6638\n",
            "Epoch: 009, Step: 097, Loss: 0.6948\n",
            "Epoch: 009, Step: 098, Loss: 0.7228\n",
            "Epoch: 009, Step: 099, Loss: 0.7307\n",
            "Epoch: 009, Step: 100, Loss: 0.6863\n",
            "Epoch: 009, Step: 101, Loss: 0.6906\n",
            "Epoch: 009, Step: 102, Loss: 0.7006\n",
            "Epoch: 009, Step: 103, Loss: 0.6901\n",
            "Epoch: 009, Step: 104, Loss: 0.6924\n",
            "Epoch: 009, Step: 105, Loss: 0.6957\n",
            "Epoch: 009, Step: 106, Loss: 0.6908\n",
            "Epoch: 009, Step: 107, Loss: 0.7267\n",
            "Epoch: 009, Step: 108, Loss: 0.6937\n",
            "Epoch: 009, Step: 109, Loss: 0.6948\n",
            "Epoch: 009, Step: 110, Loss: 0.6979\n",
            "Epoch: 009, Step: 111, Loss: 0.6905\n",
            "Epoch: 009, Step: 112, Loss: 0.7229\n",
            "Epoch: 009, Step: 113, Loss: 0.7189\n",
            "Epoch: 009, Step: 114, Loss: 0.7269\n",
            "Epoch: 009, Step: 115, Loss: 0.6891\n",
            "Epoch: 009, Step: 116, Loss: 0.6928\n",
            "Epoch: 009, Step: 117, Loss: 0.6591\n",
            "Epoch: 009, Step: 118, Loss: 0.7291\n",
            "Epoch: 009, Step: 119, Loss: 0.6588\n",
            "Epoch: 009, Step: 120, Loss: 0.6951\n",
            "Epoch: 009, Step: 121, Loss: 0.6969\n",
            "Epoch: 009, Step: 122, Loss: 0.6956\n",
            "Epoch: 009, Step: 123, Loss: 0.6654\n",
            "Epoch: 009, Step: 124, Loss: 0.6569\n",
            "Epoch: 009, Step: 125, Loss: 0.6625\n",
            "Epoch: 009, Step: 126, Loss: 0.7366\n",
            "Epoch: 009, Step: 127, Loss: 0.6918\n",
            "Epoch: 009, Step: 128, Loss: 0.7228\n",
            "Epoch: 009, Step: 129, Loss: 0.7229\n",
            "Epoch: 009, Step: 130, Loss: 0.6917\n",
            "Epoch: 009, Step: 131, Loss: 0.7249\n",
            "Epoch: 009, Step: 132, Loss: 0.6994\n",
            "Epoch: 009, Step: 133, Loss: 0.6581\n",
            "Epoch: 009, Step: 134, Loss: 0.7202\n",
            "Epoch: 009, Step: 135, Loss: 0.6971\n",
            "Epoch: 009, Step: 136, Loss: 0.6923\n",
            "Epoch: 009, Step: 137, Loss: 0.6933\n",
            "Epoch: 009, Step: 138, Loss: 0.6667\n",
            "Epoch: 009, Step: 139, Loss: 0.7262\n",
            "Epoch: 009, Step: 140, Loss: 0.7227\n",
            "Epoch: 009, Step: 141, Loss: 0.7288\n",
            "Epoch: 009, Step: 142, Loss: 0.6928\n",
            "Epoch: 009, Step: 143, Loss: 0.6618\n",
            "Epoch: 009, Step: 144, Loss: 0.7250\n",
            "Epoch: 009, Step: 145, Loss: 0.6878\n",
            "Epoch: 009, Step: 146, Loss: 0.6989\n",
            "Epoch: 009, Step: 147, Loss: 0.6696\n",
            "Epoch: 009, Step: 148, Loss: 0.6626\n",
            "Epoch: 009, Step: 149, Loss: 0.6641\n",
            "Epoch: 009, Step: 150, Loss: 0.6933\n",
            "Epoch: 009, Step: 151, Loss: 0.6944\n",
            "Epoch: 009, Step: 152, Loss: 0.7262\n",
            "Epoch: 009, Step: 153, Loss: 0.7274\n",
            "Epoch: 009, Step: 154, Loss: 0.7210\n",
            "Epoch: 009, Step: 155, Loss: 0.6868\n",
            "Epoch: 009, Step: 156, Loss: 0.7269\n",
            "Epoch: 009, Step: 157, Loss: 0.6985\n",
            "Epoch: 009, Step: 158, Loss: 0.7304\n",
            "Epoch: 009, Step: 159, Loss: 0.7274\n",
            "Epoch: 009, Step: 160, Loss: 0.7200\n",
            "Epoch: 009, Step: 161, Loss: 0.6917\n",
            "Epoch: 009, Step: 162, Loss: 0.7203\n",
            "Epoch: 009, Step: 163, Loss: 0.7354\n",
            "Epoch: 009, Step: 164, Loss: 0.6947\n",
            "Epoch: 009, Step: 165, Loss: 0.7294\n",
            "Epoch: 009, Step: 166, Loss: 0.7300\n",
            "Epoch: 009, Step: 167, Loss: 0.6974\n",
            "Epoch: 009, Step: 168, Loss: 0.7284\n",
            "Epoch: 009, Step: 169, Loss: 0.6941\n",
            "Epoch: 009, Step: 170, Loss: 0.6945\n",
            "Epoch: 009, Step: 171, Loss: 0.6707\n",
            "Epoch: 009, Step: 172, Loss: 0.7190\n",
            "Epoch: 009, Step: 173, Loss: 0.7291\n",
            "Epoch: 009, Step: 174, Loss: 0.6947\n",
            "Epoch: 009, Step: 175, Loss: 0.6876\n",
            "Epoch: 009, Step: 176, Loss: 0.6995\n",
            "Epoch: 009, Step: 177, Loss: 0.6932\n",
            "Epoch: 009, Step: 178, Loss: 0.6619\n",
            "Epoch: 009, Step: 179, Loss: 0.7266\n",
            "Epoch: 009, Step: 180, Loss: 0.7227\n",
            "Epoch: 009, Step: 181, Loss: 0.6622\n",
            "Epoch: 009, Step: 182, Loss: 0.6840\n",
            "Epoch: 009, Step: 183, Loss: 0.6920\n",
            "Epoch: 009, Step: 184, Loss: 0.6952\n",
            "Epoch: 009, Step: 185, Loss: 0.6924\n",
            "Epoch: 009, Step: 186, Loss: 0.7239\n",
            "Epoch: 009, Step: 187, Loss: 0.6577\n",
            "Epoch: 009, Step: 188, Loss: 0.6941\n",
            "Epoch: 009, Step: 189, Loss: 0.7336\n",
            "Epoch: 009, Step: 190, Loss: 0.6944\n",
            "Epoch: 009, Step: 191, Loss: 0.7340\n",
            "Epoch: 009, Step: 192, Loss: 0.6575\n",
            "Epoch: 009, Step: 193, Loss: 0.6674\n",
            "Epoch: 009, Step: 194, Loss: 0.7273\n",
            "Epoch: 009, Step: 195, Loss: 0.6911\n",
            "Epoch: 009, Step: 196, Loss: 0.7217\n",
            "Epoch: 009, Step: 197, Loss: 0.6931\n",
            "Epoch: 009, Step: 198, Loss: 0.6528\n",
            "Epoch: 009, Step: 199, Loss: 0.6926\n",
            "Epoch: 009, Step: 200, Loss: 0.7272\n",
            "Epoch: 009, Step: 201, Loss: 0.6595\n",
            "Epoch: 009, Step: 202, Loss: 0.6618\n",
            "Epoch: 009, Step: 203, Loss: 0.7241\n",
            "Epoch: 009, Step: 204, Loss: 0.6894\n",
            "Epoch: 009, Step: 205, Loss: 0.7250\n",
            "Epoch: 009, Step: 206, Loss: 0.6995\n",
            "Epoch: 009, Step: 207, Loss: 0.7271\n",
            "Epoch: 009, Step: 208, Loss: 0.6984\n",
            "Epoch: 009, Step: 209, Loss: 0.6594\n",
            "Epoch: 009, Step: 210, Loss: 0.6894\n",
            "Epoch: 009, Step: 211, Loss: 0.6886\n",
            "Epoch: 009, Step: 212, Loss: 0.6965\n",
            "Epoch: 009, Step: 213, Loss: 0.6572\n",
            "Epoch: 009, Step: 214, Loss: 0.6929\n",
            "Epoch: 009, Step: 215, Loss: 0.6928\n",
            "Epoch: 009, Step: 216, Loss: 0.6967\n",
            "Epoch: 009, Step: 217, Loss: 0.6567\n",
            "Epoch: 009, Step: 218, Loss: 0.6973\n",
            "Epoch: 009, Step: 219, Loss: 0.6957\n",
            "Epoch: 009, Step: 220, Loss: 0.7231\n",
            "Epoch: 009, Step: 221, Loss: 0.7273\n",
            "Epoch: 009, Step: 222, Loss: 0.7284\n",
            "Epoch: 009, Step: 223, Loss: 0.7261\n",
            "Epoch: 009, Step: 224, Loss: 0.7306\n",
            "Epoch: 009, Step: 225, Loss: 0.6945\n",
            "Epoch: 009, Step: 226, Loss: 0.7242\n",
            "Epoch: 009, Step: 227, Loss: 0.6601\n",
            "Epoch: 009, Step: 228, Loss: 0.7253\n",
            "Epoch: 009, Step: 229, Loss: 0.7277\n",
            "Epoch: 009, Step: 230, Loss: 0.6957\n",
            "Epoch: 009, Step: 231, Loss: 0.7251\n",
            "Epoch: 009, Step: 232, Loss: 0.6559\n",
            "Epoch: 009, Step: 233, Loss: 0.6878\n",
            "Epoch: 009, Step: 234, Loss: 0.6914\n",
            "Epoch: 009, Step: 235, Loss: 0.6920\n",
            "Epoch: 009, Step: 236, Loss: 0.7255\n",
            "Epoch: 009, Step: 237, Loss: 0.7249\n",
            "Epoch: 009, Step: 238, Loss: 0.7001\n",
            "Epoch: 009, Step: 239, Loss: 0.6614\n",
            "Epoch: 009, Step: 240, Loss: 0.7276\n",
            "Epoch: 009, Step: 241, Loss: 0.7261\n",
            "Epoch: 009, Step: 242, Loss: 0.6974\n",
            "Epoch: 009, Step: 243, Loss: 0.7237\n",
            "Epoch: 009, Step: 244, Loss: 0.6849\n",
            "Epoch: 009, Step: 245, Loss: 0.7224\n",
            "Epoch: 009, Step: 246, Loss: 0.6909\n",
            "Epoch: 009, Step: 247, Loss: 0.6983\n",
            "Epoch: 009, Step: 248, Loss: 0.7014\n",
            "Epoch: 009, Step: 249, Loss: 0.7254\n",
            "Epoch: 009, Step: 250, Loss: 0.6966\n",
            "Epoch: 009, Step: 251, Loss: 0.6891\n",
            "Epoch: 009, Step: 252, Loss: 0.6897\n",
            "Epoch: 009, Step: 253, Loss: 0.6972\n",
            "Epoch: 009, Step: 254, Loss: 0.6636\n",
            "Epoch: 009, Step: 255, Loss: 0.7244\n",
            "Epoch: 009, Step: 256, Loss: 0.6949\n",
            "Epoch: 009, Step: 257, Loss: 0.6906\n",
            "Epoch: 009, Step: 258, Loss: 0.7308\n",
            "Epoch: 009, Step: 259, Loss: 0.6956\n",
            "Epoch: 009, Step: 260, Loss: 0.7288\n",
            "Epoch: 009, Step: 261, Loss: 0.7264\n",
            "Epoch: 009, Step: 262, Loss: 0.6970\n",
            "Epoch: 009, Step: 263, Loss: 0.6550\n",
            "Epoch: 009, Step: 264, Loss: 0.6631\n",
            "Epoch: 009, Step: 265, Loss: 0.7228\n",
            "Epoch: 009, Step: 266, Loss: 0.6978\n",
            "Epoch: 009, Step: 267, Loss: 0.6955\n",
            "Epoch: 009, Step: 268, Loss: 0.7326\n",
            "Epoch: 009, Step: 269, Loss: 0.6939\n",
            "Epoch: 009, Step: 270, Loss: 0.6861\n",
            "Epoch: 009, Step: 271, Loss: 0.7004\n",
            "Epoch: 009, Step: 272, Loss: 0.7204\n",
            "Epoch: 009, Step: 273, Loss: 0.6935\n",
            "Epoch: 009, Step: 274, Loss: 0.6901\n",
            "Epoch: 009, Step: 275, Loss: 0.6957\n",
            "Epoch: 009, Step: 276, Loss: 0.7342\n",
            "Epoch: 009, Step: 277, Loss: 0.7221\n",
            "Epoch: 009, Step: 278, Loss: 0.6619\n",
            "Epoch: 009, Step: 279, Loss: 0.7243\n",
            "Epoch: 009, Step: 280, Loss: 0.6908\n",
            "Epoch: 009, Step: 281, Loss: 0.6899\n",
            "Epoch: 009, Step: 282, Loss: 0.7210\n",
            "Epoch: 009, Step: 283, Loss: 0.6591\n",
            "Epoch: 009, Step: 284, Loss: 0.6679\n",
            "Epoch: 009, Step: 285, Loss: 0.6989\n",
            "Epoch: 009, Step: 286, Loss: 0.6905\n",
            "Epoch: 009, Step: 287, Loss: 0.6922\n",
            "Epoch: 009, Step: 288, Loss: 0.6902\n",
            "Epoch: 009, Step: 289, Loss: 0.7009\n",
            "Epoch: 009, Step: 290, Loss: 0.6958\n",
            "Epoch: 009, Step: 291, Loss: 0.7323\n",
            "Epoch: 009, Step: 292, Loss: 0.7214\n",
            "Epoch: 009, Step: 293, Loss: 0.6962\n",
            "Epoch: 009, Step: 294, Loss: 0.7017\n",
            "Epoch: 009, Step: 295, Loss: 0.7203\n",
            "Epoch: 009, Step: 296, Loss: 0.7316\n",
            "Epoch: 009, Step: 297, Loss: 0.7298\n",
            "Epoch: 009, Step: 298, Loss: 0.7271\n",
            "Epoch: 009, Step: 299, Loss: 0.6534\n",
            "Epoch: 009, Step: 300, Loss: 0.6941\n",
            "Epoch: 009, Step: 301, Loss: 0.7321\n",
            "Epoch: 009, Step: 302, Loss: 0.6926\n",
            "Epoch: 009, Step: 303, Loss: 0.6615\n",
            "Epoch: 009, Step: 304, Loss: 0.6882\n",
            "Epoch: 009, Step: 305, Loss: 0.6921\n",
            "Epoch: 009, Step: 306, Loss: 0.7286\n",
            "Epoch: 009, Step: 307, Loss: 0.7305\n",
            "Epoch: 009, Step: 308, Loss: 0.6939\n",
            "Epoch: 009, Step: 309, Loss: 0.7321\n",
            "Epoch: 009, Step: 310, Loss: 0.6972\n",
            "Epoch: 009, Step: 311, Loss: 0.6973\n",
            "Epoch: 009, Step: 312, Loss: 0.7272\n",
            "Epoch: 009, Step: 313, Loss: 0.6962\n",
            "Epoch: 009, Step: 314, Loss: 0.6644\n",
            "Epoch: 009, Step: 315, Loss: 0.6945\n",
            "Epoch: 009, Step: 316, Loss: 0.6939\n",
            "Epoch: 009, Step: 317, Loss: 0.7353\n",
            "Epoch: 009, Step: 318, Loss: 0.6607\n",
            "Epoch: 009, Step: 319, Loss: 0.6573\n",
            "Epoch: 009, Step: 320, Loss: 0.6591\n",
            "Epoch: 009, Step: 321, Loss: 0.6627\n",
            "Epoch: 009, Step: 322, Loss: 0.6972\n",
            "Epoch: 009, Step: 323, Loss: 0.6576\n",
            "Epoch: 009, Step: 324, Loss: 0.6947\n",
            "Epoch: 009, Step: 325, Loss: 0.7200\n",
            "Epoch: 009, Step: 326, Loss: 0.6957\n",
            "Epoch: 009, Step: 327, Loss: 0.6910\n",
            "Epoch: 009, Step: 328, Loss: 0.6870\n",
            "Epoch: 009, Step: 329, Loss: 0.6845\n",
            "Epoch: 009, Step: 330, Loss: 0.7020\n",
            "Epoch: 009, Step: 331, Loss: 0.7293\n",
            "Epoch: 009, Step: 332, Loss: 0.7205\n",
            "Epoch: 009, Step: 333, Loss: 0.6933\n",
            "Epoch: 009, Step: 334, Loss: 0.7262\n",
            "Epoch: 009, Step: 335, Loss: 0.6634\n",
            "Epoch: 009, Step: 336, Loss: 0.7322\n",
            "Epoch: 009, Step: 337, Loss: 0.6910\n",
            "Epoch: 009, Step: 338, Loss: 0.6587\n",
            "Epoch: 009, Step: 339, Loss: 0.6975\n",
            "Epoch: 009, Step: 340, Loss: 0.7329\n",
            "Epoch: 009, Step: 341, Loss: 0.7344\n",
            "Epoch: 009, Step: 342, Loss: 0.6916\n",
            "Epoch: 009, Step: 343, Loss: 0.6577\n",
            "Epoch: 009, Step: 344, Loss: 0.7261\n",
            "Epoch: 009, Step: 345, Loss: 0.7317\n",
            "Epoch: 009, Step: 346, Loss: 0.6889\n",
            "Epoch: 009, Step: 347, Loss: 0.7273\n",
            "Epoch: 009, Step: 348, Loss: 0.6606\n",
            "Epoch: 009, Step: 349, Loss: 0.6886\n",
            "Epoch: 009, Step: 350, Loss: 0.6916\n",
            "Epoch: 009, Step: 351, Loss: 0.6608\n",
            "Epoch: 009, Step: 352, Loss: 0.6544\n",
            "Epoch: 009, Step: 353, Loss: 0.7254\n",
            "Epoch: 009, Step: 354, Loss: 0.6929\n",
            "Epoch: 009, Step: 355, Loss: 0.6928\n",
            "Epoch: 009, Step: 356, Loss: 0.7297\n",
            "Epoch: 009, Step: 357, Loss: 0.7189\n",
            "Epoch: 009, Step: 358, Loss: 0.6967\n",
            "Epoch: 009, Step: 359, Loss: 0.6949\n",
            "Epoch: 009, Step: 360, Loss: 0.6628\n",
            "Epoch: 009, Step: 361, Loss: 0.6955\n",
            "Epoch: 009, Step: 362, Loss: 0.6618\n",
            "Epoch: 009, Step: 363, Loss: 0.6870\n",
            "Epoch: 009, Step: 364, Loss: 0.7276\n",
            "Epoch: 009, Step: 365, Loss: 0.6540\n",
            "Epoch: 009, Step: 366, Loss: 0.6936\n",
            "Epoch: 009, Step: 367, Loss: 0.6526\n",
            "Epoch: 009, Step: 368, Loss: 0.6697\n",
            "Epoch: 009, Step: 369, Loss: 0.7273\n",
            "Epoch: 009, Step: 370, Loss: 0.6975\n",
            "Epoch: 009, Step: 371, Loss: 0.6955\n",
            "Epoch: 009, Step: 372, Loss: 0.7276\n",
            "Epoch: 009, Step: 373, Loss: 0.6844\n",
            "Epoch: 009, Step: 374, Loss: 0.6930\n",
            "Epoch: 009, Step: 375, Loss: 0.7292\n",
            "Epoch: 009, Step: 376, Loss: 0.6955\n",
            "Epoch: 009, Step: 377, Loss: 0.6969\n",
            "Epoch: 009, Step: 378, Loss: 0.6616\n",
            "Epoch: 009, Step: 379, Loss: 0.6999\n",
            "Epoch: 009, Step: 380, Loss: 0.6635\n",
            "Epoch: 009, Step: 381, Loss: 0.6941\n",
            "Epoch: 009, Step: 382, Loss: 0.7254\n",
            "Epoch: 009, Step: 383, Loss: 0.6972\n",
            "Epoch: 009, Step: 384, Loss: 0.6962\n",
            "Epoch: 009, Step: 385, Loss: 0.6996\n",
            "Epoch: 009, Step: 386, Loss: 0.6936\n",
            "Epoch: 009, Step: 387, Loss: 0.7254\n",
            "Epoch: 009, Step: 388, Loss: 0.7274\n",
            "Epoch: 009, Step: 389, Loss: 0.7213\n",
            "Epoch: 009, Step: 390, Loss: 0.6878\n",
            "Epoch: 009, Step: 391, Loss: 0.6867\n",
            "Epoch: 009, Step: 392, Loss: 0.6958\n",
            "Epoch: 009, Step: 393, Loss: 0.7306\n",
            "Epoch: 009, Step: 394, Loss: 0.6884\n",
            "Epoch: 009, Step: 395, Loss: 0.6517\n",
            "Epoch: 009, Step: 396, Loss: 0.7286\n",
            "Epoch: 009, Step: 397, Loss: 0.7247\n",
            "Epoch: 009, Step: 398, Loss: 0.6604\n",
            "Epoch: 009, Step: 399, Loss: 0.7295\n",
            "Epoch: 009, Step: 400, Loss: 0.6978\n",
            "Epoch: 009, Step: 401, Loss: 0.7268\n",
            "Epoch: 009, Step: 402, Loss: 0.6637\n",
            "Epoch: 009, Step: 403, Loss: 0.6681\n",
            "Epoch: 009, Step: 404, Loss: 0.6932\n",
            "Epoch: 009, Step: 405, Loss: 0.7254\n",
            "Epoch: 009, Step: 406, Loss: 0.7283\n",
            "Epoch: 009, Step: 407, Loss: 0.7317\n",
            "Epoch: 009, Step: 408, Loss: 0.7195\n",
            "Epoch: 009, Step: 409, Loss: 0.6931\n",
            "Epoch: 009, Step: 410, Loss: 0.6880\n",
            "Epoch: 009, Step: 411, Loss: 0.6925\n",
            "Epoch: 009, Step: 412, Loss: 0.7253\n",
            "Epoch: 009, Step: 000, Val Loss: 0.7280\n",
            "Epoch: 009, Step: 001, Val Loss: 0.6621\n",
            "Epoch: 009, Step: 002, Val Loss: 0.6948\n",
            "Epoch: 009, Step: 003, Val Loss: 0.7252\n",
            "Epoch: 009, Step: 004, Val Loss: 0.6603\n",
            "Epoch: 009, Step: 005, Val Loss: 0.6929\n",
            "Epoch: 009, Step: 006, Val Loss: 0.6938\n",
            "Epoch: 009, Step: 007, Val Loss: 0.7266\n",
            "Epoch: 009, Step: 008, Val Loss: 0.6607\n",
            "Epoch: 009, Step: 009, Val Loss: 0.6615\n",
            "Epoch: 009, Step: 010, Val Loss: 0.7251\n",
            "Epoch: 009, Step: 011, Val Loss: 0.6608\n",
            "Epoch: 009, Step: 012, Val Loss: 0.6946\n",
            "Epoch: 009, Step: 013, Val Loss: 0.6924\n",
            "Epoch: 009, Step: 014, Val Loss: 0.6944\n",
            "Epoch: 009, Step: 015, Val Loss: 0.6935\n",
            "Epoch: 009, Step: 016, Val Loss: 0.6608\n",
            "Epoch: 009, Step: 017, Val Loss: 0.7275\n",
            "Epoch: 009, Step: 018, Val Loss: 0.6612\n",
            "Epoch: 009, Step: 019, Val Loss: 0.7266\n",
            "Epoch: 009, Step: 020, Val Loss: 0.7269\n",
            "Epoch: 009, Step: 021, Val Loss: 0.6932\n",
            "Epoch: 009, Step: 022, Val Loss: 0.6936\n",
            "Epoch: 009, Step: 023, Val Loss: 0.7259\n",
            "Epoch: 009, Step: 024, Val Loss: 0.7265\n",
            "Epoch: 009, Step: 025, Val Loss: 0.7257\n",
            "Epoch: 009, Step: 026, Val Loss: 0.6612\n",
            "Epoch: 009, Step: 027, Val Loss: 0.6614\n",
            "Epoch: 009, Step: 028, Val Loss: 0.6918\n",
            "Epoch: 009, Step: 029, Val Loss: 0.6939\n",
            "Epoch: 009, Step: 030, Val Loss: 0.7255\n",
            "Epoch: 009, Step: 031, Val Loss: 0.6938\n",
            "Epoch: 009, Step: 032, Val Loss: 0.6941\n",
            "Epoch: 009, Step: 033, Val Loss: 0.6934\n",
            "Epoch: 009, Step: 034, Val Loss: 0.6605\n",
            "Epoch: 009, Step: 035, Val Loss: 0.6612\n",
            "Epoch: 009, Step: 036, Val Loss: 0.6605\n",
            "Epoch: 009, Step: 037, Val Loss: 0.7263\n",
            "Epoch: 009, Step: 038, Val Loss: 0.7263\n",
            "Epoch: 009, Step: 039, Val Loss: 0.6934\n",
            "Epoch: 009, Step: 040, Val Loss: 0.7267\n",
            "Epoch: 009, Step: 041, Val Loss: 0.7254\n",
            "Epoch: 009, Step: 042, Val Loss: 0.6596\n",
            "Epoch: 009, Step: 043, Val Loss: 0.6933\n",
            "Epoch: 009, Step: 044, Val Loss: 0.6948\n",
            "Epoch: 009, Step: 045, Val Loss: 0.6924\n",
            "Epoch: 009, Step: 046, Val Loss: 0.7262\n",
            "Epoch: 009, Step: 047, Val Loss: 0.6945\n",
            "Epoch: 009, Step: 048, Val Loss: 0.7268\n",
            "Epoch: 009, Step: 049, Val Loss: 0.7262\n",
            "Epoch: 009, Step: 050, Val Loss: 0.6955\n",
            "Epoch: 009, Step: 051, Val Loss: 0.6627\n",
            "Epoch: 009, Step: 052, Val Loss: 0.6930\n",
            "Epoch: 009, Step: 053, Val Loss: 0.7257\n",
            "Epoch: 009, Step: 054, Val Loss: 0.6619\n",
            "Epoch: 009, Step: 055, Val Loss: 0.6932\n",
            "Epoch: 009, Step: 056, Val Loss: 0.7251\n",
            "Epoch: 009, Step: 057, Val Loss: 0.6605\n",
            "Epoch: 009, Step: 058, Val Loss: 0.6946\n",
            "Epoch: 009, Step: 059, Val Loss: 0.6941\n",
            "Epoch: 009, Step: 060, Val Loss: 0.6936\n",
            "Epoch: 009, Step: 061, Val Loss: 0.7253\n",
            "Epoch: 009, Step: 062, Val Loss: 0.6929\n",
            "Epoch: 009, Step: 063, Val Loss: 0.6933\n",
            "Epoch: 009, Step: 064, Val Loss: 0.6939\n",
            "Epoch: 009, Step: 065, Val Loss: 0.6939\n",
            "Epoch: 009, Step: 066, Val Loss: 0.6601\n",
            "Epoch: 009, Step: 067, Val Loss: 0.6618\n",
            "Epoch: 009, Step: 068, Val Loss: 0.7264\n",
            "Epoch: 009, Step: 069, Val Loss: 0.7259\n",
            "Epoch: 009, Step: 070, Val Loss: 0.7268\n",
            "Epoch: 009, Step: 071, Val Loss: 0.7257\n",
            "Epoch: 009, Step: 072, Val Loss: 0.6608\n",
            "Epoch: 009, Step: 073, Val Loss: 0.6940\n",
            "Epoch: 009, Step: 074, Val Loss: 0.7266\n",
            "Epoch: 009, Step: 075, Val Loss: 0.6608\n",
            "Epoch: 009, Step: 076, Val Loss: 0.7267\n",
            "Epoch: 009, Step: 077, Val Loss: 0.7272\n",
            "Epoch: 009, Step: 078, Val Loss: 0.7256\n",
            "Epoch: 009, Step: 079, Val Loss: 0.7255\n",
            "Epoch: 009, Step: 080, Val Loss: 0.6939\n",
            "Epoch: 009, Step: 081, Val Loss: 0.6944\n",
            "Epoch: 009, Step: 082, Val Loss: 0.7263\n",
            "Epoch: 009, Step: 083, Val Loss: 0.6584\n",
            "Epoch: 009, Step: 084, Val Loss: 0.7271\n",
            "Epoch: 009, Step: 085, Val Loss: 0.6926\n",
            "Epoch: 009, Step: 086, Val Loss: 0.7260\n",
            "Epoch: 009, Step: 087, Val Loss: 0.7268\n",
            "Epoch: 009, Step: 088, Val Loss: 0.7260\n",
            "Epoch: 009, Step: 089, Val Loss: 0.7253\n",
            "Epoch: 009, Step: 090, Val Loss: 0.7265\n",
            "Epoch: 009, Step: 091, Val Loss: 0.6939\n",
            "Epoch: 009, Step: 092, Val Loss: 0.6936\n",
            "Epoch: 009, Step: 093, Val Loss: 0.6933\n",
            "Epoch: 009, Step: 094, Val Loss: 0.6942\n",
            "Epoch: 009, Step: 095, Val Loss: 0.6935\n",
            "Epoch: 009, Step: 096, Val Loss: 0.7257\n",
            "Epoch: 009, Step: 097, Val Loss: 0.6938\n",
            "Epoch: 009, Step: 098, Val Loss: 0.7256\n",
            "Epoch: 009, Step: 099, Val Loss: 0.6929\n",
            "Epoch: 009, Step: 100, Val Loss: 0.7269\n",
            "Epoch: 009, Step: 101, Val Loss: 0.6930\n",
            "Epoch: 009, Step: 102, Val Loss: 0.6943\n",
            "Epoch: 009, Step: 103, Val Loss: 0.7268\n",
            "Epoch: 009, Step: 104, Val Loss: 0.7267\n",
            "Epoch: 009, Step: 105, Val Loss: 0.7249\n",
            "Epoch: 009, Step: 106, Val Loss: 0.7259\n",
            "Epoch: 009, Step: 107, Val Loss: 0.7260\n",
            "Epoch: 009, Step: 108, Val Loss: 0.7261\n",
            "Epoch: 009, Step: 109, Val Loss: 0.6587\n",
            "Epoch: 009, Step: 110, Val Loss: 0.6598\n",
            "Epoch: 009, Step: 111, Val Loss: 0.6926\n",
            "Epoch: 009, Step: 112, Val Loss: 0.6933\n",
            "Epoch: 009, Step: 113, Val Loss: 0.6934\n",
            "Epoch: 009, Step: 114, Val Loss: 0.6937\n",
            "Epoch: 009, Step: 115, Val Loss: 0.6610\n",
            "Epoch: 009, Step: 116, Val Loss: 0.6945\n",
            "Epoch: 009, Step: 117, Val Loss: 0.7263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_y, preds = test(model2, test_loader, criterion)"
      ],
      "metadata": {
        "id": "RKwRUjXxmkiG"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(test_y, preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OilI4U26m3Hu",
        "outputId": "309b94d6-7727-42cb-a151-0f4fc754e0fc"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.57      0.72       110\n",
            "           1       0.08      0.57      0.14         7\n",
            "\n",
            "    accuracy                           0.57       117\n",
            "   macro avg       0.52      0.57      0.43       117\n",
            "weighted avg       0.90      0.57      0.68       117\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LrlBHR3Do76p"
      },
      "execution_count": 127,
      "outputs": []
    }
  ]
}